{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone Vector Database\n",
    "\n",
    "## GenAI Foundation Training - Day 2\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn Pinecone-specific concepts that differ from ChromaDB:\n",
    "\n",
    "1. **When to use Pinecone vs ChromaDB** - Decision framework for choosing the right database\n",
    "2. **Serverless architecture** - Auto-scaling, managed infrastructure\n",
    "3. **Namespaces** - Multi-tenancy within a single index (game-changer!)\n",
    "4. **Production-ready patterns** - Building scalable search engines\n",
    "5. **Free tier optimization** - Maximize 100K vector limit\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "‚úÖ **Completed Notebook 03** - Vector Databases and Embeddings  \n",
    "‚úÖ **OpenAI API key** - For generating embeddings  \n",
    "‚úÖ **Pinecone account** - Free Starter tier (create at [pinecone.io](https://pinecone.io))\n",
    "\n",
    "### What We WON'T Repeat from Notebook 03\n",
    "\n",
    "- ‚ùå Embeddings fundamentals ‚Üí Already covered in Section 3\n",
    "- ‚ùå Chunking strategies ‚Üí Already covered in Section 7\n",
    "- ‚ùå Similarity metrics ‚Üí Already covered in Section 6\n",
    "\n",
    "**Focus**: Pinecone's unique features and when to use them.\n",
    "\n",
    "### Duration\n",
    "\n",
    "This notebook takes approximately **1 hour** to complete.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 1: Setup & Installation\n\n### Package Installation\n\n**Important**: As of 2025, the official package is `pinecone` (renamed from the deprecated `pinecone-client` in v5.1.0)\n\nWe'll need:\n- **pinecone** - Pinecone Python SDK (v8.0.0, API version 2025-10)\n- **openai** - For generating embeddings (same as notebook 03)\n\n**Requirements**:\n- Python 3.10 or later (Python 3.9 is no longer supported)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install pinecone openai -q\n\nprint(\"‚úÖ Packages installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# OpenAI for embeddings\n",
    "import openai\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup API Keys (Using Google Colab Secrets)\n",
    "\n",
    "**Setting Up Google Colab Secrets:**\n",
    "\n",
    "1. Click the **üîë (key icon)** in the left sidebar\n",
    "2. Add these secrets:\n",
    "   - `PINECONE_API_KEY` - From your Pinecone dashboard ([pinecone.io](https://pinecone.io))\n",
    "   - `OPENAI_API_KEY` - From OpenAI platform\n",
    "3. Toggle **\"Notebook access\"** ON for each key\n",
    "\n",
    "**Getting your Pinecone API Key:**\n",
    "1. Go to [https://www.pinecone.io/](https://www.pinecone.io/)\n",
    "2. Sign up (free, no credit card required)\n",
    "3. Navigate to **API Keys** in the dashboard\n",
    "4. Copy your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import userdata for Colab secrets\n",
    "from google.colab import userdata\n",
    "\n",
    "# Retrieve API keys\n",
    "try:\n",
    "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
    "    print(\"‚úÖ Pinecone API key loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading Pinecone API key: {e}\")\n",
    "    print(\"Please set PINECONE_API_KEY in Google Colab Secrets.\")\n",
    "\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ OpenAI API key loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading OpenAI API key: {e}\")\n",
    "    print(\"Please set OPENAI_API_KEY in Google Colab Secrets.\")\n",
    "\n",
    "print(\"\\n‚úÖ API keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Free Starter Tier Limits\n",
    "\n",
    "Pinecone's free tier includes:\n",
    "\n",
    "- **1 serverless index** (cannot create multiple)\n",
    "- **100,000 vectors** (~50MB with 1536-dim embeddings)\n",
    "- **5 queries/second**\n",
    "- **No credit card required**\n",
    "\n",
    "This is perfect for:\n",
    "- Learning and experimentation\n",
    "- Small production applications\n",
    "- Prototyping before scaling\n",
    "\n",
    "**Pro tip**: Use **namespaces** (covered in Section 3) to organize multiple projects within your single free index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: ChromaDB vs Pinecone - Decision Framework\n",
    "\n",
    "### When Should You Use Each?\n",
    "\n",
    "Let's understand the key differences to make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Table\n",
    "\n",
    "| Criterion | ChromaDB | Pinecone |\n",
    "|-----------|----------|----------|\n",
    "| **Deployment** | Self-hosted (local/server) | Fully managed cloud |\n",
    "| **Cost** | Free (you host) | Free tier + paid ($0.096/hr starter) |\n",
    "| **Setup** | `pip install` ‚Üí instant | Account + index creation (~2 min) |\n",
    "| **Scalability** | Manual (upgrade RAM/server) | Automatic (serverless auto-scaling) |\n",
    "| **Operations** | You manage backups/monitoring | Built-in monitoring, backups, SLA |\n",
    "| **Latency** | Local: <10ms, Server: 50-100ms | 50-100ms (cloud API) |\n",
    "| **Privacy** | 100% local/your infrastructure | Data in Pinecone cloud |\n",
    "| **Multi-tenancy** | Multiple collections | **Namespaces** (unique feature!) |\n",
    "| **Metadata Filtering** | Basic filtering | Rich operators ($eq, $gte, $in, etc.) |\n",
    "| **Production Ready** | Requires setup | Managed, production-grade |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**ChromaDB** = Local/self-hosted, full control, zero cost  \n",
    "**Pinecone** = Managed cloud, auto-scaling, production features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Pattern Comparison\n",
    "\n",
    "Let's see how the APIs differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromaDB pattern (from notebook 03)\n",
    "print(\"ChromaDB API Pattern:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "from chromadb import Client\n",
    "client = Client()\n",
    "collection = client.create_collection(\"docs\")\n",
    "collection.add(\n",
    "    documents=[...], \n",
    "    embeddings=[...], \n",
    "    ids=[...]\n",
    ")\n",
    "results = collection.query(\n",
    "    query_embeddings=[...], \n",
    "    n_results=3\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPinecone API Pattern:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=\"...\")\n",
    "index = pc.Index(\"docs\")\n",
    "index.upsert(\n",
    "    vectors=[\n",
    "        (id, embedding, metadata), \n",
    "        ...\n",
    "    ],\n",
    "    namespace=\"project-a\"  # Multi-tenancy!\n",
    ")\n",
    "results = index.query(\n",
    "    vector=[...], \n",
    "    top_k=3, \n",
    "    namespace=\"project-a\"\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"1. Pinecone uses upsert() instead of add()\")\n",
    "print(\"2. Pinecone has namespaces for multi-tenancy\")\n",
    "print(\"3. Pinecone returns matches with similarity scores\")\n",
    "print(\"4. Pinecone vector format: (id, embedding, metadata) tuples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: When to Use Each\n",
    "\n",
    "**Choose ChromaDB When:**\n",
    "\n",
    "‚úÖ Prototyping or learning  \n",
    "‚úÖ Small datasets (<100K vectors)  \n",
    "‚úÖ Local development environment  \n",
    "‚úÖ Privacy-sensitive data (healthcare, legal, finance)  \n",
    "‚úÖ Budget constraints (free forever)  \n",
    "‚úÖ Need fastest local queries (<10ms)  \n",
    "‚úÖ Want full control over infrastructure\n",
    "\n",
    "**Choose Pinecone When:**\n",
    "\n",
    "‚úÖ Production applications at scale  \n",
    "‚úÖ Unpredictable traffic (need auto-scaling)  \n",
    "‚úÖ Multi-tenant applications (SaaS products)  \n",
    "‚úÖ Want managed infrastructure (no DevOps overhead)  \n",
    "‚úÖ Need advanced features (hybrid search, rich filtering)  \n",
    "‚úÖ Global deployment with low latency  \n",
    "‚úÖ Enterprise SLA and support\n",
    "\n",
    "**Hybrid Approach (Common in Production):**\n",
    "\n",
    "Use **both**!\n",
    "- **ChromaDB** for development/testing (fast iteration, no costs)\n",
    "- **Pinecone** for production (managed, scalable, reliable)\n",
    "\n",
    "This is what many companies do - develop with ChromaDB, deploy with Pinecone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Example\n",
    "\n",
    "**Scenario**: Building a RAG chatbot for a SaaS product\n",
    "\n",
    "**Development Phase**:  \n",
    "‚Üí Use ChromaDB locally  \n",
    "‚Üí Fast iteration, no costs  \n",
    "‚Üí Test chunking strategies, retrieval quality\n",
    "\n",
    "**Production Phase**:  \n",
    "‚Üí Deploy with Pinecone  \n",
    "‚Üí Auto-scaling for traffic spikes  \n",
    "‚Üí Namespaces for multi-tenant isolation  \n",
    "‚Üí Managed backups and monitoring\n",
    "\n",
    "**Cost**: ~$70/month for Pinecone Starter (production) + $0 for ChromaDB (dev)\n",
    "\n",
    "This is a common and cost-effective pattern!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Pinecone Architecture & Core Concepts\n",
    "\n",
    "Now let's get hands-on with Pinecone-specific features.\n",
    "\n",
    "### Initialize Pinecone Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Pinecone client initialized!\")\n",
    "print(f\"\\nExisting indexes: {[idx.name for idx in pc.list_indexes()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Indexes\n",
    "\n",
    "An **index** is a container for vectors with specific configuration:\n",
    "\n",
    "- **Dimension**: Must match your embedding model (1536 for OpenAI text-embedding-3-small)\n",
    "- **Metric**: cosine, euclidean, or dotproduct\n",
    "- **Spec**: Serverless (auto-scaling) or Pods (fixed capacity)\n",
    "\n",
    "**Free tier**: You can create **1 serverless index** only.\n",
    "\n",
    "### Create a Serverless Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"rag-demo\"\n",
    "DIMENSION = 1536  # OpenAI text-embedding-3-small\n",
    "\n",
    "# Check if index exists\n",
    "existing = [idx.name for idx in pc.list_indexes()]\n",
    "\n",
    "if INDEX_NAME in existing:\n",
    "    print(f\"‚ö†Ô∏è  Index '{INDEX_NAME}' already exists. Deleting to start fresh...\")\n",
    "    pc.delete_index(INDEX_NAME)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create serverless index\n",
    "print(f\"üì¶ Creating serverless index '{INDEX_NAME}'...\")\n",
    "pc.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=DIMENSION,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wait until ready\n",
    "while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "    print(\"‚è≥ Waiting for index to be ready...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n‚úÖ Index '{INDEX_NAME}' is ready!\")\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Display stats\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nüìä Index Stats:\")\n",
    "print(f\"   Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "print(f\"   Dimension: {DIMENSION}\")\n",
    "print(f\"   Metric: cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespaces: The Multi-Tenancy Superpower\n",
    "\n",
    "**Problem**: Free tier = 1 index. How do you separate different projects or customers?\n",
    "\n",
    "**Solution**: **Namespaces!**\n",
    "\n",
    "A **namespace** is a logical partition within an index:\n",
    "- Same index, different \"folders\"\n",
    "- Queries only search the specified namespace\n",
    "- **Perfect for multi-tenant SaaS applications**\n",
    "\n",
    "**Examples**:\n",
    "```python\n",
    "# E-commerce\n",
    "namespace = \"customer-123\"\n",
    "\n",
    "# SaaS product\n",
    "namespace = \"org-acme-corp\"\n",
    "\n",
    "# Multi-project\n",
    "namespace = \"project-research\"\n",
    "```\n",
    "\n",
    "**Free tier hack**: 1 index with **unlimited namespaces**!\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Index: \"rag-demo\"\n",
    "‚îú‚îÄ‚îÄ namespace: \"customer-1\"     (10K vectors)\n",
    "‚îú‚îÄ‚îÄ namespace: \"customer-2\"     (15K vectors)\n",
    "‚îú‚îÄ‚îÄ namespace: \"customer-3\"     (8K vectors)\n",
    "‚îî‚îÄ‚îÄ namespace: \"test\"           (1K vectors)\n",
    "\n",
    "Total: 34K vectors in 1 index\n",
    "```\n",
    "\n",
    "Each customer's data is isolated, but you only use 1 index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Filtering\n",
    "\n",
    "Pinecone supports rich metadata queries with powerful operators:\n",
    "\n",
    "**Available Operators**:\n",
    "- `$eq`: Equal to\n",
    "- `$ne`: Not equal to\n",
    "- `$gt`, `$gte`: Greater than (or equal)\n",
    "- `$lt`, `$lte`: Less than (or equal)\n",
    "- `$in`: In a list\n",
    "- `$nin`: Not in a list\n",
    "\n",
    "**Examples**:\n",
    "```python\n",
    "# Equal to\n",
    "filter = {\"category\": {\"$eq\": \"ai\"}}\n",
    "\n",
    "# Greater than or equal\n",
    "filter = {\"year\": {\"$gte\": 2020}}\n",
    "\n",
    "# Multiple conditions (AND)\n",
    "filter = {\n",
    "    \"category\": \"ai\",\n",
    "    \"year\": {\"$gte\": 2020}\n",
    "}\n",
    "\n",
    "# In a list\n",
    "filter = {\"author\": {\"$in\": [\"Brown\", \"Vaswani\"]}}\n",
    "```\n",
    "\n",
    "We'll see these in action in Section 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Hands-On - Building a Pinecone Search Engine\n",
    "\n",
    "Let's build a production-ready search engine using Pinecone.\n",
    "\n",
    "### Sample Dataset: AI Research Summaries\n",
    "\n",
    "We'll use 8 AI research summaries to demonstrate:\n",
    "- Similarity search\n",
    "- Namespace isolation\n",
    "- Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with rich metadata\n",
    "documents = [\n",
    "    {\n",
    "        \"text\": \"Transformers revolutionized NLP using self-attention mechanisms.\",\n",
    "        \"metadata\": {\"category\": \"nlp\", \"year\": 2017, \"author\": \"Vaswani\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"BERT uses bidirectional transformers for language understanding.\",\n",
    "        \"metadata\": {\"category\": \"nlp\", \"year\": 2018, \"author\": \"Devlin\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"GPT-3 demonstrated few-shot learning with 175B parameters.\",\n",
    "        \"metadata\": {\"category\": \"llm\", \"year\": 2020, \"author\": \"Brown\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Stable Diffusion enables text-to-image generation using latent diffusion.\",\n",
    "        \"metadata\": {\"category\": \"cv\", \"year\": 2022, \"author\": \"Rombach\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"RLHF aligns LLMs with human preferences through reward modeling.\",\n",
    "        \"metadata\": {\"category\": \"llm\", \"year\": 2022, \"author\": \"Ouyang\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"AlphaFold 2 predicts protein structures with atomic accuracy.\",\n",
    "        \"metadata\": {\"category\": \"biology\", \"year\": 2021, \"author\": \"Jumper\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"RAG combines retrieval with generation for knowledge-grounded responses.\",\n",
    "        \"metadata\": {\"category\": \"llm\", \"year\": 2020, \"author\": \"Lewis\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Vision Transformers apply transformers to image classification.\",\n",
    "        \"metadata\": {\"category\": \"cv\", \"year\": 2020, \"author\": \"Dosovitskiy\"}\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} research summaries\")\n",
    "print(f\"\\nCategories: {set(d['metadata']['category'] for d in documents)}\")\n",
    "print(f\"Year range: {min(d['metadata']['year'] for d in documents)} - {max(d['metadata']['year'] for d in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings\n",
    "\n",
    "We'll reuse the embedding pattern from notebook 03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings using OpenAI (same as notebook 03).\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "texts = [doc[\"text\"] for doc in documents]\n",
    "embeddings = get_embeddings(texts)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert Vectors to Pinecone\n",
    "\n",
    "**Upsert** = Insert or Update (same API for both operations)\n",
    "\n",
    "**Vector format**: List of tuples `(id, embedding, metadata)`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare vectors for Pinecone\nvectors = [\n    (\n        f\"doc_{i}\",                    # ID\n        embeddings[i],                  # Embedding vector\n        {\n            **documents[i][\"metadata\"],  # Spread existing metadata (category, year, author)\n            \"text\": documents[i][\"text\"]  # Add the text! (Critical for RAG)\n        }\n    )\n    for i in range(len(documents))\n]\n\n# Upsert with namespace\nNAMESPACE = \"research\"\nindex.upsert(vectors=vectors, namespace=NAMESPACE)\n\nprint(f\"‚úÖ Upserted {len(vectors)} vectors to namespace '{NAMESPACE}'\")\nprint(\"   Each vector includes: embedding + metadata (category, year, author, TEXT)\")\n\n# Check stats\ntime.sleep(1)  # Wait for index to update\nstats = index.describe_index_stats()\nprint(f\"\\nüìä Index Stats:\")\nprint(f\"   Total vectors: {stats.get('total_vector_count', 0)}\")\nprint(f\"   Namespaces: {stats.get('namespaces', {})}\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectors for Pinecone\n",
    "vectors = [\n",
    "    (\n",
    "        f\"doc_{i}\",                    # ID\n",
    "        embeddings[i],                  # Embedding vector\n",
    "        documents[i][\"metadata\"]        # Metadata\n",
    "    )\n",
    "    for i in range(len(documents))\n",
    "]\n",
    "\n",
    "# Upsert with namespace\n",
    "NAMESPACE = \"research\"\n",
    "index.upsert(vectors=vectors, namespace=NAMESPACE)\n",
    "\n",
    "print(f\"‚úÖ Upserted {len(vectors)} vectors to namespace '{NAMESPACE}'\")\n",
    "\n",
    "# Check stats\n",
    "time.sleep(1)  # Wait for index to update\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nüìä Index Stats:\")\n",
    "print(f\"   Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "print(f\"   Namespaces: {stats.get('namespaces', {})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def search(query: str, top_k: int = 3, namespace: str = NAMESPACE, filter_dict: Dict = None) -> Dict:\n    \"\"\"\n    Search Pinecone for similar documents.\n    \n    Args:\n        query: Search query\n        top_k: Number of results to return\n        namespace: Namespace to search in\n        filter_dict: Optional metadata filter\n    \n    Returns:\n        Query results\n    \"\"\"\n    # Generate query embedding\n    query_emb = get_embeddings([query])[0]\n    \n    # Search\n    results = index.query(\n        vector=query_emb,\n        top_k=top_k,\n        namespace=namespace,\n        filter=filter_dict,\n        include_metadata=True\n    )\n    \n    return results\n\n# Test search\nquery = \"How do attention mechanisms work in neural networks?\"\nresults = search(query)\n\nprint(f\"Query: {query}\\n\")\nprint(\"Top 3 results:\\n\")\nfor i, match in enumerate(results['matches'], 1):\n    print(f\"{i}. Score: {match['score']:.4f}\")\n    print(f\"   Text: {match['metadata']['text']}\")\n    print(f\"   Category: {match['metadata']['category']}\")\n    print(f\"   Author: {match['metadata']['author']} ({match['metadata']['year']})\")\n    print(f\"   ID: {match['id']}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, top_k: int = 3, namespace: str = NAMESPACE, filter_dict: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Search Pinecone for similar documents.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        top_k: Number of results to return\n",
    "        namespace: Namespace to search in\n",
    "        filter_dict: Optional metadata filter\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_emb = get_embeddings([query])[0]\n",
    "    \n",
    "    # Search\n",
    "    results = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        namespace=namespace,\n",
    "        filter=filter_dict,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"How do attention mechanisms work in neural networks?\"\n",
    "results = search(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "for i, match in enumerate(results['matches'], 1):\n",
    "    print(f\"{i}. Score: {match['score']:.4f}\")\n",
    "    print(f\"   Category: {match['metadata']['category']}\")\n",
    "    print(f\"   Author: {match['metadata']['author']} ({match['metadata']['year']})\")\n",
    "    print(f\"   ID: {match['id']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Search only in LLM papers\nquery = \"What are recent advances in language models?\"\nresults = search(query, filter_dict={\"category\": {\"$eq\": \"llm\"}})\n\nprint(f\"Query: {query}\")\nprint(\"Filter: category = 'llm'\\n\")\nprint(\"Results:\\n\")\nfor i, match in enumerate(results['matches'], 1):\n    print(f\"{i}. {match['metadata']['author']} ({match['metadata']['year']})\")\n    print(f\"   Text: {match['metadata']['text']}\")\n    print(f\"   Score: {match['score']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search papers from 2020 or later\nquery = \"Recent AI research\"\nresults = search(query, top_k=5, filter_dict={\"year\": {\"$gte\": 2020}})\n\nprint(f\"Query: {query}\")\nprint(\"Filter: year >= 2020\\n\")\nprint(\"Results:\\n\")\nfor i, match in enumerate(results['matches'], 1):\n    print(f\"{i}. {match['metadata']['author']} ({match['metadata']['year']}) - {match['metadata']['category']}\")\n    print(f\"   Text: {match['metadata']['text']}\")\n    print(f\"   Score: {match['score']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Combined filters (AND logic)\nquery = \"Language model research\"\nresults = search(\n    query, \n    filter_dict={\n        \"category\": \"llm\",\n        \"year\": {\"$gte\": 2020}\n    }\n)\n\nprint(f\"Query: {query}\")\nprint(\"Filter: category='llm' AND year >= 2020\\n\")\nprint(\"Results:\\n\")\nfor i, match in enumerate(results['matches'], 1):\n    print(f\"{i}. {match['metadata']['author']} ({match['metadata']['year']})\")\n    print(f\"   Text: {match['metadata']['text']}\")\n    print(f\"   Score: {match['score']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined filters (AND logic)\n",
    "query = \"Language model research\"\n",
    "results = search(\n",
    "    query, \n",
    "    filter_dict={\n",
    "        \"category\": \"llm\",\n",
    "        \"year\": {\"$gte\": 2020}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Filter: category='llm' AND year >= 2020\\n\")\n",
    "print(\"Results:\\n\")\n",
    "for i, match in enumerate(results['matches'], 1):\n",
    "    print(f\"{i}. {match['metadata']['author']} ({match['metadata']['year']})\")\n",
    "    print(f\"   Score: {match['score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "class PineconeSearchEngine:\n    \"\"\"\n    Production-ready search engine using Pinecone.\n    \n    Features:\n    - Automatic embedding generation\n    - Namespace support for multi-tenancy\n    - Metadata filtering\n    - Batch operations\n    \"\"\"\n    \n    def __init__(self, pinecone_key: str, openai_key: str, index_name: str, namespace: str = \"default\"):\n        \"\"\"\n        Initialize the search engine.\n        \n        Args:\n            pinecone_key: Pinecone API key\n            openai_key: OpenAI API key\n            index_name: Name of Pinecone index\n            namespace: Default namespace\n        \"\"\"\n        self.pc = Pinecone(api_key=pinecone_key)\n        self.index = self.pc.Index(index_name)\n        self.client = openai.OpenAI(api_key=openai_key)\n        self.namespace = namespace\n    \n    def _embed(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Generate embeddings for texts.\"\"\"\n        response = self.client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=texts\n        )\n        return [item.embedding for item in response.data]\n    \n    def add_documents(self, documents: List[Dict], namespace: str = None) -> None:\n        \"\"\"\n        Add documents with automatic embedding.\n        \n        Args:\n            documents: List of dicts with 'text' and optional 'metadata'\n            namespace: Namespace to use (defaults to self.namespace)\n        \"\"\"\n        namespace = namespace or self.namespace\n        \n        # Extract texts and generate embeddings\n        texts = [doc[\"text\"] for doc in documents]\n        embeddings = self._embed(texts)\n        \n        # Prepare vectors\n        vectors = [\n            (\n                f\"doc_{i}_{namespace}\",\n                embeddings[i],\n                {\n                    **doc.get(\"metadata\", {}),\n                    \"text\": doc[\"text\"]  # Always include the text!\n                }\n            )\n            for i, doc in enumerate(documents)\n        ]\n        \n        # Upsert\n        self.index.upsert(vectors=vectors, namespace=namespace)\n        print(f\"‚úÖ Added {len(vectors)} documents to namespace '{namespace}'\")\n    \n    def search(self, query: str, top_k: int = 5, filter_dict: Dict = None, namespace: str = None) -> Dict:\n        \"\"\"\n        Search for similar documents.\n        \n        Args:\n            query: Search query\n            top_k: Number of results\n            filter_dict: Optional metadata filter\n            namespace: Namespace to search (defaults to self.namespace)\n        \n        Returns:\n            Dict with query and formatted results\n        \"\"\"\n        namespace = namespace or self.namespace\n        \n        # Generate query embedding\n        query_emb = self._embed([query])[0]\n        \n        # Search\n        results = self.index.query(\n            vector=query_emb,\n            top_k=top_k,\n            namespace=namespace,\n            filter=filter_dict,\n            include_metadata=True\n        )\n        \n        # Format results\n        return {\n            \"query\": query,\n            \"namespace\": namespace,\n            \"results\": [\n                {\n                    \"id\": match['id'],\n                    \"score\": match['score'],\n                    \"metadata\": match.get('metadata', {})\n                }\n                for match in results['matches']\n            ]\n        }\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Get index statistics.\"\"\"\n        return self.index.describe_index_stats()\n\nprint(\"‚úÖ PineconeSearchEngine class ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PineconeSearchEngine:\n",
    "    \"\"\"\n",
    "    Production-ready search engine using Pinecone.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic embedding generation\n",
    "    - Namespace support for multi-tenancy\n",
    "    - Metadata filtering\n",
    "    - Batch operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pinecone_key: str, openai_key: str, index_name: str, namespace: str = \"default\"):\n",
    "        \"\"\"\n",
    "        Initialize the search engine.\n",
    "        \n",
    "        Args:\n",
    "            pinecone_key: Pinecone API key\n",
    "            openai_key: OpenAI API key\n",
    "            index_name: Name of Pinecone index\n",
    "            namespace: Default namespace\n",
    "        \"\"\"\n",
    "        self.pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index = self.pc.Index(index_name)\n",
    "        self.client = openai.OpenAI(api_key=openai_key)\n",
    "        self.namespace = namespace\n",
    "    \n",
    "    def _embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for texts.\"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "        return [item.embedding for item in response.data]\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict], namespace: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Add documents with automatic embedding.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of dicts with 'text' and optional 'metadata'\n",
    "            namespace: Namespace to use (defaults to self.namespace)\n",
    "        \"\"\"\n",
    "        namespace = namespace or self.namespace\n",
    "        \n",
    "        # Extract texts and generate embeddings\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "        embeddings = self._embed(texts)\n",
    "        \n",
    "        # Prepare vectors\n",
    "        vectors = [\n",
    "            (\n",
    "                f\"doc_{i}_{namespace}\",\n",
    "                embeddings[i],\n",
    "                doc.get(\"metadata\", {})\n",
    "            )\n",
    "            for i, doc in enumerate(documents)\n",
    "        ]\n",
    "        \n",
    "        # Upsert\n",
    "        self.index.upsert(vectors=vectors, namespace=namespace)\n",
    "        print(f\"‚úÖ Added {len(vectors)} documents to namespace '{namespace}'\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5, filter_dict: Dict = None, namespace: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Search for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            top_k: Number of results\n",
    "            filter_dict: Optional metadata filter\n",
    "            namespace: Namespace to search (defaults to self.namespace)\n",
    "        \n",
    "        Returns:\n",
    "            Dict with query and formatted results\n",
    "        \"\"\"\n",
    "        namespace = namespace or self.namespace\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_emb = self._embed([query])[0]\n",
    "        \n",
    "        # Search\n",
    "        results = self.index.query(\n",
    "            vector=query_emb,\n",
    "            top_k=top_k,\n",
    "            namespace=namespace,\n",
    "            filter=filter_dict,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"namespace\": namespace,\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"id\": match['id'],\n",
    "                    \"score\": match['score'],\n",
    "                    \"metadata\": match.get('metadata', {})\n",
    "                }\n",
    "                for match in results['matches']\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get index statistics.\"\"\"\n",
    "        return self.index.describe_index_stats()\n",
    "\n",
    "print(\"‚úÖ PineconeSearchEngine class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Initialize search engine\nengine = PineconeSearchEngine(\n    pinecone_key=PINECONE_API_KEY,\n    openai_key=OPENAI_API_KEY,\n    index_name=INDEX_NAME,\n    namespace=\"research\"\n)\n\n# Search\nresult = engine.search(\"transformers in AI\", top_k=3)\n\nprint(f\"Query: {result['query']}\")\nprint(f\"Namespace: {result['namespace']}\\n\")\nprint(\"Results:\\n\")\nfor i, r in enumerate(result['results'], 1):\n    print(f\"{i}. {r['metadata']['author']} ({r['metadata']['year']})\")\n    print(f\"   Category: {r['metadata']['category']}\")\n    print(f\"   Text: {r['metadata']['text']}\")\n    print(f\"   Score: {r['score']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search with filter\nresult = engine.search(\n    \"computer vision research\",\n    top_k=3,\n    filter_dict={\"category\": \"cv\"}\n)\n\nprint(f\"Query: {result['query']}\")\nprint(\"Filter: category='cv'\\n\")\nprint(\"Results:\\n\")\nfor i, r in enumerate(result['results'], 1):\n    print(f\"{i}. {r['metadata']['author']} ({r['metadata']['year']})\")\n    print(f\"   Text: {r['metadata']['text']}\")\n    print(f\"   Score: {r['score']:.4f}\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with filter\n",
    "result = engine.search(\n",
    "    \"computer vision research\",\n",
    "    top_k=3,\n",
    "    filter_dict={\"category\": \"cv\"}\n",
    ")\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(\"Filter: category='cv'\\n\")\n",
    "print(\"Results:\\n\")\n",
    "for i, r in enumerate(result['results'], 1):\n",
    "    print(f\"{i}. {r['metadata']['author']} ({r['metadata']['year']})\")\n",
    "    print(f\"   Score: {r['score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stats\n",
    "stats = engine.get_stats()\n",
    "print(\"üìä Index Statistics:\\n\")\n",
    "print(f\"Total vectors: {stats.get('total_vector_count', 0)}\")\n",
    "print(f\"Namespaces: {list(stats.get('namespaces', {}).keys())}\")\n",
    "for ns, info in stats.get('namespaces', {}).items():\n",
    "    print(f\"  - {ns}: {info.get('vector_count', 0)} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Advanced Features & Free Tier Optimization\n",
    "\n",
    "### Hybrid Search (Preview)\n",
    "\n",
    "Pinecone supports **hybrid search** - combining semantic and keyword search:\n",
    "\n",
    "**Semantic search** (what we did):\n",
    "- Finds by meaning\n",
    "- \"ML\" matches \"machine learning\"\n",
    "- Uses dense vectors (OpenAI embeddings)\n",
    "\n",
    "**Keyword search**:\n",
    "- Exact terms: \"GPT-3\" finds documents with \"GPT-3\"\n",
    "- Uses sparse vectors (BM25 weights)\n",
    "\n",
    "**Hybrid = Semantic + Keyword**\n",
    "\n",
    "Pinecone's hybrid search combines:\n",
    "- **Dense vectors**: OpenAI embedding (semantic similarity)\n",
    "- **Sparse vectors**: BM25 keyword weights (exact matches)\n",
    "\n",
    "**Note**: Hybrid search requires the `pinecone-text` library and additional setup. The free tier supports it!\n",
    "\n",
    "**When to use**:\n",
    "- Legal/medical documents (exact terminology matters)\n",
    "- Code search (function names, variable names)\n",
    "- Product search (model numbers, SKUs)\n",
    "\n",
    "**Learn more**: [Pinecone Hybrid Search Guide](https://docs.pinecone.io/guides/data/understanding-hybrid-search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Tier Optimization Strategies\n",
    "\n",
    "**Limits**: 1 index, 100K vectors, 5 QPS\n",
    "\n",
    "**How to maximize your free tier**:\n",
    "\n",
    "#### 1. Use Namespaces (Not Multiple Indexes)\n",
    "\n",
    "‚ùå **Wrong**: Create multiple indexes\n",
    "```python\n",
    "# This won't work on free tier!\n",
    "pc.create_index(\"customer-1\")\n",
    "pc.create_index(\"customer-2\")  # Error: free tier = 1 index\n",
    "```\n",
    "\n",
    "‚úÖ **Right**: Use namespaces\n",
    "```python\n",
    "# Single index, multiple namespaces\n",
    "index.upsert(vectors=[...], namespace=\"customer-1\")\n",
    "index.upsert(vectors=[...], namespace=\"customer-2\")\n",
    "```\n",
    "\n",
    "#### 2. Chunk Size Optimization\n",
    "\n",
    "**Calculate your capacity**:\n",
    "```\n",
    "100K vectors √∑ chunks per document = number of documents\n",
    "\n",
    "Example:\n",
    "- 512 chars/chunk ‚Üí ~4K chunks per 2MB doc\n",
    "- 100K √∑ 4K = ~25 documents (2MB each)\n",
    "```\n",
    "\n",
    "**Adjust based on your use case**:\n",
    "- Smaller chunks (256 chars) = more precise, fewer docs\n",
    "- Larger chunks (1024 chars) = less precise, more docs\n",
    "\n",
    "#### 3. Selective Metadata\n",
    "\n",
    "‚ùå **Don't store large metadata**:\n",
    "```python\n",
    "metadata = {\n",
    "    \"full_text\": \"...\",  # Don't duplicate text!\n",
    "    \"large_field\": \"...\"  # Keep <40KB per vector\n",
    "}\n",
    "```\n",
    "\n",
    "‚úÖ **Only filterable fields**:\n",
    "```python\n",
    "metadata = {\n",
    "    \"source\": \"paper.pdf\",\n",
    "    \"page\": 5,\n",
    "    \"category\": \"ai\",\n",
    "    \"year\": 2023\n",
    "}\n",
    "```\n",
    "\n",
    "#### 4. Batch Upserts\n",
    "\n",
    "Batch up to **100 vectors** per upsert for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_upsert(index, vectors: List, namespace: str, batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Efficiently upsert vectors in batches.\n",
    "    \n",
    "    Args:\n",
    "        index: Pinecone index\n",
    "        vectors: List of (id, embedding, metadata) tuples\n",
    "        namespace: Namespace to use\n",
    "        batch_size: Vectors per batch (max 100)\n",
    "    \"\"\"\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i + batch_size]\n",
    "        index.upsert(vectors=batch, namespace=namespace)\n",
    "        print(f\"‚úÖ Batch {i//batch_size + 1}: {len(batch)} vectors\")\n",
    "\n",
    "print(\"üí° Use batch operations to improve performance and reduce API calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Cache Frequent Queries\n",
    "\n",
    "For frequently asked questions, cache results:\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "cache = {}  # Or use Redis/Memcached\n",
    "\n",
    "def cached_search(query):\n",
    "    if query in cache:\n",
    "        return cache[query]\n",
    "    \n",
    "    results = engine.search(query)\n",
    "    cache[query] = results\n",
    "    return results\n",
    "```\n",
    "\n",
    "This reduces QPS usage and improves response time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: ChromaDB vs Pinecone\n",
    "\n",
    "| Metric | ChromaDB (local) | Pinecone (cloud) |\n",
    "|--------|------------------|------------------|\n",
    "| **Query Latency** | <10ms | 50-100ms |\n",
    "| **Scalability** | Manual (upgrade server) | Automatic (serverless) |\n",
    "| **Concurrent Users** | Limited by your server | Excellent (auto-scaling) |\n",
    "| **Cold Start** | None | ~100ms (first query) |\n",
    "| **Setup Time** | Instant | ~2 minutes |\n",
    "| **Maintenance** | You manage | Fully managed |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**ChromaDB**: Faster queries, but you handle scaling  \n",
    "**Pinecone**: Slightly higher latency, but zero-ops scaling\n",
    "\n",
    "For most production apps, the **50-100ms latency** is acceptable given the operational benefits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Summary & Best Practices\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "‚úÖ **Decision framework** - When to use ChromaDB vs Pinecone vs hybrid  \n",
    "‚úÖ **Serverless architecture** - Auto-scaling managed infrastructure  \n",
    "‚úÖ **Namespaces** - Multi-tenancy within single index (free tier hack!)  \n",
    "‚úÖ **Metadata filtering** - Rich query operators ($eq, $gte, $in)  \n",
    "‚úÖ **Production patterns** - PineconeSearchEngine class  \n",
    "‚úÖ **Free tier optimization** - Maximize 100K vector limit\n",
    "\n",
    "### Key Differences Recap\n",
    "\n",
    "| Feature | ChromaDB | Pinecone |\n",
    "|---------|----------|----------|\n",
    "| **Architecture** | Self-hosted | Managed cloud |\n",
    "| **Multi-tenancy** | Collections | **Namespaces** |\n",
    "| **Scaling** | Manual | Automatic |\n",
    "| **Cost** | Free (DIY) | Free tier + paid |\n",
    "| **Best For** | Dev/prototyping | Production/scale |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "#### 1. Index Configuration\n",
    "\n",
    "‚úÖ Match dimension to embedding model (1536 for OpenAI)  \n",
    "‚úÖ Use `cosine` metric for most cases  \n",
    "‚úÖ Start with serverless (auto-scaling)  \n",
    "‚ùå Don't create multiple indexes on free tier\n",
    "\n",
    "#### 2. Namespaces\n",
    "\n",
    "‚úÖ Plan namespace strategy early (`customer-{id}`, `org-{name}`)  \n",
    "‚úÖ Use consistent naming (lowercase, hyphens)  \n",
    "‚úÖ Document your namespace schema  \n",
    "‚ùå Don't mix data from different tenants\n",
    "\n",
    "#### 3. Metadata\n",
    "\n",
    "‚úÖ Only store filterable fields  \n",
    "‚úÖ Keep metadata <40KB per vector  \n",
    "‚úÖ Use consistent field names  \n",
    "‚ùå Don't duplicate the text in metadata\n",
    "\n",
    "#### 4. Queries\n",
    "\n",
    "‚úÖ Set appropriate `top_k` (3-10 for most cases)  \n",
    "‚úÖ Use metadata filters when possible  \n",
    "‚úÖ Cache frequent queries  \n",
    "‚ùå Don't query without filters if you have multi-tenant data\n",
    "\n",
    "#### 5. Free Tier\n",
    "\n",
    "‚úÖ Monitor vector count (100K max)  \n",
    "‚úÖ Use namespaces, not multiple indexes  \n",
    "‚úÖ Optimize chunk sizes for your use case  \n",
    "‚úÖ Batch upsert operations (up to 100 vectors)  \n",
    "‚ùå Don't exceed QPS limits (5/second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Pitfalls\n",
    "\n",
    "‚ùå **Creating multiple indexes** (free tier = 1)  \n",
    "‚Üí ‚úÖ Use namespaces instead\n",
    "\n",
    "‚ùå **Mismatched dimensions** (index vs embedding model)  \n",
    "‚Üí ‚úÖ Match exactly (1536 for OpenAI small)\n",
    "\n",
    "‚ùå **Forgetting namespace in queries**  \n",
    "‚Üí ‚úÖ Specify namespace or use default\n",
    "\n",
    "‚ùå **Large metadata (>40KB per vector)**  \n",
    "‚Üí ‚úÖ Store only filterable fields\n",
    "\n",
    "‚ùå **Single-vector upserts** (slow)  \n",
    "‚Üí ‚úÖ Batch up to 100 vectors\n",
    "\n",
    "‚ùå **Not monitoring usage** (exceed free tier)  \n",
    "‚Üí ‚úÖ Check stats regularly with `get_stats()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "**Official Documentation**:\n",
    "- [Pinecone Docs](https://docs.pinecone.io/)\n",
    "- [Serverless Indexes Guide](https://docs.pinecone.io/guides/indexes/understanding-indexes)\n",
    "- [Metadata Filtering](https://docs.pinecone.io/guides/data/filter-with-metadata)\n",
    "- [Hybrid Search](https://docs.pinecone.io/guides/data/understanding-hybrid-search)\n",
    "\n",
    "**Tutorials**:\n",
    "- [Pinecone Quickstart](https://docs.pinecone.io/guides/get-started/quickstart)\n",
    "- [Production Best Practices](https://docs.pinecone.io/guides/production/best-practices)\n",
    "\n",
    "**Community**:\n",
    "- [Pinecone Community Forum](https://community.pinecone.io/)\n",
    "- [Pinecone GitHub Examples](https://github.com/pinecone-io/examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps in This Course\n",
    "\n",
    "**In upcoming notebooks**:\n",
    "- **Notebook 04**: LangChain + Pinecone integration\n",
    "- **Notebook 07**: Advanced RAG with Pinecone\n",
    "- **LAB1**: Build a RAG chatbot with Pinecone backend\n",
    "\n",
    "**You're now ready for**:\n",
    "- Production RAG applications\n",
    "- Multi-tenant SaaS products\n",
    "- Scalable semantic search\n",
    "- Hybrid search architectures\n",
    "\n",
    "### Cleanup (Optional)\n",
    "\n",
    "If you want to delete the index to stay within free tier limits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the index\n",
    "# pc.delete_index(INDEX_NAME)\n",
    "# print(f\"‚úÖ Deleted index '{INDEX_NAME}'\")\n",
    "\n",
    "print(\"üí° Keep the index if you want to use it in future notebooks!\")\n",
    "print(\"üí° Free tier includes 1 index, so you can keep it without charges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You now understand:\n",
    "\n",
    "‚úÖ When to use Pinecone vs ChromaDB  \n",
    "‚úÖ How to build production-ready search with Pinecone  \n",
    "‚úÖ Namespaces for multi-tenancy  \n",
    "‚úÖ Metadata filtering for precise queries  \n",
    "‚úÖ Free tier optimization strategies\n",
    "\n",
    "**You're ready to build scalable, production-grade semantic search applications!**\n",
    "\n",
    "See you in the next notebook! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}