{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# LangChain Essentials\n\n---\n\n## What You'll Learn\n\nThis notebook covers everything you need to know about LangChain for building production LLM applications:\n\n1. ‚úÖ What is LangChain and when to use it\n2. ‚úÖ LCEL (LangChain Expression Language) - pipe syntax fundamentals\n3. ‚úÖ Document loaders and text splitters\n4. ‚úÖ Vector stores integration\n5. ‚úÖ Building RAG pipelines\n6. ‚úÖ Advanced LCEL patterns (sequential, parallel, branching)\n7. ‚úÖ Conversation memory with RunnableWithMessageHistory\n8. ‚úÖ Production patterns and best practices\n\n## Prerequisites\n\n‚úÖ Completed environment setup notebook  \n‚úÖ OpenAI API key  \n‚úÖ Understanding of embeddings (helpful but not required)  \n\n## What is LangChain?\n\nLangChain is a framework for building LLM-powered applications. It provides:\n- Pre-built components for common tasks\n- Composable abstractions (chains, retrievers, memory)\n- Production-ready patterns\n\n**When to use LangChain**:\n- Building RAG applications\n- Creating conversational AI\n- Implementing document processing pipelines\n- Need for reusable, testable components\n\n**When NOT to use LangChain**:\n- Very simple single LLM calls (use direct API)\n- Maximum performance critical (abstractions add overhead)\n- Need for very custom logic (abstractions might be restrictive)\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Package Installation\n",
    "\n",
    "LangChain packages are now modular. We need separate packages for different integrations.\n",
    "\n",
    "### Packages We'll Install:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|----------|\n",
    "| `langchain` | Core framework |\n",
    "| `langchain-openai` | OpenAI integrations (ChatOpenAI, embeddings) |\n",
    "| `langchain-chroma` | ChromaDB integration |\n",
    "| `langchain-community` | Community integrations (loaders, FAISS) |\n",
    "| `langchain-text-splitters` | Text splitting |\n",
    "| `pypdf` | PDF parsing backend |\n",
    "| `chromadb` | Vector database client |\n",
    "\n",
    "Let's install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall existing langchain packages (clean slate)\n",
    "!pip uninstall -y langchain langchain-core langchain-community langchain-openai langchain-chroma langchain-text-splitters\n",
    "\n",
    "# Install with compatible versions (let pip resolve dependencies)\n",
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    langchain-openai \\\n",
    "    langchain-chroma \\\n",
    "    langchain-community \\\n",
    "    langchain-text-splitters \\\n",
    "    pypdf \\\n",
    "    chromadb\n",
    "\n",
    "# Show installed versions for verification\n",
    "!pip list | grep langchain\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "Let's verify that the packages are installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_openai\n",
    "import langchain_chroma\n",
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "import chromadb\n",
    "\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(f\"‚úÖ ChromaDB version: {chromadb.__version__}\")\n",
    "print(\"\\nüéâ All imports successful! Ready to build.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Verify LCEL Imports\n",
    "\n",
    "After installation, let's verify that the critical LCEL imports work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LCEL imports (modern LangChain approach)\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    print(\"‚úÖ LCEL imports successful!\")\n",
    "    print(\"‚ÑπÔ∏è  Note: LangChain uses LCEL (pipe syntax) as the standard approach\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"\\nüîß Troubleshooting:\")\n",
    "    print(\"1. Restart runtime\")\n",
    "    print(\"2. Re-run installation cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Setup API Keys\n",
    "\n",
    "We'll need an OpenAI API key for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "print(\"‚úÖ API key set successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: LCEL vs Traditional Chains\n",
    "\n",
    "## What is LCEL?\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is the way to build chains in LangChain.\n",
    "\n",
    "**Key features**:\n",
    "- Pipe syntax: `prompt | llm | parser`\n",
    "- Streaming built-in\n",
    "- Batch processing\n",
    "- Async support\n",
    "\n",
    "## Why LCEL?\n",
    "\n",
    "‚ùå **Old way (deprecated)**:\n",
    "```python\n",
    "from langchain.chains import LLMChain  # DEPRECATED\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "result = chain.run(\"question\")\n",
    "```\n",
    "\n",
    "‚úÖ **New way**:\n",
    "```python\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({\"question\": \"...\"}) \n",
    "```\n",
    "\n",
    "**Benefits**: Cleaner, more composable, streaming by default\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## LCEL Fundamentals\n",
    "\n",
    "Let's build a simple chain using LCEL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4zmp1lepec",
   "source": "## Understanding LCEL Core Concepts\n\nBefore we build our first chain, let's understand three foundational LCEL concepts.\n\n### 1. What is a Runnable?\n\nA **Runnable** is any component in LangChain that implements a standard interface with these methods:\n- `invoke()` - Process single input\n- `stream()` - Stream results\n- `batch()` - Process multiple inputs\n\n**Examples of Runnables**:\n- Prompts (`ChatPromptTemplate`)\n- LLMs (`ChatOpenAI`)\n- Output parsers (`StrOutputParser`)\n- Retrievers\n- Custom components\n\n**Key insight**: Everything in LCEL is a Runnable, so they all work the same way!\n\n### 2. The Pipe Operator (`|`)\n\nThe **pipe operator** (`|`) chains Runnables together:\n\n```python\nchain = prompt | llm | parser\n```\n\n**How it works**:\n1. Output of `prompt` becomes input to `llm`\n2. Output of `llm` becomes input to `parser`\n3. Final output is returned\n\n**Data flow**:\n```\nInput ‚Üí prompt (creates formatted message) ‚Üí llm (generates text) ‚Üí parser (extracts string) ‚Üí Output\n```\n\n**Why use pipes?**\n- ‚úÖ Clear data flow (left to right)\n- ‚úÖ Composable (mix and match components)\n- ‚úÖ Streaming built-in\n- ‚úÖ Error handling automatic\n\n### 3. What is StrOutputParser?\n\n**StrOutputParser** extracts the text content from LLM responses.\n\n**Without parser**:\n```python\nresult = llm.invoke(...)\n# Returns: AIMessage(content=\"text here\", ...)\n# Need to access: result.content\n```\n\n**With parser**:\n```python\nresult = (llm | StrOutputParser()).invoke(...)\n# Returns: \"text here\"  (just the string!)\n```\n\n**Why use it?**\n- Simplifies code (no need to access `.content`)\n- Consistent output format\n- Works with all LangChain LLMs\n\nNow let's see these concepts in action!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LCEL Example\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define components\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that explains concepts concisely.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Compose with pipe syntax\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke\n",
    "result = chain.invoke({\"question\": \"What is LCEL in one sentence?\"})\n",
    "\n",
    "print(\"Answer:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### LCEL Supports Streaming\n",
    "\n",
    "Streaming works out of the box with LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL supports streaming out of the box\n",
    "print(\"Streaming response:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"Explain transformers in 3 sentences.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Streaming works automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### LCEL Supports Batch Processing\n",
    "\n",
    "Process multiple inputs at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL supports batch processing\n",
    "questions = [\n",
    "    {\"question\": \"What is Python?\"},\n",
    "    {\"question\": \"What is JavaScript?\"},\n",
    "    {\"question\": \"What is TypeScript?\"}\n",
    "]\n",
    "\n",
    "results = chain.batch(questions)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result[:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **LCEL is the standard** - use pipe syntax for all new chains  \n",
    "‚úÖ **Composable**: `prompt | llm | parser` - clear data flow  \n",
    "‚úÖ **Streaming & batch** - built-in without extra code  \n",
    "‚úÖ **Replace legacy chains** - LLMChain, ConversationChain are deprecated  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Advanced LCEL Patterns\n",
    "\n",
    "Learn how to build complex chains:\n",
    "- Sequential chains (multi-step)\n",
    "- Parallel chains (concurrent execution)\n",
    "- Branching logic (conditional)\n",
    "\n",
    "## Sequential Chains\n",
    "\n",
    "Chain multiple steps where each step feeds into the next:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmnwfymzlk",
   "source": "## Understanding RunnablePassthrough\n\n`RunnablePassthrough` is a special Runnable that passes data through unchanged while wrapping it in a dictionary.\n\n### What It Does\n\n```python\n{\"text\": RunnablePassthrough()}\n# Input: \"Hello world\"\n# Output: {\"text\": \"Hello world\"}\n```\n\n### Why Do We Need It?\n\nPrompts expect dictionary inputs with named variables:\n\n```python\n# Prompt template expects:\nChatPromptTemplate.from_messages([\n    (\"human\", \"{text}\")  # ‚Üê Needs {\"text\": \"...\"}\n])\n```\n\nBut sometimes we have just a string from the previous step. RunnablePassthrough wraps it:\n\n```python\n# Without RunnablePassthrough:\n\"Hello\" | prompt  # ‚ùå Error: prompt expects dict\n\n# With RunnablePassthrough:\n{\"text\": RunnablePassthrough()} | prompt  # ‚úÖ Creates {\"text\": \"Hello\"}\n```\n\n### Common Patterns\n\n**Pattern 1: Wrap single input**\n```python\n{\"text\": RunnablePassthrough()}\n```\n\n**Pattern 2: Parallel composition** (RAG pattern from earlier!)\n```python\n{\n    \"context\": retriever | format_docs,\n    \"question\": RunnablePassthrough()\n}\n```\n\n**Pattern 3: Sequential wrapping** (what we're about to do!)\n```python\n{\"step1\": RunnablePassthrough()} | process | {\"step2\": RunnablePassthrough()}\n```\n\nüéØ **Key insight**: RunnablePassthrough = \"Wrap this data to fit the expected format\"\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain: Analyze ‚Üí Summarize\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Step 1: Analyze text\n",
    "analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Analyze the following text and extract key themes.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# Step 2: Summarize analysis\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize the following analysis in one sentence.\"),\n",
    "    (\"human\", \"{analysis}\")\n",
    "])\n",
    "\n",
    "# Build sequential chain\n",
    "sequential_chain = (\n",
    "    {\"text\": RunnablePassthrough()}\n",
    "    | analysis_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | {\"analysis\": RunnablePassthrough()}\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = sequential_chain.invoke(\n",
    "    \"Machine learning is transforming industries. From healthcare to finance, \"\n",
    "    \"AI systems are making predictions and automating decisions.\"\n",
    ")\n",
    "\n",
    "print(\"Final summary:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Parallel Chains\n",
    "\n",
    "Run multiple chains concurrently and combine results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel chains: Analyze text in 3 ways simultaneously\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Define 3 different analysis chains\n",
    "summary_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Summarize this text in one sentence.\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "sentiment_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"What is the sentiment of this text? (positive/negative/neutral)\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "keywords_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Extract 3 keywords from this text.\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run all 3 chains in parallel\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"sentiment\": sentiment_chain,\n",
    "    \"keywords\": keywords_chain\n",
    "})\n",
    "\n",
    "text = \"LangChain makes building AI applications incredibly easy and fun. The community is helpful and the documentation is excellent!\"\n",
    "\n",
    "results = parallel_chain.invoke({\"text\": text})\n",
    "\n",
    "print(\"Summary:\", results[\"summary\"])\n",
    "print(\"Sentiment:\", results[\"sentiment\"])\n",
    "print(\"Keywords:\", results[\"keywords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Branching with RunnableLambda\n",
    "\n",
    "Add custom logic for conditional routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching: Route based on text length\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def route_by_length(inputs):\n",
    "    \"\"\"Route to different prompts based on text length\"\"\"\n",
    "    text = inputs[\"text\"]\n",
    "    if len(text) < 100:\n",
    "        return {\"text\": text, \"instruction\": \"This is short. Expand on it.\"}\n",
    "    else:\n",
    "        return {\"text\": text, \"instruction\": \"This is long. Summarize it.\"}\n",
    "\n",
    "routing_chain = (\n",
    "    RunnableLambda(route_by_length)\n",
    "    | ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{instruction}\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "short_text = \"AI is the future.\"\n",
    "long_text = \"Artificial intelligence is revolutionizing every industry. From healthcare diagnostics to financial forecasting, AI systems are becoming indispensable tools for modern businesses.\"\n",
    "\n",
    "print(\"Short text result:\")\n",
    "print(routing_chain.invoke({\"text\": short_text}))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Long text result:\")\n",
    "print(routing_chain.invoke({\"text\": long_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Sequential**: Chain steps with `|` operator  \n",
    "‚úÖ **Parallel**: Use `RunnableParallel` for concurrent execution  \n",
    "‚úÖ **Branching**: Add custom logic with `RunnableLambda`  \n",
    "‚úÖ **Composable**: Mix and match patterns as needed  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Document Loading\n",
    "\n",
    "The first step in RAG is loading documents. LangChain provides **Document Loaders** for this.\n",
    "\n",
    "## What is a Document Loader?\n",
    "\n",
    "A Document Loader:\n",
    "- Reads files from various sources (PDF, TXT, web, databases)\n",
    "- Extracts text content\n",
    "- Preserves metadata (source, page numbers, etc.)\n",
    "\n",
    "## Document Structure\n",
    "\n",
    "Each loaded document has:\n",
    "- `page_content`: The actual text\n",
    "- `metadata`: Dictionary with source info (file path, page number, etc.)\n",
    "\n",
    "## Common Document Loaders\n",
    "\n",
    "| Loader | File Type | Use Case |\n",
    "|--------|-----------|----------|\n",
    "| `PyPDFLoader` | PDF | Research papers, reports |\n",
    "| `TextLoader` | TXT | Plain text files |\n",
    "| `DirectoryLoader` | Multiple files | Bulk loading |\n",
    "| `WebBaseLoader` | Web pages | Scrape websites |\n",
    "\n",
    "Let's load a sample document!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Create a Sample Document\n",
    "\n",
    "First, let's create a sample text file to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample document about Machine Learning\n",
    "sample_content = \"\"\"Machine Learning: A Comprehensive Guide\n",
    "\n",
    "Introduction to Machine Learning\n",
    "Machine learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from and make decisions based on data. Unlike traditional programming where rules are explicitly coded, machine learning algorithms learn patterns from data.\n",
    "\n",
    "Types of Machine Learning\n",
    "There are three main types of machine learning:\n",
    "\n",
    "1. Supervised Learning: The algorithm learns from labeled data. Examples include classification and regression tasks. Common algorithms include linear regression, logistic regression, decision trees, and neural networks.\n",
    "\n",
    "2. Unsupervised Learning: The algorithm finds patterns in unlabeled data. Examples include clustering and dimensionality reduction. Common algorithms include K-means clustering and principal component analysis (PCA).\n",
    "\n",
    "3. Reinforcement Learning: The algorithm learns through trial and error by receiving rewards or penalties. This is commonly used in robotics, game playing, and autonomous systems.\n",
    "\n",
    "Deep Learning\n",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks). It has revolutionized fields like computer vision, natural language processing, and speech recognition. Popular frameworks include TensorFlow, PyTorch, and Keras.\n",
    "\n",
    "Applications of Machine Learning\n",
    "Machine learning is used in various domains:\n",
    "- Healthcare: Disease diagnosis, drug discovery\n",
    "- Finance: Fraud detection, algorithmic trading\n",
    "- E-commerce: Recommendation systems, demand forecasting\n",
    "- Transportation: Autonomous vehicles, route optimization\n",
    "- Natural Language Processing: Chatbots, translation, sentiment analysis\n",
    "\n",
    "Challenges in Machine Learning\n",
    "Despite its success, machine learning faces several challenges:\n",
    "- Data quality and quantity requirements\n",
    "- Model interpretability and explainability\n",
    "- Bias and fairness concerns\n",
    "- Computational resource requirements\n",
    "- Overfitting and generalization issues\n",
    "\n",
    "The Future of Machine Learning\n",
    "The field continues to evolve with trends like AutoML, federated learning, and edge AI. As computing power increases and algorithms improve, machine learning will become even more integral to our daily lives.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"ml_guide.txt\", \"w\") as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(\"‚úÖ Sample document created: ml_guide.txt\")\n",
    "print(f\"Document length: {len(sample_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### Load Document with TextLoader\n",
    "\n",
    "Now let's load our sample document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"ml_guide.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} document(s)\")\n",
    "print(f\"\\nDocument structure:\")\n",
    "print(f\"- page_content: {len(documents[0].page_content)} characters\")\n",
    "print(f\"- metadata: {documents[0].metadata}\")\n",
    "\n",
    "print(f\"\\nFirst 300 characters:\")\n",
    "print(documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Understanding Document Metadata\n",
    "\n",
    "Metadata is crucial for RAG because it enables:\n",
    "- **Citations**: Show users where answers came from\n",
    "- **Filtering**: Search only specific sources\n",
    "- **Tracking**: Monitor which documents are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect metadata\n",
    "for doc in documents:\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ Document loaders extract text and preserve metadata  \n",
    "‚úÖ Use `langchain_community.document_loaders` for imports  \n",
    "‚úÖ Each document has `page_content` (text) and `metadata` (source info)  \n",
    "‚úÖ Metadata enables citations and filtering  \n",
    "\n",
    "**Next**: We'll chunk these documents into smaller pieces for better retrieval!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Text Splitting\n",
    "\n",
    "Now let's see how **LangChain** makes text splitting production-ready!\n",
    "\n",
    "## Why Use LangChain Text Splitters?\n",
    "\n",
    "| Manual Chunking | LangChain Splitters |\n",
    "|-----------------|---------------------|\n",
    "| Write chunking logic yourself | Pre-built, tested splitters |\n",
    "| Basic fixed-size or sentence split | Intelligent recursive splitting |\n",
    "| Manual edge case handling | Handles edge cases automatically |\n",
    "| Good for learning | Production-ready |\n",
    "\n",
    "## RecursiveCharacterTextSplitter (Recommended)\n",
    "\n",
    "This splitter:\n",
    "- Tries to split on paragraphs (`\\n\\n`) first\n",
    "- Falls back to sentences (`. `)\n",
    "- Then words (` `)\n",
    "- Finally characters\n",
    "\n",
    "This preserves semantic meaning better!\n",
    "\n",
    "Text splitters are now in `langchain_text_splitters` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,  # Characters per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks (10%)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    ")\n",
    "\n",
    "# Split our documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split {len(documents)} document(s) into {len(chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(chunks[0].page_content[:200] + \"...\")\n",
    "print(f\"\\nChunk metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Why 512 Characters + 50 Overlap?\n",
    "\n",
    "‚úÖ **Sweet spot**: 512 chars ‚âà 128 tokens (good balance)  \n",
    "‚úÖ **Overlap**: Maintains context across chunks  \n",
    "‚úÖ **Not too small**: Enough context for LLM  \n",
    "‚úÖ **Not too large**: Precise retrieval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect chunk sizes\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "print(f\"Chunk size statistics:\")\n",
    "print(f\"- Average: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "print(f\"- Min: {min(chunk_sizes)} characters\")\n",
    "print(f\"- Max: {max(chunk_sizes)} characters\")\n",
    "print(f\"\\nAll chunks have metadata: {all(chunk.metadata for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ LangChain splitters are **production-ready**  \n",
    "‚úÖ Use `langchain_text_splitters.RecursiveCharacterTextSplitter`  \n",
    "‚úÖ 512 characters + 50 overlap is a good default  \n",
    "‚úÖ Splitters preserve metadata automatically  \n",
    "\n",
    "**Next**: We'll store these chunks in a vector database!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Vector Stores with LangChain\n",
    "\n",
    "Now let's use LangChain's wrapper for ChromaDB!\n",
    "\n",
    "Chroma is now in `langchain_chroma` package:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eq16o6z0syo",
   "source": "## Understanding Vector Embeddings\n\nBefore we create a vector store, let's understand what **embeddings** are and why they're crucial for RAG.\n\n### What are Embeddings?\n\n**Embeddings** are numerical representations of text that capture semantic meaning.\n\n```\nText: \"Machine learning is amazing\"\n‚Üì\nEmbedding: [0.234, -0.891, 0.542, ..., 0.123]  (1536 numbers)\n```\n\n### Why Numbers?\n\nComputers can't understand text directly, but they can:\n- Compare numbers\n- Calculate similarity\n- Search efficiently\n\n### How Similarity Works\n\nSimilar texts have similar embeddings:\n\n```\n\"AI is transforming healthcare\" ‚Üí [0.8, 0.2, ...]\n\"Machine learning in medicine\"  ‚Üí [0.7, 0.3, ...]  (Similar!)\n\n\"I love pizza\" ‚Üí [-0.3, 0.9, ...]  (Very different!)\n```\n\n### Why Needed for RAG?\n\n1. **Semantic Search**: Find documents by meaning, not just keywords\n   - \"ML applications\" matches \"machine learning uses\"\n2. **Fast Retrieval**: Vector databases are optimized for similarity search\n3. **Better Context**: Retrieve truly relevant chunks\n\n### The RAG Flow with Embeddings\n\n```\n1. Chunk documents ‚Üí 2. Generate embeddings ‚Üí 3. Store in vector DB\n‚Üì\nUser question ‚Üí Embed question ‚Üí Find similar chunks ‚Üí Send to LLM\n```\n\n**Model**: We'll use `text-embedding-3-small` (fast + accurate for most use cases)\n\nNow let's create embeddings and store them!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"langchain_essentials\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created vector store with {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "### Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search\n",
    "results = vector_store.similarity_search(\"What is deep learning?\", k=3)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ LangChain provides wrappers for vector databases  \n",
    "‚úÖ Use `langchain_chroma.Chroma` for ChromaDB  \n",
    "‚úÖ `from_documents` creates store and embeds in one step  \n",
    "‚úÖ Supports similarity search out of the box  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: RAG Pipeline Fundamentals\n",
    "\n",
    "## Building a Complete RAG Pipeline with LCEL\n",
    "\n",
    "Now let's combine everything into a complete RAG system using LCEL:\n",
    "\n",
    "**LCEL RAG Pattern**:\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "Let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3z1uz5do64j",
   "source": "## Understanding Retrievers\n\nA **Retriever** is a Runnable that fetches relevant documents based on a query.\n\n### Vector Store vs Retriever\n\n**Vector Store**: Storage + search capabilities\n```python\nvector_store.similarity_search(\"query\", k=3)  # Manual search\n```\n\n**Retriever**: Runnable interface for vector store\n```python\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\nretriever.invoke(\"query\")  # Same as similarity_search, but Runnable!\n```\n\n### Why Use Retriever?\n\n**Because it's a Runnable**, you can use it in LCEL chains:\n\n```python\n# Can pipe retriever into other components!\nchain = retriever | format_docs | prompt | llm\n```\n\n**Cannot do this with vector_store.similarity_search** (not a Runnable)\n\n### Search Parameters\n\n`search_kwargs={\"k\": 3}`:\n- `k`: Number of documents to retrieve\n- `k=3` means \"get top 3 most similar chunks\"\n\n**Trade-off**:\n- Higher k = More context, but more noise and cost\n- Lower k = Less context, but more focused\n\n**Good defaults**: k=3 for most RAG applications\n\nNow let's build a complete RAG chain using retrievers!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain using LCEL (LangChain Expression Language)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Setup components\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build LCEL chain with retriever\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created using LCEL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### Test the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain with LCEL\n",
    "question = \"What are the types of machine learning?\"\n",
    "\n",
    "# With LCEL, we invoke with the question directly (simpler!)\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(\"\\n‚úÖ LCEL makes RAG simple and clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "## Adding Citations\n",
    "\n",
    "Let's enhance our RAG to show source documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citations with LCEL\n",
    "def ask_with_citations(question):\n",
    "    \"\"\"Ask question and return answer with source citations\"\"\"\n",
    "    # Get answer from chain\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    # Get source documents separately\n",
    "    source_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format citations\n",
    "    citations = []\n",
    "    for i, doc in enumerate(source_docs, 1):\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        citations.append(f\"[{i}] {source}: {doc.page_content[:100]}...\")\n",
    "    \n",
    "    return f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(citations)\n",
    "\n",
    "# Test with citations\n",
    "result = ask_with_citations(\"What is deep learning?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "## Production-Ready RAG Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": "# Imports (global scope - best practice)\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nclass RAGPipeline:\n    \"\"\"Production-ready RAG pipeline using LCEL\"\"\"\n\n    def __init__(self, file_path):\n        # Load and chunk documents\n        loader = TextLoader(file_path)\n        documents = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n        chunks = text_splitter.split_documents(documents)\n\n        # Create vector store\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        self.vector_store = Chroma.from_documents(chunks, embeddings)\n        self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n        # Helper to format docs\n        def format_docs(docs):\n            return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n        # Build LCEL chain\n        template = \"\"\"Answer based on context:\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n        prompt = ChatPromptTemplate.from_template(template)\n\n        self.rag_chain = (\n            {\"context\": self.retriever | format_docs, \"question\": RunnablePassthrough()}\n            | prompt\n            | self.llm\n            | StrOutputParser()\n        )\n\n        print(f\"‚úÖ RAG Pipeline ready with {len(chunks)} chunks (using LCEL)\")\n\n    def ask(self, question):\n        \"\"\"Ask a question and get answer with sources\"\"\"\n        answer = self.rag_chain.invoke(question)\n        # Get source docs separately for citations\n        docs = self.retriever.invoke(question)\n        return {\"answer\": answer, \"sources\": docs}\n\n# Create pipeline\npipeline = RAGPipeline(\"ml_guide.txt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "response = pipeline.ask(\"What are machine learning challenges?\")\n",
    "\n",
    "print(\"Answer:\", response[\"answer\"])\n",
    "print(f\"\\n‚úÖ Retrieved {len(response['sources'])} source documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **LCEL makes RAG simple**: `{context: retriever, question} | prompt | llm`  \n",
    "‚úÖ **Retriever integration**: Vector store becomes retriever  \n",
    "‚úÖ **Citations**: Retrieve source docs for attribution  \n",
    "‚úÖ **Production-ready**: Encapsulate in reusable class  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": "---\n\n# Section 8: Conversation Memory with RunnableWithMessageHistory\n\n## Understanding Conversation Memory\n\nLLMs are **stateless** - they don't remember previous interactions. We need to add memory using:\n\n**RunnableWithMessageHistory** - the approach for conversational apps\n\n## Memory Patterns\n\n| Pattern | Implementation | Use Case |\n|---------|----------------|----------|\n| **Full History** | RunnableWithMessageHistory + ChatMessageHistory | Short conversations |\n| **Sliding Window** | RunnableWithMessageHistory + custom windowed history | Keep last N messages |\n| **Summarization** | Custom summarization logic | Long conversations |\n| **LangGraph** | LangGraph with built-in checkpointers | Complex multi-agent apps |\n\nLet's implement conversational memory!"
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## Full History Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gor2w8h4dhg",
   "source": "## Understanding Conversation Memory Components\n\nTo add memory to chains, we need three components working together.\n\n### 1. MessagesPlaceholder\n\n**What**: Placeholder in prompt template for chat history\n\n```python\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are helpful\"),\n    MessagesPlaceholder(variable_name=\"history\"),  # ‚Üê Chat history goes here\n    (\"human\", \"{question}\")\n])\n```\n\n**Why needed**: Prompts are static, but chat history is dynamic (grows with each turn)\n\n### 2. ChatMessageHistory\n\n**What**: Stores conversation messages (user questions + AI responses)\n\n```python\nhistory = ChatMessageHistory()\nhistory.add_user_message(\"Hi!\")\nhistory.add_ai_message(\"Hello! How can I help?\")\n# history.messages = [HumanMessage(\"Hi!\"), AIMessage(\"Hello...\")]\n```\n\n**Why needed**: Need to remember previous messages to maintain context\n\n### 3. RunnableWithMessageHistory\n\n**What**: Wraps any chain to automatically manage chat history\n\n```python\nchain_with_history = RunnableWithMessageHistory(\n    chain,                      # Your LCEL chain\n    get_session_history,        # Function to get/create history\n    input_messages_key=\"question\",   # What user types\n    history_messages_key=\"history\"   # Where history goes in prompt\n)\n```\n\n**How it works**:\n1. User sends message\n2. RunnableWithMessageHistory retrieves session history\n3. Injects history into prompt's MessagesPlaceholder\n4. Chain executes with full context\n5. Saves new messages to history\n\n### The get_session_history Function\n\n**Why a function?**: Different conversations need different histories\n\n```python\nstore = {}  # session_id ‚Üí ChatMessageHistory\n\ndef get_session_history(session_id: str):\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n```\n\n**Multi-user example**:\n- Alice's session: `session_id=\"alice\"` ‚Üí separate history\n- Bob's session: `session_id=\"bob\"` ‚Üí separate history\n\nNow let's see these components in action!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern approach: RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# In-memory store (for demo purposes)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Retrieve or create chat history for a session\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Build conversational chain with LCEL\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Wrap with message history management\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qaiyaf59tvc",
   "source": "### üí° Note: LangGraph for Complex Applications\n\n**What you just learned** (RunnableWithMessageHistory) is the **correct approach for LCEL-based conversational applications**.\n\nHowever, for **complex applications** with advanced state management needs, LangGraph provides an alternative approach:\n\n**Use RunnableWithMessageHistory when**:\n- ‚úÖ Building LCEL chains with conversation memory\n- ‚úÖ Simple to moderate conversational flows\n- ‚úÖ You want explicit control over history management\n\n**Consider LangGraph when**:\n- ‚úÖ Building multi-agent systems\n- ‚úÖ Complex state management across multiple components\n- ‚úÖ Need built-in checkpointing and persistence\n- ‚úÖ Advanced features like \"time travel\" through conversation history\n\n**For this training**: We're using RunnableWithMessageHistory because it's the standard for LCEL chains and perfect for most RAG applications.\n\n**Learn more**: [LangGraph Memory Documentation](https://docs.langchain.com/oss/python/langgraph/memory)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "### Test Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b10qjxx",
   "source": "## Understanding Session Management\n\nWhen invoking a chain with memory, you need to specify which session to use.\n\n### The Config Pattern\n\n```python\nchain_with_history.invoke(\n    {\"question\": \"What's my name?\"},\n    config={\"configurable\": {\"session_id\": \"alice\"}}\n)\n```\n\n### Why This Structure?\n\n**`config`**: Reserved LangChain parameter for chain configuration\n\n**`\"configurable\"`**: Nested dict for runtime-configurable parameters\n- Parameters that change per invocation\n- Not part of the chain definition\n\n**`\"session_id\"`**: Your custom key to identify the conversation\n- LangChain passes this to `get_session_history()`\n- Different session_id = different conversation\n\n### Visual Flow\n\n```\ninvoke(question, config={\"configurable\": {\"session_id\": \"alice\"}})\n                                                ‚Üì\n                RunnableWithMessageHistory calls get_session_history(\"alice\")\n                                                ‚Üì\n                        Returns Alice's ChatMessageHistory\n                                                ‚Üì\n                    Injects Alice's history into prompt\n                                                ‚Üì\n                                Executes chain\n                                                ‚Üì\n                    Saves new messages to Alice's history\n```\n\n### Multi-Session Example\n\n```python\n# Alice's conversation\nchain_with_history.invoke(\n    {\"question\": \"My name is Alice\"},\n    config={\"configurable\": {\"session_id\": \"alice\"}}\n)\n\n# Bob's conversation (completely separate!)\nchain_with_history.invoke(\n    {\"question\": \"My name is Bob\"},\n    config={\"configurable\": {\"session_id\": \"bob\"}}\n)\n```\n\nNow let's test this pattern!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversational memory with sessions\n",
    "session_id = \"user-alice\"\n",
    "\n",
    "# Turn 1: Introduce yourself\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"question\": \"My name is Alice\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"User: My name is Alice\")\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "# Turn 2: Share profession\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"question\": \"I work as a data scientist\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"User: I work as a data scientist\")\n",
    "print(f\"Assistant: {response2}\\n\")\n",
    "\n",
    "# Turn 3: Test memory - ask about previous context\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"question\": \"What's my name and profession?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"User: What's my name and profession?\")\n",
    "print(f\"Assistant: {response3}\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory working! Assistant remembers Alice is a data scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "## Sliding Window Memory\n",
    "\n",
    "Keep only the last N messages to prevent unbounded memory growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window approach: Keep last N messages\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class SlidingWindowHistory(ChatMessageHistory):\n",
    "    \"\"\"Custom history that keeps only last N messages\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 4):\n",
    "        # Initialize with empty messages first\n",
    "        super().__init__()\n",
    "        # Store window size in a way that works with Pydantic\n",
    "        object.__setattr__(self, '_window_size', window_size)\n",
    "    \n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        \"\"\"Add a message and trim to window size\"\"\"\n",
    "        super().add_message(message)\n",
    "        # Keep only last N messages\n",
    "        if len(self.messages) > self._window_size:\n",
    "            # Update messages directly\n",
    "            self.messages = self.messages[-self._window_size:]\n",
    "\n",
    "# Create store with sliding window\n",
    "window_store = {}\n",
    "\n",
    "def get_window_history(session_id: str):\n",
    "    if session_id not in window_store:\n",
    "        window_store[session_id] = SlidingWindowHistory(window_size=4)  # Last 2 exchanges\n",
    "    return window_store[session_id]\n",
    "\n",
    "# Create chain with window\n",
    "chain_with_window = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_window_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Test: Add 3 exchanges, oldest should be dropped\n",
    "session = \"window-demo\"\n",
    "chain_with_window.invoke({\"question\": \"Hi\"}, config={\"configurable\": {\"session_id\": session}})\n",
    "chain_with_window.invoke({\"question\": \"My name is Bob\"}, config={\"configurable\": {\"session_id\": session}})\n",
    "chain_with_window.invoke({\"question\": \"I like Python\"}, config={\"configurable\": {\"session_id\": session}})\n",
    "\n",
    "# Check how many messages are kept\n",
    "history = get_window_history(session)\n",
    "print(f\"Window size: {len(history.messages)} messages (kept last 4)\")\n",
    "print(\"‚úÖ Oldest messages automatically dropped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ns1kuv4tt",
   "source": "## Persistent Storage for Production\n\nIn production, you'll want to persist conversation history beyond in-memory storage. Here's how to use SQLite for persistent message history:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7kjqazpyv5r",
   "source": "# Production: Persistent message history with SQLite\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory\n\n# Create store with SQLite persistence\ndef get_persistent_history(session_id: str):\n    \"\"\"Get or create persistent chat history stored in SQLite\"\"\"\n    return SQLChatMessageHistory(\n        session_id=session_id,\n        connection_string=\"sqlite:///chat_history.db\"  # Persists to file\n    )\n\n# Create chain with persistent storage\nchain_with_persistent_memory = RunnableWithMessageHistory(\n    chain,\n    get_persistent_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n\n# Test: Conversation persists even after restart!\nsession = \"persistent-demo\"\nresponse = chain_with_persistent_memory.invoke(\n    {\"question\": \"Remember: my favorite color is blue\"},\n    config={\"configurable\": {\"session_id\": session}}\n)\nprint(f\"Assistant: {response}\")\n\n# Simulate restart - history is still there!\nresponse2 = chain_with_persistent_memory.invoke(\n    {\"question\": \"What's my favorite color?\"},\n    config={\"configurable\": {\"session_id\": session}}\n)\nprint(f\"\\nAfter 'restart': {response2}\")\nprint(\"\\n‚úÖ Chat history persisted to database!\")\nprint(\"üíæ Check chat_history.db file for stored conversations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": "### Key Takeaways\n\n‚úÖ **RunnableWithMessageHistory**: Standard LCEL approach for conversational memory  \n‚úÖ **Session management**: Use `session_id` to track different conversations  \n‚úÖ **Full history**: ChatMessageHistory stores complete conversation  \n‚úÖ **Sliding window**: Custom history class to keep last N messages  \n‚úÖ **Persistent storage**: Use SQLChatMessageHistory for production (survives restarts)  \n‚úÖ **LangGraph alternative**: For complex multi-agent systems, consider LangGraph  \n\n**Production note**: For persistence beyond in-memory, use Redis, PostgreSQL, or other backends with LangChain's message history integrations.\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 9: Production Patterns & Best Practices\n",
    "\n",
    "## Production Best Practices\n",
    "\n",
    "**1. Observability**\n",
    "- ‚úÖ Use LangSmith for tracing and monitoring\n",
    "- ‚úÖ Log all operations\n",
    "- ‚úÖ Track token usage and costs\n",
    "\n",
    "**2. Error Handling**\n",
    "- ‚úÖ Try-except in critical paths\n",
    "- ‚úÖ Retry with exponential backoff\n",
    "- ‚úÖ Fallback paths for failures\n",
    "\n",
    "**3. Performance**\n",
    "- ‚úÖ Batch operations where possible\n",
    "- ‚úÖ Use streaming for long responses\n",
    "- ‚úÖ Cache embeddings when appropriate\n",
    "\n",
    "**4. Testing**\n",
    "- ‚úÖ Unit test each component\n",
    "- ‚úÖ Integration tests for full chains\n",
    "- ‚úÖ LLM-as-a-judge for quality evaluation\n",
    "\n",
    "## Error Handling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready RAG with error handling\n",
    "def safe_rag_query(question: str, max_retries: int = 3):\n",
    "    \"\"\"RAG query with error handling and retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt the query\n",
    "            answer = rag_chain.invoke(question)\n",
    "            sources = retriever.invoke(question)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"‚ö†Ô∏è  Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                print(f\"üîÑ Retrying... ({attempt + 2}/{max_retries})\")\n",
    "            else:\n",
    "                print(f\"‚ùå All retries exhausted\")\n",
    "                return {\n",
    "                    \"answer\": None,\n",
    "                    \"sources\": None,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "# Test\n",
    "result = safe_rag_query(\"What is supervised learning?\")\n",
    "if result[\"error\"]:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Sources: {len(result['sources'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "## Streaming for Better UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream RAG responses for better user experience\n",
    "print(\"Streaming RAG response:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chunk in rag_chain.stream(\"What are the applications of machine learning?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Streaming provides better UX for long responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-64",
   "metadata": {},
   "source": [
    "## Exercise: Build Your Own RAG\n",
    "\n",
    "**Task**: Create a RAG system for a domain of your choice\n",
    "\n",
    "**Steps**:\n",
    "1. Create a text file with domain knowledge\n",
    "2. Load and chunk the document\n",
    "3. Create a vector store\n",
    "4. Build a RAG chain with LCEL\n",
    "5. Add conversation memory\n",
    "6. Test with multiple questions\n",
    "\n",
    "**Bonus**:\n",
    "- Add error handling\n",
    "- Implement streaming\n",
    "- Add source citations\n",
    "\n",
    "Use the cells below for your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here\n",
    "# Step 1: Create your domain content and save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2-4: Build RAG pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Add conversation memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test your RAG system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 10: Summary & Next Steps\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "‚úÖ **LangChain fundamentals** - When to use it and when not to  \n",
    "‚úÖ **LCEL (pipe syntax)** - The way to build chains  \n",
    "‚úÖ **Advanced LCEL patterns** - Sequential, parallel, branching  \n",
    "‚úÖ **Document loading** - TextLoader and metadata handling  \n",
    "‚úÖ **Text splitting** - RecursiveCharacterTextSplitter  \n",
    "‚úÖ **Vector stores** - ChromaDB integration  \n",
    "‚úÖ **RAG pipelines** - Complete implementation with LCEL  \n",
    "‚úÖ **Conversation memory** - RunnableWithMessageHistory  \n",
    "‚úÖ **Production patterns** - Error handling, streaming, best practices  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **LCEL is the standard** - Use pipe syntax for all new LangChain code\n",
    "2. **Start simple** - Use direct APIs for simple tasks, LangChain for RAG and complex workflows\n",
    "3. **Memory patterns** - Choose full history or sliding window based on use case\n",
    "4. **Production-ready** - Add error handling, observability, and testing\n",
    "5. **Composability** - Mix and match LCEL patterns as needed\n",
    "\n",
    "## Resources\n",
    "\n",
    "**Official Documentation**:\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangSmith](https://www.langchain.com/langsmith) - Observability platform\n",
    "\n",
    "**Learn More**:\n",
    "- [LangChain Academy](https://academy.langchain.com/) - Free course\n",
    "- [LangChain Best Practices](https://www.swarnendu.de/blog/langchain-best-practices/)\n",
    "- [Building RAG Applications](https://blog.langchain.com/)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Practice**: Complete the exercise above\n",
    "2. **Experiment**: Try different LCEL patterns\n",
    "3. **Integrate**: Build a RAG chatbot for your domain\n",
    "4. **Production**: Add observability with LangSmith\n",
    "5. **Advanced**: Explore LangGraph for stateful workflows\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've mastered LangChain essentials! You can now:\n",
    "- Build production-ready RAG applications\n",
    "- Use LCEL to compose complex chains\n",
    "- Add conversation memory to chatbots\n",
    "- Apply best practices for production systems\n",
    "\n",
    "**You're ready for advanced topics: LangGraph, Multi-Agent Systems, and Function Calling!**\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}