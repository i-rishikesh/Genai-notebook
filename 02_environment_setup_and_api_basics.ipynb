{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Environment Setup & API Basics\n",
    "\n",
    "## GenAI Foundation Training Program\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Set up and authenticate with OpenAI, Anthropic (Claude), and Google AI (Gemini) APIs\n",
    "2. Make basic API calls to all three providers\n",
    "3. Generate **structured JSON outputs** using Pydantic models\n",
    "4. Implement **error handling** patterns for production use\n",
    "5. Compare providers and understand their differences\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Google Colab account (you're already here!)\n",
    "- API keys from:\n",
    "  - OpenAI (https://platform.openai.com/api-keys)\n",
    "  - Anthropic (https://console.anthropic.com/settings/keys)\n",
    "  - Google AI (https://aistudio.google.com/app/apikey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Introduction & Setup Overview\n",
    "\n",
    "### Google Colab vs Local Development\n",
    "\n",
    "**Google Colab** provides a cloud-based Jupyter notebook environment with:\n",
    "- ‚úÖ Pre-installed Python 3.10+\n",
    "- ‚úÖ Free GPU/TPU access\n",
    "- ‚úÖ No local setup required\n",
    "- ‚úÖ Easy collaboration and sharing\n",
    "- ‚úÖ Pre-configured environment\n",
    "\n",
    "**For local development**, you would typically:\n",
    "1. Install Python 3.10 or higher\n",
    "2. Create a virtual environment:\n",
    "   ```bash\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "   ```\n",
    "3. Install packages with pip\n",
    "\n",
    "**For this training, we'll use Google Colab exclusively** - everything is already set up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Package Installation\n",
    "\n",
    "Let's install all the required packages for working with the three major LLM providers.\n",
    "\n",
    "**Packages:**\n",
    "- `openai` - OpenAI's official Python SDK\n",
    "- `anthropic` - Anthropic's official Python SDK for Claude\n",
    "- `google-generativeai` - Google's Gemini API SDK\n",
    "- `pydantic` - Data validation and structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install -q openai anthropic google-generativeai pydantic\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Import All Required Libraries\n",
    "\n",
    "Let's import everything we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import time\n",
    "from typing import List, Optional, Literal, Any, Dict\n",
    "from enum import Enum\n",
    "\n",
    "# LLM Provider SDKs\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Pydantic for structured outputs\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Google Colab specific\n",
    "from google.colab import userdata\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: API Key Management & Security\n",
    "\n",
    "### üîí Security Best Practices\n",
    "\n",
    "**NEVER hardcode API keys in your code!** Instead:\n",
    "- ‚úÖ Use environment variables\n",
    "- ‚úÖ Use secrets management (Google Colab Secrets)\n",
    "- ‚úÖ Rotate keys regularly\n",
    "- ‚úÖ Set spending limits\n",
    "- ‚ùå Never commit keys to Git\n",
    "- ‚ùå Never log keys in console\n",
    "\n",
    "### Setting Up Google Colab Secrets\n",
    "\n",
    "1. Click the **üîë (key icon)** in the left sidebar\n",
    "2. Add these three secrets:\n",
    "   - `OPENAI_API_KEY` - Your OpenAI API key\n",
    "   - `ANTHROPIC_API_KEY` - Your Anthropic API key\n",
    "   - `GOOGLE_API_KEY` - Your Google AI API key\n",
    "3. Toggle **\"Notebook access\"** ON for each key\n",
    "\n",
    "### Cost Monitoring\n",
    "\n",
    "- OpenAI: https://platform.openai.com/usage\n",
    "- Anthropic: https://console.anthropic.com/settings/usage\n",
    "- Google AI: https://aistudio.google.com/app/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Validation Helper\n",
    "def validate_api_keys():\n",
    "    \"\"\"Validate that all required API keys are present in Colab secrets.\"\"\"\n",
    "    required_keys = ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GOOGLE_API_KEY']\n",
    "    missing_keys = []\n",
    "    \n",
    "    for key in required_keys:\n",
    "        try:\n",
    "            value = userdata.get(key)\n",
    "            if not value or len(value) < 10:\n",
    "                missing_keys.append(key)\n",
    "        except Exception:\n",
    "            missing_keys.append(key)\n",
    "    \n",
    "    if missing_keys:\n",
    "        print(\"‚ùå Missing or invalid API keys:\")\n",
    "        for key in missing_keys:\n",
    "            print(f\"   - {key}\")\n",
    "        print(\"\\n‚ö†Ô∏è  Please add these keys in Google Colab Secrets (üîë icon in sidebar)\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ All API keys validated successfully!\")\n",
    "        return True\n",
    "\n",
    "# Validate keys\n",
    "validate_api_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve API keys from Colab secrets\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    print(\"‚úÖ API keys loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading API keys: {e}\")\n",
    "    print(\"Please set up your API keys in Google Colab Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: OpenAI API Setup & First Calls\n\n### About OpenAI\n\n- **Models:** GPT-4o, GPT-4o-mini, GPT-4 Turbo\n- **Strengths:** Best-in-class reasoning, wide adoption, excellent documentation\n- **Use cases:** General-purpose tasks, coding, analysis, creative writing\n- **Pricing:** https://openai.com/api/pricing/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text completion example\n",
    "def simple_openai_call(prompt: str, model: str = \"gpt-4o-mini\"):\n",
    "    \"\"\"Make a simple OpenAI API call.\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"model\": response.model,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test it!\n",
    "result = simple_openai_call(\"Explain what a Large Language Model is in 2 sentences.\")\n",
    "print(\"üìù Response:\", result['content'])\n",
    "print(\"\\nüìä Tokens used:\", result['usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs with Pydantic (OpenAI)\n",
    "\n",
    "**Why structured outputs?**\n",
    "- Guarantees valid JSON response\n",
    "- Type safety and validation\n",
    "- Easier integration with downstream systems\n",
    "- Better error handling\n",
    "\n",
    "OpenAI supports **native Pydantic integration** with `response_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for product reviews\n",
    "class SentimentEnum(str, Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    \"\"\"Structured product review analysis.\"\"\"\n",
    "    sentiment: SentimentEnum = Field(description=\"Overall sentiment of the review\")\n",
    "    confidence_score: float = Field(ge=0.0, le=1.0, description=\"Confidence in sentiment (0-1)\")\n",
    "    key_points: List[str] = Field(description=\"Main points mentioned in review\")\n",
    "    pros: List[str] = Field(default_factory=list, description=\"Positive aspects\")\n",
    "    cons: List[str] = Field(default_factory=list, description=\"Negative aspects\")\n",
    "    recommended: bool = Field(description=\"Whether product is recommended\")\n",
    "\n",
    "print(\"‚úÖ ProductReview model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use structured output with OpenAI\n",
    "def analyze_review_openai(review_text: str) -> ProductReview:\n",
    "    \"\"\"Analyze product review using OpenAI with structured output.\"\"\"\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a product review analyzer. Extract structured insights from reviews.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Analyze this product review:\\n\\n{review_text}\"}\n",
    "        ],\n",
    "        response_format=ProductReview\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "# Test with a sample review\n",
    "sample_review = \"\"\"\n",
    "I've been using this laptop for 3 months now. The battery life is absolutely incredible - \n",
    "easily 12+ hours of real work. The display is stunning with great color accuracy. \n",
    "However, the trackpad is way too sensitive and causes accidental clicks. \n",
    "The keyboard is also a bit shallow for my taste. Despite these minor issues, \n",
    "I'd definitely recommend this for students and professionals who need portability.\n",
    "\"\"\"\n",
    "\n",
    "analysis = analyze_review_openai(sample_review)\n",
    "print(\"üìä Structured Analysis:\\n\")\n",
    "print(json.dumps(analysis.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Anthropic (Claude) API Setup & First Calls\n\n### About Anthropic Claude\n\n- **Models:** Claude Opus 4.5, Claude Sonnet 4, Claude Haiku 4.5\n- **Strengths:** Excellent instruction following, strong reasoning, safety-focused\n- **Use cases:** Analysis, research, creative tasks, coding\n- **Pricing:**\n  - Claude Haiku 4.5: $0.80/1M input tokens, $4.00/1M output tokens\n  - Claude Sonnet 4: $3.00/1M input tokens, $15.00/1M output tokens\n  - Claude Opus 4.5: $15.00/1M input tokens, $75.00/1M output tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Anthropic client\n",
    "anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Anthropic client initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple message example with Claude\n",
    "def simple_anthropic_call(prompt: str, model: str = \"claude-haiku-4-20251015\"):\n",
    "    \"\"\"Make a simple Anthropic API call.\"\"\"\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.content[0].text,\n",
    "        \"model\": response.model,\n",
    "        \"usage\": {\n",
    "            \"input_tokens\": response.usage.input_tokens,\n",
    "            \"output_tokens\": response.usage.output_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test it!\n",
    "result = simple_anthropic_call(\"Explain what a transformer architecture is in 2 sentences.\")\n",
    "print(\"üìù Response:\", result['content'])\n",
    "print(\"\\nüìä Tokens used:\", result['usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs with Pydantic (Anthropic)\n",
    "\n",
    "Anthropic uses **tool calling** to generate structured JSON outputs. We define a \"tool\" with our Pydantic schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for meeting notes\n",
    "class ActionItem(BaseModel):\n",
    "    task: str = Field(description=\"The action item to be completed\")\n",
    "    assignee: str = Field(description=\"Person responsible\")\n",
    "    deadline: Optional[str] = Field(description=\"Due date if mentioned\")\n",
    "    priority: str = Field(default=\"medium\", description=\"Priority level: low, medium, high\")\n",
    "\n",
    "class MeetingNotes(BaseModel):\n",
    "    \"\"\"Structured meeting notes.\"\"\"\n",
    "    meeting_title: str\n",
    "    date: str\n",
    "    participants: List[str]\n",
    "    key_decisions: List[str] = Field(description=\"Important decisions made\")\n",
    "    action_items: List[ActionItem]\n",
    "    next_meeting: Optional[str] = Field(description=\"Next meeting date\")\n",
    "    summary: str = Field(description=\"Brief meeting summary\")\n",
    "\n",
    "print(\"‚úÖ MeetingNotes model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use structured output with Anthropic (tool calling pattern)\n",
    "def extract_meeting_notes_anthropic(meeting_text: str) -> MeetingNotes:\n",
    "    \"\"\"Extract structured meeting notes using Claude with tool calling.\"\"\"\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=2048,\n",
    "        tools=[\n",
    "            {\n",
    "                \"name\": \"extract_meeting_notes\",\n",
    "                \"description\": \"Extract structured meeting notes from meeting transcript\",\n",
    "                \"input_schema\": MeetingNotes.model_json_schema()\n",
    "            }\n",
    "        ],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Extract structured meeting notes from this transcript:\\n\\n{meeting_text}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract tool use from response\n",
    "    for block in response.content:\n",
    "        if block.type == \"tool_use\":\n",
    "            return MeetingNotes(**block.input)\n",
    "    \n",
    "    raise ValueError(\"No tool use found in response\")\n",
    "\n",
    "# Test with sample meeting transcript\n",
    "sample_meeting = \"\"\"\n",
    "Product Roadmap Meeting - January 15, 2025\n",
    "\n",
    "Attendees: Sarah (PM), John (Engineering), Lisa (Design), Mike (Marketing)\n",
    "\n",
    "Key Decisions:\n",
    "- We will prioritize the mobile app redesign for Q1\n",
    "- Backend API v2 will be postponed to Q2\n",
    "- Marketing budget increased by 20% for new feature launch\n",
    "\n",
    "Action Items:\n",
    "- Sarah to finalize mobile app wireframes by Jan 22\n",
    "- John to provide API migration estimate by Jan 20\n",
    "- Lisa to create design system documentation (high priority) - deadline Feb 1\n",
    "- Mike to schedule user research sessions next week\n",
    "\n",
    "Next meeting scheduled for January 29, 2025 at 2 PM.\n",
    "\"\"\"\n",
    "\n",
    "notes = extract_meeting_notes_anthropic(sample_meeting)\n",
    "print(\"üìã Structured Meeting Notes:\\n\")\n",
    "print(json.dumps(notes.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 7: Google AI (Gemini) API Setup & First Calls\n\n### About Google AI (Gemini)\n\n- **Models:** Gemini 3 Pro, Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.0 Flash\n- **Strengths:** Multimodal (text, images, video), long context windows, cost-effective\n- **Use cases:** Content generation, multimodal analysis, summarization\n- **Pricing:**\n  - Gemini 2.0 Flash: $0.075/1M input tokens, $0.30/1M output tokens (estimated)\n  - Gemini 2.5 Flash: $0.10/1M input tokens, $0.40/1M output tokens (estimated)\n  - Gemini 3 Pro: $2.50/1M input tokens, $10.00/1M output tokens (estimated)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Google AI\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(\"‚úÖ Google AI configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple content generation with Gemini\n",
    "def simple_google_call(prompt: str, model_name: str = \"gemini-2.0-flash-exp\"):\n",
    "    \"\"\"Make a simple Google AI API call.\"\"\"\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.text,\n",
    "        \"model\": model_name\n",
    "    }\n",
    "\n",
    "# Test it!\n",
    "result = simple_google_call(\"Explain what attention mechanism is in neural networks in 2 sentences.\")\n",
    "print(\"üìù Response:\", result['content'])\n",
    "print(\"\\nüìä Model:\", result['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs with Pydantic (Google AI)\n",
    "\n",
    "Google AI supports **JSON mode** with schema definition using `response_mime_type` and `response_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for recipe analysis\n",
    "class RecipeAnalysis(BaseModel):\n",
    "    \"\"\"Structured recipe analysis.\"\"\"\n",
    "    cuisine_type: str = Field(description=\"Type of cuisine (e.g., Italian, Indian, Mexican)\")\n",
    "    difficulty_level: Literal[\"easy\", \"medium\", \"hard\"] = Field(description=\"Cooking difficulty\")\n",
    "    estimated_time_minutes: int = Field(description=\"Total cooking time in minutes\")\n",
    "    ingredients_count: int = Field(description=\"Number of ingredients required\")\n",
    "    dietary_tags: List[str] = Field(description=\"Dietary classifications (vegetarian, vegan, gluten-free, etc.)\")\n",
    "    main_protein: Optional[str] = Field(description=\"Primary protein source if any\")\n",
    "    summary: str = Field(max_length=200, description=\"Brief recipe summary\")\n",
    "\n",
    "print(\"‚úÖ RecipeAnalysis model defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use structured output with Google AI\n",
    "def analyze_recipe_google(recipe_text: str) -> RecipeAnalysis:\n",
    "    \"\"\"Analyze recipe using Google AI with structured output.\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash-exp',\n",
    "        generation_config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_schema\": RecipeAnalysis\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(\n",
    "        f\"Analyze this recipe and provide structured information:\\n\\n{recipe_text}\"\n",
    "    )\n",
    "    \n",
    "    return RecipeAnalysis.model_validate_json(response.text)\n",
    "\n",
    "# Test with sample recipe\n",
    "sample_recipe = \"\"\"\n",
    "Chickpea Curry\n",
    "\n",
    "Ingredients:\n",
    "- 2 cans chickpeas\n",
    "- 1 can coconut milk\n",
    "- 2 tomatoes, diced\n",
    "- 1 onion, chopped\n",
    "- 3 cloves garlic\n",
    "- 2 tbsp curry powder\n",
    "- 1 tsp cumin\n",
    "- Salt and pepper\n",
    "- Fresh cilantro\n",
    "- Rice for serving\n",
    "\n",
    "Instructions:\n",
    "1. Saut√© onion and garlic until fragrant (5 minutes)\n",
    "2. Add curry powder and cumin, cook 1 minute\n",
    "3. Add tomatoes and cook until softened (10 minutes)\n",
    "4. Add chickpeas and coconut milk, simmer 15 minutes\n",
    "5. Season with salt and pepper\n",
    "6. Garnish with cilantro and serve over rice\n",
    "\n",
    "Total time: 35 minutes\n",
    "\"\"\"\n",
    "\n",
    "recipe_info = analyze_recipe_google(sample_recipe)\n",
    "print(\"üç≥ Structured Recipe Analysis:\\n\")\n",
    "print(json.dumps(recipe_info.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Error Handling Patterns\n",
    "\n",
    "### Common API Errors\n",
    "\n",
    "**Understanding HTTP Status Codes:**\n",
    "- `401 Unauthorized` - Invalid or missing API key\n",
    "- `429 Too Many Requests` - Rate limit exceeded\n",
    "- `400 Bad Request` - Invalid request parameters\n",
    "- `500 Internal Server Error` - Provider-side issues\n",
    "- `503 Service Unavailable` - Temporary service outage\n",
    "- Network timeouts - Connection issues\n",
    "\n",
    "### Why Error Handling Matters\n",
    "\n",
    "- **Production reliability** - Graceful degradation\n",
    "- **User experience** - Better error messages\n",
    "- **Debugging** - Easier troubleshooting\n",
    "- **Cost control** - Prevent runaway costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import provider-specific exceptions\n",
    "from openai import OpenAIError, RateLimitError, APIError, AuthenticationError\n",
    "from anthropic import APIError as AnthropicAPIError\n",
    "\n",
    "# Basic error handling for OpenAI\n",
    "def safe_openai_call(prompt: str, model: str = \"gpt-4o-mini\"):\n",
    "    \"\"\"OpenAI call with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": response.choices[0].message.content,\n",
    "            \"usage\": response.usage.model_dump()\n",
    "        }\n",
    "    \n",
    "    except AuthenticationError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"authentication_error\",\n",
    "            \"message\": \"Invalid API key. Please check your credentials.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "    \n",
    "    except RateLimitError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"rate_limit\",\n",
    "            \"message\": \"Rate limit exceeded. Please try again later.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "    \n",
    "    except APIError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"api_error\",\n",
    "            \"message\": \"OpenAI API error occurred.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"unknown\",\n",
    "            \"message\": \"Unexpected error occurred.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "\n",
    "# Test error handling\n",
    "result = safe_openai_call(\"What is machine learning?\")\n",
    "print(\"Result:\", json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling for Anthropic\n",
    "def safe_anthropic_call(prompt: str, model: str = \"claude-haiku-4-20251015\"):\n",
    "    \"\"\"Anthropic call with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": response.content[0].text,\n",
    "            \"usage\": {\n",
    "                \"input_tokens\": response.usage.input_tokens,\n",
    "                \"output_tokens\": response.usage.output_tokens\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except AnthropicAPIError as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"api_error\",\n",
    "            \"message\": \"Anthropic API error occurred.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"unknown\",\n",
    "            \"message\": \"Unexpected error occurred.\",\n",
    "            \"details\": str(e)\n",
    "        }\n",
    "\n",
    "# Test error handling\n",
    "result = safe_anthropic_call(\"What is deep learning?\")\n",
    "print(\"Result:\", json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Comparative Examples\n",
    "\n",
    "Let's compare all three providers on the **same task** to understand their differences in:\n",
    "- Response quality\n",
    "- Token usage\n",
    "- Response time\n",
    "- Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email classification - unified model for all providers\n",
    "class EmailClassification(BaseModel):\n",
    "    \"\"\"Structured email classification.\"\"\"\n",
    "    category: Literal[\"sales\", \"support\", \"inquiry\", \"complaint\", \"spam\"]\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n",
    "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"]\n",
    "    requires_response: bool\n",
    "    suggested_department: str\n",
    "    key_topics: List[str] = Field(max_items=3)\n",
    "    summary: str = Field(max_length=200)\n",
    "\n",
    "# Sample email for testing\n",
    "test_email = \"\"\"\n",
    "Subject: URGENT: System Down - Payment Processing Failed\n",
    "\n",
    "Hi Support Team,\n",
    "\n",
    "Our payment processing system has been down for the past 2 hours. \n",
    "We've lost over $50,000 in potential sales. Multiple customers are \n",
    "complaining about failed transactions. This is completely unacceptable!\n",
    "\n",
    "We need this fixed immediately. Our business is suffering.\n",
    "\n",
    "Please escalate to your engineering team ASAP.\n",
    "\n",
    "John Smith\n",
    "CEO, TechStartup Inc.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìß Test Email Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three providers\n",
    "\n",
    "def classify_email_openai(email_text: str) -> dict:\n",
    "    \"\"\"Classify email using OpenAI.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an email classification assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify this email:\\n\\n{email_text}\"}\n",
    "        ],\n",
    "        response_format=EmailClassification\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"provider\": \"OpenAI GPT-4o-mini\",\n",
    "        \"result\": response.choices[0].message.parsed.model_dump(),\n",
    "        \"response_time\": f\"{elapsed:.2f}s\",\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "        \"cost_estimate\": f\"${(response.usage.prompt_tokens * 0.15 + response.usage.completion_tokens * 0.60) / 1_000_000:.6f}\"\n",
    "    }\n",
    "\n",
    "def classify_email_anthropic(email_text: str) -> dict:\n",
    "    \"\"\"Classify email using Anthropic.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-haiku-4-20251015\",\n",
    "        max_tokens=1024,\n",
    "        tools=[{\n",
    "            \"name\": \"classify_email\",\n",
    "            \"description\": \"Classify an email into structured categories\",\n",
    "            \"input_schema\": EmailClassification.model_json_schema()\n",
    "        }],\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Classify this email:\\n\\n{email_text}\"}]\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Extract tool use\n",
    "    result = None\n",
    "    for block in response.content:\n",
    "        if block.type == \"tool_use\":\n",
    "            result = EmailClassification(**block.input).model_dump()\n",
    "    \n",
    "    total_tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "    \n",
    "    return {\n",
    "        \"provider\": \"Anthropic Claude Haiku 4.5\",\n",
    "        \"result\": result,\n",
    "        \"response_time\": f\"{elapsed:.2f}s\",\n",
    "        \"tokens\": total_tokens,\n",
    "        \"cost_estimate\": f\"${(response.usage.input_tokens * 0.80 + response.usage.output_tokens * 4.00) / 1_000_000:.6f}\"\n",
    "    }\n",
    "\n",
    "def classify_email_google(email_text: str) -> dict:\n",
    "    \"\"\"Classify email using Google AI.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash-exp',\n",
    "        generation_config={\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "            \"response_schema\": EmailClassification\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(f\"Classify this email:\\n\\n{email_text}\")\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    result = EmailClassification.model_validate_json(response.text)\n",
    "    \n",
    "    return {\n",
    "        \"provider\": \"Google Gemini 2.0 Flash\",\n",
    "        \"result\": result.model_dump(),\n",
    "        \"response_time\": f\"{elapsed:.2f}s\",\n",
    "        \"tokens\": \"N/A\",\n",
    "        \"cost_estimate\": \"~$0.000010 (estimated)\"\n",
    "    }\n",
    "\n",
    "# Run comparison\n",
    "print(\"üîÑ Running comparison across all three providers...\\n\")\n",
    "\n",
    "openai_result = classify_email_openai(test_email)\n",
    "anthropic_result = classify_email_anthropic(test_email)\n",
    "google_result = classify_email_google(test_email)\n",
    "\n",
    "# Display results\n",
    "for result in [openai_result, anthropic_result, google_result]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Provider: {result['provider']}\")\n",
    "    print(f\"Response Time: {result['response_time']}\")\n",
    "    print(f\"Tokens: {result['tokens']}\")\n",
    "    print(f\"Cost: {result['cost_estimate']}\")\n",
    "    print(f\"\\nClassification:\")\n",
    "    print(json.dumps(result['result'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Best Practices & Production Tips\n",
    "\n",
    "### Token Management\n",
    "\n",
    "**Cost optimization strategies:**\n",
    "- Use cheaper models for simple tasks (gpt-4o-mini, claude-haiku, gemini-flash)\n",
    "- Count tokens before sending to avoid surprises\n",
    "- Cache responses when possible\n",
    "- Use streaming for long responses\n",
    "- Set `max_tokens` to prevent runaway costs\n",
    "\n",
    "### Security Checklist\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Store API keys in secrets/environment variables\n",
    "- Rotate keys regularly (every 90 days)\n",
    "- Monitor usage dashboards daily\n",
    "- Set spending limits\n",
    "- Sanitize user inputs\n",
    "- Log errors but not sensitive data\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Hardcode API keys\n",
    "- Commit keys to version control\n",
    "- Share keys in chat/email\n",
    "- Log API keys\n",
    "- Ignore rate limits\n",
    "- Skip error handling\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "When building production applications:\n",
    "- Always implement error handling\n",
    "- Monitor API usage and costs\n",
    "- Use appropriate models for each task\n",
    "- Validate all inputs and outputs\n",
    "- Keep API keys secure\n",
    "- Test with real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Hands-On Exercises\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Multi-Provider Sentiment Analyzer\n",
    "\n",
    "**Task:** Create a sentiment analyzer that:\n",
    "1. Takes a customer review as input\n",
    "2. Analyzes it using all three providers (OpenAI, Anthropic, Google)\n",
    "3. Returns structured sentiment analysis from each\n",
    "4. Compares the results\n",
    "\n",
    "**Requirements:**\n",
    "- Use Pydantic model for structured output\n",
    "- Include error handling\n",
    "- Compare response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    \"\"\"Sentiment analysis result.\"\"\"\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n",
    "    confidence: float = Field(ge=0, le=1)\n",
    "    key_phrases: List[str]\n",
    "    recommendation: str\n",
    "\n",
    "# TODO: Implement multi_provider_sentiment_analysis function\n",
    "def multi_provider_sentiment_analysis(review_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment using all three providers and compare results.\n",
    "    \n",
    "    Args:\n",
    "        review_text: Customer review to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results from all providers\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_review = \"\"\"\n",
    "The new smartphone is amazing! Camera quality is top-notch and battery lasts all day.\n",
    "However, it's quite expensive and the charger is sold separately which is disappointing.\n",
    "Overall, worth it if you can afford the premium price.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to test:\n",
    "# results = multi_provider_sentiment_analysis(test_review)\n",
    "# print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Email Parser with Error Handling\n",
    "\n",
    "**Task:** Build an email parser that:\n",
    "1. Extracts structured data from raw email text\n",
    "2. Implements comprehensive error handling\n",
    "3. Falls back to another provider if the first fails\n",
    "\n",
    "**Requirements:**\n",
    "- Pydantic model for email structure\n",
    "- Try OpenAI first, fallback to Anthropic on error\n",
    "- Handle validation errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "class EmailData(BaseModel):\n",
    "    \"\"\"Structured email data.\"\"\"\n",
    "    sender: str\n",
    "    subject: str\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"]\n",
    "    category: str\n",
    "    action_required: bool\n",
    "    deadline: Optional[str]\n",
    "    summary: str\n",
    "\n",
    "# TODO: Implement robust_email_parser function\n",
    "def robust_email_parser(email_text: str) -> EmailData:\n",
    "    \"\"\"\n",
    "    Parse email with error handling and fallback.\n",
    "    \n",
    "    Args:\n",
    "        email_text: Raw email text\n",
    "    \n",
    "    Returns:\n",
    "        Structured email data\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_email_ex2 = \"\"\"\n",
    "From: jane.doe@company.com\n",
    "Subject: Q1 Budget Approval Needed by Friday\n",
    "\n",
    "Hi Team,\n",
    "\n",
    "We need to finalize the Q1 budget by end of day Friday.\n",
    "Please review the attached spreadsheet and send your approvals.\n",
    "\n",
    "This is blocking our vendor negotiations, so please prioritize.\n",
    "\n",
    "Thanks,\n",
    "Jane\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to test:\n",
    "# result = robust_email_parser(test_email_ex2)\n",
    "# print(json.dumps(result.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Provider Comparison Dashboard\n",
    "\n",
    "**Task:** Create a comparison function that:\n",
    "1. Takes a prompt and runs it through all three providers\n",
    "2. Measures response time for each\n",
    "3. Estimates cost for each\n",
    "4. Returns a comparison summary\n",
    "\n",
    "**Requirements:**\n",
    "- Handle errors for each provider separately\n",
    "- Track timing accurately\n",
    "- Calculate cost estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# TODO: Implement compare_all_providers function\n",
    "def compare_all_providers(prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run the same prompt through all providers and compare results.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text prompt to send to all providers\n",
    "    \n",
    "    Returns:\n",
    "        Comparison summary with timing and cost data\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# Uncomment to test:\n",
    "# comparison = compare_all_providers(\"Explain quantum computing in simple terms.\")\n",
    "# print(json.dumps(comparison, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Summary & Next Steps\n",
    "\n",
    "### What We Learned Today\n",
    "\n",
    "‚úÖ **API Setup & Authentication**\n",
    "- OpenAI, Anthropic, and Google AI clients\n",
    "- Secure API key management with Colab secrets\n",
    "\n",
    "‚úÖ **Structured Outputs**\n",
    "- Pydantic models for type-safe JSON\n",
    "- Provider-specific approaches:\n",
    "  - OpenAI: `response_format` with native Pydantic\n",
    "  - Anthropic: Tool calling pattern\n",
    "  - Google AI: JSON schema with `response_mime_type`\n",
    "\n",
    "‚úÖ **Error Handling**\n",
    "- Common API errors and how to handle them\n",
    "- Provider-specific exception handling\n",
    "- Graceful error responses\n",
    "\n",
    "‚úÖ **Best Practices**\n",
    "- Token management and cost optimization\n",
    "- Security checklist\n",
    "- Provider comparison and selection\n",
    "\n",
    "### Key Differences Between Providers\n",
    "\n",
    "| Feature | OpenAI | Anthropic | Google AI |\n",
    "|---------|--------|-----------|----------|\n",
    "| **Best For** | General-purpose | Analysis & reasoning | Multimodal, cost-effective |\n",
    "| **Pricing (per 1M tokens)** | $0.15-$0.60 (mini) | $0.80-$75 (Haiku-Opus) | $0.075-$10 |\n",
    "| **Context Window** | 128K tokens | 200K tokens | 2M tokens (Gemini 2.5) |\n",
    "| **Structured Output** | Native Pydantic | Tool calling | JSON schema |\n",
    "| **Strengths** | Wide adoption | Safety-focused | Long context, multimodal |\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**Official Documentation:**\n",
    "- OpenAI: https://platform.openai.com/docs\n",
    "- Anthropic: https://docs.anthropic.com\n",
    "- Google AI: https://ai.google.dev/docs\n",
    "\n",
    "**Community Resources:**\n",
    "- LangChain: https://python.langchain.com\n",
    "- Pydantic: https://docs.pydantic.dev\n",
    "\n",
    "### Preview: Next Session\n",
    "\n",
    "In the next notebook, we'll cover:\n",
    "\n",
    "üîÆ **Vector Databases & Embeddings**\n",
    "- What are embeddings and why they matter\n",
    "- Vector similarity search\n",
    "- Chunking strategies\n",
    "- Working with Chroma, Pinecone, FAISS\n",
    "\n",
    "üîÆ **RAG (Retrieval-Augmented Generation)**\n",
    "- Building a complete RAG pipeline\n",
    "- Document loading and processing\n",
    "- Semantic search with citations\n",
    "- Hands-on: Build a RAG chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 13: Bonus - OpenAI Responses API (Advanced)\n\n### What is the Responses API?\n\nOpenAI's **Responses API** provides an alternative API interface. While we've been using the **Chat Completions API** throughout this notebook (which OpenAI will support **indefinitely**), it's valuable to understand when you might choose the Responses API for production applications.\n\n**Key Point:** The Chat Completions API we've used is perfect for this training and will continue to be the industry standard. The Responses API is an advanced option for specific production use cases.\n\n### üîÑ Chat Completions vs Responses API\n\n| Feature | Chat Completions API | Responses API |\n|---------|---------------------|---------------|\n| **Use Case** | Stateless, single-turn interactions | Stateful, multi-turn conversations |\n| **State Management** | Client-side (you manage conversation history) | Server-side (OpenAI manages for you) |\n| **Built-in Tools** | No (you implement functions) | Yes (web search, file search, code execution) |\n| **Cost** | Standard pricing | 40-80% lower (better cache utilization) |\n| **Complexity** | Simple, direct | More features, more complex |\n| **Support Status** | Supported **indefinitely** | Current, evolving |\n| **Best For** | Training, simple bots, full control | Production apps, complex workflows |\n| **GPT-5 Optimization** | Works fine | Optimized for GPT-5 |\n| **Learning Curve** | Easier | Steeper |\n\n### üéØ When to Use Chat Completions (What We've Used)\n\n‚úÖ **Choose Chat Completions API when you need:**\n- **Educational/training purposes** - Teaching fundamentals (like this notebook!)\n- **Simple, stateless interactions** - One question, one answer\n- **Full control over context** - You manage conversation history\n- **Lightweight applications** - Chatbots, Q&A systems\n- **Industry standard interface** - Most stable, well-documented\n- **Custom function calling** - You implement your own tools\n\n### üöÄ When to Use Responses API\n\n‚úÖ **Choose Responses API when you need:**\n- **Server-side state management** - Don't want to track conversation history\n- **Built-in tools** - Web search, file search, code execution sandbox\n- **Cost optimization** - Benefit from 40-80% better caching\n- **GPT-5 applications** - GPT-5 works best with Responses API\n- **Document analysis** - Legal contracts, research papers, knowledge bases\n- **Complex multi-turn workflows** - Extended conversations with context retention\n\n### üìä Key Advantages of Responses API\n\n1. **Automatic State Management**\n   - Pass a `response_id` instead of full conversation history\n   - OpenAI maintains conversation context server-side\n   - Reduces payload size and complexity\n\n2. **Built-in Tools**\n   - Web search without implementing custom functions\n   - File search for document analysis\n   - Code execution sandbox for running Python code\n\n3. **Better Cache Utilization**\n   - 40-80% cost reduction in internal OpenAI tests\n   - More efficient prompt caching\n   - Reduced latency for repeated patterns\n\n4. **GPT-5 Optimization**\n   - GPT-5 orchestrates tools as part of reasoning\n   - Legacy endpoints may cause degraded behavior\n   - Future models will be optimized for Responses API\n\n### üí° Example Comparison\n\n**Chat Completions (What We've Used):**\n```python\n# You manage conversation history\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is AI?\"},\n    {\"role\": \"assistant\", \"content\": \"AI is...\"},\n    {\"role\": \"user\", \"content\": \"Tell me more\"}\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages  # Send full history each time\n)\n```\n\n**Responses API:**\n```python\n# OpenAI manages state for you\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    previous_response_id=\"resp_abc123\",  # Reference to previous state\n    messages=[{\"role\": \"user\", \"content\": \"Tell me more\"}]\n)\n# Only send new message, not full history\n```\n\n### üìö Migration Timeline\n\n- **Chat Completions API:** Supported **indefinitely** ‚úÖ\n- **Assistants API:** Being deprecated (migrate by August 26, 2026)\n- **Responses API:** Recommended for projects with state/tools\n\n### üéì What Should You Use?\n\n**For this training:** We're using **Chat Completions API** because:\n- ‚úÖ Perfect for learning fundamentals\n- ‚úÖ Simpler, more direct\n- ‚úÖ Industry standard that will be supported forever\n- ‚úÖ Better for understanding how LLM APIs work\n\n**For production apps:** Consider **Responses API** if:\n- You need server-side state management\n- You want built-in tools (web search, file search, code execution)\n- Cost optimization is critical (40-80% savings)\n- You're building with GPT-5\n\n### üîó Resources\n\n- [Responses API Documentation](https://platform.openai.com/docs/api-reference/responses)\n- [Migration Guide: Chat Completions ‚Üí Responses](https://platform.openai.com/docs/guides/migrate-to-responses)\n- [Responses vs Chat Completions Comparison](https://platform.openai.com/docs/guides/responses-vs-chat-completions)\n\n---\n\n**Bottom Line:** The Chat Completions API you've learned in this notebook is production-ready, will be supported indefinitely, and is perfect for most use cases. The Responses API is an advanced option for specific scenarios requiring server-side state or built-in tools."
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 13.1 Hands-On: Basic Responses API\n\nNow let's see the Responses API in action! We'll explore:\n\n1. **Basic text generation** with `client.responses.create()`\n2. **Code interpreter tool** for solving math problems\n\n**Key Difference from Chat Completions:**\n- Responses API uses `input` instead of `messages`\n- Can specify `instructions` for system-level behavior\n- Returns `output_text` (simpler than accessing `.choices[0].message.content`)\n\nLet's start with a basic example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Basic Responses API Example\n# Compare this to the Chat Completions API we used earlier!\n\ntry:\n    # Using Responses API (new way)\n    response = openai_client.responses.create(\n        model=\"gpt-4o\",  # GPT-4o recommended for Responses API\n        instructions=\"You are a helpful coding assistant that explains concepts concisely.\",\n        input=\"How do I check if a Python object is an instance of a class?\",\n    )\n    \n    # Notice: Much simpler access to output!\n    print(\"üìù Response from Responses API:\")\n    print(\"=\"*60)\n    print(response.output_text)\n    print(\"=\"*60)\n    \n    # Response metadata\n    print(f\"\\n‚úÖ Model used: {response.model}\")\n    print(f\"‚úÖ Response ID: {response.id}\")\n    print(f\"   (Use this ID with 'previous_response_id' for stateful conversations)\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error: {str(e)}\")\n    print(\"Note: Responses API requires OpenAI Python SDK >= 1.60.0\")\n    print(\"Run: !pip install --upgrade openai\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n### 13.2 Hands-On: Code Interpreter Tool\n\nOne of the most powerful features of the Responses API is **built-in tools**. Let's use the **code_interpreter** tool to solve a math problem.\n\n**How Code Interpreter Works:**\n1. You provide a problem that requires computation\n2. The model writes Python code to solve it\n3. Code runs in a **sandboxed container** (secure, isolated environment)\n4. Model returns the solution with explanation\n\n**Code Interpreter Configuration:**\n```python\ntools=[{\n    \"type\": \"code_interpreter\",\n    \"container\": {\"type\": \"auto\"}  # Auto-manages containers\n}]\n```\n\n**Container Management:**\n- `\"auto\"` mode: Reuses existing container or creates new one\n- Containers expire after 20 minutes of inactivity\n- Cost: $0.03 per container\n\nLet's solve a math problem:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Code Interpreter Tool Example\n# The model will write and execute Python code to solve this problem!\n\ntry:\n    response = openai_client.responses.create(\n        model=\"gpt-4o\",\n        tools=[\n            {\n                \"type\": \"code_interpreter\",\n                \"container\": {\"type\": \"auto\"}  # Auto-manage containers\n            }\n        ],\n        instructions=\"You are a personal math tutor. Write and run Python code to answer math questions. Show your work step-by-step.\",\n        input=\"I need to solve the equation 3x + 11 = 14. Can you help me? Also, verify the answer.\",\n    )\n    \n    print(\"üßÆ Math Problem: Solve 3x + 11 = 14\")\n    print(\"=\"*60)\n    print(\"\\nüìù Model's Response:\")\n    print(response.output_text)\n    print(\"=\"*60)\n    \n    # Show tool usage information\n    print(f\"\\n‚úÖ Response ID: {response.id}\")\n    print(f\"‚úÖ Model: {response.model}\")\n    \n    # Check if code was executed\n    if hasattr(response, 'output') and response.output:\n        print(f\"‚úÖ Code interpreter was used to solve this problem!\")\n        print(f\"   The model wrote Python code, executed it, and verified the answer.\")\n    \n    print(\"\\nüí° Key Insight:\")\n    print(\"   The Responses API automatically ran Python code in a secure sandbox\")\n    print(\"   to solve and verify the equation. This is built-in functionality!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error: {str(e)}\")\n    print(\"\\nüîß Troubleshooting:\")\n    print(\"   - Ensure you have OpenAI Python SDK >= 1.60.0\")\n    print(\"   - Code interpreter requires GPT-4o or GPT-4.1+ models\")\n    print(\"   - Run: !pip install --upgrade openai\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Questions?\n",
    "\n",
    "Great work completing this notebook! You now have a solid foundation for working with major LLM APIs.\n",
    "\n",
    "**Next:** Save this notebook and move on to `02_vector_databases_and_embeddings.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}