{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LAB 1: Build a Complete RAG Chatbot\n\n**Duration**: ~1 hour hands-on lab\n\n## Prerequisites\n\nâœ… **Completed Notebook 03 (LangChain Essentials)** - Document loaders, text splitters, vector stores, LCEL, RAG fundamentals  \nâœ… OpenAI API key  \n\n**Note**: This lab applies the concepts from Notebook 03 in a complete hands-on project.\n\n---\n\n## What You'll Build\n\nA production-ready RAG (Retrieval-Augmented Generation) chatbot with:\n- Document loading and chunking\n- Vector storage with ChromaDB\n- Semantic search\n- Conversation memory\n- Source citations\n\n## Learning Objectives\n\nBy the end of this lab, you'll be able to:\n- Load and process documents for RAG\n- Generate and store embeddings in ChromaDB\n- Build a retrieval chain with LangChain\n- Add conversation memory for multi-turn conversations\n- Implement source citation for transparency\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall existing langchain packages (Colab clean slate)\n",
    "!pip uninstall -y langchain langchain-core langchain-community langchain-openai langchain-chroma langchain-text-splitters\n",
    "\n",
    "# Install with compatible versions (let pip resolve dependencies)\n",
    "!pip install -qU \\\n",
    "    openai \\\n",
    "    langchain \\\n",
    "    langchain-openai \\\n",
    "    langchain-chroma \\\n",
    "    langchain-community \\\n",
    "    langchain-text-splitters \\\n",
    "    pypdf \\\n",
    "    chromadb\n",
    "\n",
    "# Show installed versions for verification\n",
    "!pip list | grep langchain\n",
    "\n",
    "print(\"\\nâœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API keys\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"âœ… API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Document Loading and Chunking\n",
    "\n",
    "First, we'll load documents and split them into chunks for processing.\n",
    "\n",
    "**Why chunking?**\n",
    "- LLMs have token limits\n",
    "- Smaller chunks = better semantic search\n",
    "- Balance: too small = loss of context, too large = irrelevant matches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # âœ… Updated import (2025)\n",
    "\n",
    "# Create a sample document\n",
    "sample_doc = \"\"\"Artificial Intelligence (AI) is transforming how we work and live. Machine Learning,\n",
    "a subset of AI, enables computers to learn from data without explicit programming.\n",
    "\n",
    "Deep Learning is a type of Machine Learning that uses neural networks with multiple layers.\n",
    "It excels at tasks like image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "Natural Language Processing (NLP) is a field of AI focused on enabling computers to understand,\n",
    "interpret, and generate human language. Applications include chatbots, translation, and sentiment analysis.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval of relevant documents with text generation.\n",
    "This approach helps LLMs provide more accurate and grounded responses by referencing external knowledge.\n",
    "\n",
    "Vector databases store embeddings - numerical representations of text that capture semantic meaning.\n",
    "Popular vector databases include ChromaDB, Pinecone, and Weaviate.\"\"\"\n",
    "\n",
    "# Save sample document\n",
    "with open('sample_doc.txt', 'w') as f:\n",
    "    f.write(sample_doc)\n",
    "\n",
    "# Load document\n",
    "loader = TextLoader('sample_doc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document content preview: {documents[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,        # Characters per chunk\n",
    "    chunk_overlap=50,       # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"\\nSplit into {len(chunks)} chunks\")\n",
    "print(\"\\nChunk examples:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Generate Embeddings & Store in ChromaDB\n",
    "\n",
    "Now we'll convert chunks into embeddings and store them in a vector database.\n",
    "\n",
    "**Embeddings**: Numerical representations that capture semantic meaning\n",
    "**ChromaDB**: Fast, open-source vector database\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"ai_knowledge_base\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Created vector store with embeddings\")\n",
    "print(f\"Total documents in vector store: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "query = \"What is deep learning?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 2 most relevant chunks:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Build RAG Pipeline\n",
    "\n",
    "Create a complete RAG system that:\n",
    "1. Retrieves relevant documents\n",
    "2. Augments the prompt with context\n",
    "3. Generates response with LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG Pipeline using LCEL (modern LangChain 1.x)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "If you don't know the answer based on the context, say so.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create retrieval chain using LCEL\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ… RAG pipeline created using LCEL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG pipeline\n",
    "question = \"What is the difference between Machine Learning and Deep Learning?\"\n",
    "\n",
    "# LCEL: invoke with question directly\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Citations\n",
    "\n",
    "Let's add citations to show which sources informed the answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_with_citations(question):\n",
    "    \"\"\"Format response with source citations using LCEL\"\"\"\n",
    "    # Get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    # Get sources separately\n",
    "    sources = retriever.invoke(question)\n",
    "    \n",
    "    # Add citations\n",
    "    citation_text = \"\\n\\nSources:\"\n",
    "    for i, doc in enumerate(sources):\n",
    "        citation_text += f\"\\n[{i+1}] {doc.page_content[:100]}...\"\n",
    "    \n",
    "    return answer + citation_text\n",
    "\n",
    "# Test with citations\n",
    "formatted_response = format_with_citations(\"What is RAG?\")\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Add Conversation Memory\n",
    "\n",
    "Enable multi-turn conversations with memory.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational RAG using LCEL\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Helper to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Contextualize question prompt\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create contextualized question chain\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "# QA prompt with history\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, say that you don't know.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Build conversational RAG chain\n",
    "conversational_rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(\n",
    "            retriever.invoke(\n",
    "                contextualize_q_chain.invoke(x) if x.get(\"chat_history\") else x[\"input\"]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ… Conversational RAG chain created using LCEL!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-turn conversation\n",
    "chat_history = []\n",
    "\n",
    "# Turn 1\n",
    "question1 = \"What is NLP?\"\n",
    "response1 = conversational_rag_chain.invoke({\n",
    "    \"input\": question1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"Question 1: {question1}\")\n",
    "print(f\"Answer 1: {response1}\\n\")\n",
    "\n",
    "# Update chat history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=response1)\n",
    "])\n",
    "\n",
    "# Turn 2 (references previous context)\n",
    "question2 = \"What are some applications of it?\"\n",
    "response2 = conversational_rag_chain.invoke({\n",
    "    \"input\": question2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"Question 2: {question2}\")\n",
    "print(f\"Answer 2: {response2}\")\n",
    "\n",
    "print(\"\\nâœ… Conversation memory working! The chatbot understood 'it' refers to NLP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Exercise\n",
    "\n",
    "Now it's your turn! Try these exercises:\n",
    "\n",
    "1. **Add Your Own Documents**\n",
    "   - Create a new text file with content on a topic you're interested in\n",
    "   - Load and chunk it\n",
    "   - Add to the vector store\n",
    "\n",
    "2. **Experiment with Chunk Size**\n",
    "   - Try different `chunk_size` values (100, 500, 1000)\n",
    "   - Observe how it affects retrieval quality\n",
    "\n",
    "3. **Adjust Retrieval Parameters**\n",
    "   - Change `k` (number of documents retrieved)\n",
    "   - Test with different queries\n",
    "\n",
    "4. **Enhance Citations**\n",
    "   - Modify `format_with_citations()` to include relevance scores\n",
    "   - Add metadata (page numbers, document titles)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "# Example starting point:\n",
    "# 1. Create new document\n",
    "new_content = \"\"\"\n",
    "# Add your own content here\n",
    "\"\"\"\n",
    "\n",
    "# 2. Process and add to vector store\n",
    "# text_splitter = ...\n",
    "# new_chunks = ...\n",
    "# vectorstore.add_documents(new_chunks)\n",
    "\n",
    "# 3. Test with questions about your content\n",
    "\n",
    "print(\"Add your code above to complete the challenge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "Congratulations! You've built a complete RAG chatbot. Here's what you accomplished:\n",
    "\n",
    "### What You Built\n",
    "âœ… **Document Loading**: Loaded and chunked text documents\n",
    "âœ… **Vector Storage**: Created embeddings and stored in ChromaDB\n",
    "âœ… **Semantic Search**: Retrieved relevant documents based on meaning\n",
    "âœ… **RAG Pipeline**: Combined retrieval with LLM generation\n",
    "âœ… **Citations**: Added source attribution for transparency\n",
    "âœ… **Conversation Memory**: Enabled multi-turn conversations\n",
    "\n",
    "### Key Concepts\n",
    "- **RAG** = Retrieval (find relevant docs) + Augmented Generation (use context to answer)\n",
    "- **Embeddings** capture semantic meaning as vectors\n",
    "- **Vector databases** enable fast similarity search\n",
    "- **Chunking** balances context vs specificity\n",
    "\n",
    "### Production Considerations\n",
    "- Use persistent storage for vector database\n",
    "- Implement error handling for API calls\n",
    "- Add rate limiting for costs\n",
    "- Monitor retrieval quality\n",
    "- Regular vector store updates\n",
    "\n",
    "### Next Steps\n",
    "- Try different embedding models (text-embedding-3-large for better quality)\n",
    "- Integrate with a web framework (FastAPI, Streamlit)\n",
    "- Add document metadata (dates, authors, sources)\n",
    "- Implement semantic caching for faster responses\n",
    "- Add guardrails (input validation, PII filtering)\n",
    "\n",
    "### Resources\n",
    "- [LangChain RAG Documentation](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "**Portfolio Tip**: This RAG chatbot is a great addition to your portfolio. Consider:\n",
    "- Deploying it as a web app\n",
    "- Adding a custom UI\n",
    "- Training on domain-specific documents\n",
    "- Publishing on GitHub with documentation\n",
    "\n",
    "**Well done!** ðŸŽ‰\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}