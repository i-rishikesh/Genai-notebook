{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced Patterns for Production LLM Systems\n\n**Duration**: ~1-1.5 hours (streamlined)\n\n## What You'll Learn\n\nThis notebook covers **essential patterns for building production LLM systems**:\n\n1. **Framework Decision Guide** - When to use LangChain vs LangGraph\n2. **Memory Patterns** - Managing conversation context\n3. **Multi-Agent Basics** - Supervisor pattern for team workflows\n4. **Human-in-the-Loop** - Approval workflows and interrupts\n5. **Production Checklist** - Readiness for deployment\n\n## Prerequisites\n\n\u2705 **Completed Notebooks 03 & 04** - LangChain Essentials, LangGraph Essentials  \n\u2705 OpenAI API key  \n\u2705 Understanding of LCEL, RAG, and basic agents\n\n## Learning Approach\n\n**Concept-Focused**: We'll cover WHAT each pattern does and WHY it matters, with simple working examples.\n\nFor detailed implementations, see the **Advanced Reference** section at the end of this notebook.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install packages\n!pip install -qU langchain langchain-openai langgraph langgraph-checkpoint-sqlite\n\nprint(\"\u2705 Packages installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup API key\nimport os\nfrom getpass import getpass\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n\nprint(\"\u2705 API key configured!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 1: Framework Decision Guide\n\n### When to Use LangChain vs LangGraph?\n\n**Decision Matrix**:\n\n| Use Case | Framework | Why |\n|----------|-----------|-----|\n| Simple RAG chatbot | **LangChain (LCEL)** | Linear workflow, no cycles |\n| Document Q&A | **LangChain (LCEL)** | Stateless retrieve \u2192 answer |\n| Code generator with testing | **LangGraph** | Retry loops needed |\n| Multi-agent content team | **LangGraph** | Multiple agents, shared state |\n| Approval workflows | **LangGraph** | Human interrupts required |\n| Research agent with decisions | **LangGraph** | Conditional routing |\n\n### Quick Decision Flow\n\n```\nDo you need loops or retries?\n\u251c\u2500 YES \u2192 LangGraph\n\u2514\u2500 NO \u2192 Do you need complex shared state?\n    \u251c\u2500 YES \u2192 LangGraph\n    \u2514\u2500 NO \u2192 Do you need multiple agents?\n        \u251c\u2500 YES \u2192 LangGraph\n        \u2514\u2500 NO \u2192 LangChain (LCEL)\n```\n\n### Key Insight\n\n- **LangChain (LCEL)** = Stateless chains with pipe syntax: `prompt | llm | parser`\n- **LangGraph** = Stateful workflows with cycles, decisions, and multi-agent coordination\n\n**Rule of thumb**: Start with LangChain. Migrate to LangGraph when you need state, loops, or multi-agent.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Decision helper function\ndef should_use_langgraph(\n    needs_loops=False,\n    needs_shared_state=False,\n    needs_multi_agent=False,\n    needs_human_approval=False\n):\n    \"\"\"Quick framework decision helper\"\"\"\n    if any([needs_loops, needs_shared_state, needs_multi_agent, needs_human_approval]):\n        return \"LangGraph\"\n    return \"LangChain (LCEL)\"\n\n# Test examples\nprint(\"Simple RAG chatbot:\", should_use_langgraph())\nprint(\"Code gen with retry:\", should_use_langgraph(needs_loops=True))\nprint(\"Multi-agent team:\", should_use_langgraph(needs_multi_agent=True))\nprint(\"Approval workflow:\", should_use_langgraph(needs_human_approval=True))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 2: Memory Patterns\n\n### Why Memory Matters\n\nLLM chains are **stateless by default**. Each call is independent.\n\n**Memory enables**:\n- Multi-turn conversations\n- Context retention across messages\n- Personalized responses based on history\n\n### Memory Pattern Comparison\n\n| Pattern | When to Use | Pros | Cons |\n|---------|-------------|------|------|\n| **Buffer Memory** | Short conversations (< 10 turns) | Preserves all context | Token costs grow |\n| **Window Memory** | Medium conversations (10-50 turns) | Fixed cost (last N messages) | Loses older context |\n| **Summary Memory** | Long conversations (50+ turns) | Constant token cost | May lose details |\n\n### Modern Approach: RunnableWithMessageHistory\n\nThe 2025 standard for adding memory to LCEL chains.\n\n**Key components**:\n1. **Session management** - `session_id` separates conversations\n2. **Message storage** - `ChatMessageHistory` stores messages\n3. **Automatic injection** - History automatically added to prompts\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple conversational chatbot with memory\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_community.chat_message_histories import ChatMessageHistory\n\n# Create base chain\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{question}\")\n])\n\nchain = prompt | llm\n\n# Session storage\nstore = {}  # session_id -> ChatMessageHistory\n\ndef get_session_history(session_id: str):\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n# Add memory to chain\nchain_with_memory = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\"\n)\n\n# Test\nconfig = {\"configurable\": {\"session_id\": \"user-123\"}}\n\nr1 = chain_with_memory.invoke({\"question\": \"My name is Alice\"}, config)\nprint(f\"Turn 1: {r1.content}\")\n\nr2 = chain_with_memory.invoke({\"question\": \"What's my name?\"}, config)\nprint(f\"Turn 2: {r2.content}\")\n\nprint(\"\\n\u2705 Memory works - name remembered across turns!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 3: Multi-Agent Basics\n\n### The Supervisor Pattern\n\n**Problem**: Complex tasks need specialized skills (research, writing, coding, etc.)\n\n**Solution**: Supervisor pattern - one orchestrator coordinates multiple specialist agents.\n\n```\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502 Supervisor  \u2502  \u2190 Routes work to specialists\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2193          \u2193          \u2193\n  [Researcher] [Writer] [Reviewer]\n```\n\n### When to Use\n\n- Content creation teams (research \u2192 write \u2192 edit)\n- Data analysis pipelines (extract \u2192 transform \u2192 analyze)\n- Customer support (classify \u2192 route \u2192 respond)\n\n### Key Concept\n\n**Supervisor** decides which specialist agent to call next based on current state. All agents share a common state.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple 2-agent example: Researcher + Writer\nfrom langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict, Annotated\nfrom operator import add\n\n# Shared state\nclass TeamState(TypedDict):\n    messages: Annotated[list[str], add]\n    topic: str\n    research: str\n    article: str\n    next_agent: str\n\n# Agents\ndef researcher(state: TeamState):\n    topic = state[\"topic\"]\n    research = f\"Research on {topic}: Key facts include...\"\n    return {\n        \"research\": research,\n        \"messages\": [\"Researcher: Completed research\"]\n    }\n\ndef writer(state: TeamState):\n    research = state[\"research\"]\n    article = f\"Article based on: {research[:50]}...\"\n    return {\n        \"article\": article,\n        \"messages\": [\"Writer: Completed article\"]\n    }\n\ndef supervisor(state: TeamState):\n    \"\"\"Decides next agent\"\"\"\n    if not state.get(\"research\"):\n        return {\"next_agent\": \"researcher\"}\n    elif not state.get(\"article\"):\n        return {\"next_agent\": \"writer\"}\n    else:\n        return {\"next_agent\": \"END\"}\n\n# Build graph\nworkflow = StateGraph(TeamState)\nworkflow.add_node(\"supervisor\", supervisor)\nworkflow.add_node(\"researcher\", researcher)\nworkflow.add_node(\"writer\", writer)\n\n# Routes\nworkflow.add_edge(START, \"supervisor\")\nworkflow.add_conditional_edges(\n    \"supervisor\",\n    lambda s: s[\"next_agent\"],\n    {\"researcher\": \"researcher\", \"writer\": \"writer\", \"END\": END}\n)\nworkflow.add_edge(\"researcher\", \"supervisor\")\nworkflow.add_edge(\"writer\", \"supervisor\")\n\nteam = workflow.compile()\n\n# Run\nresult = team.invoke({\"messages\": [], \"topic\": \"AI safety\", \"research\": \"\", \"article\": \"\", \"next_agent\": \"\"})\nprint(\"\\n\".join(result[\"messages\"]))\nprint(f\"\\nFinal article: {result['article']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 4: Human-in-the-Loop\n\n### Why Human Approval Matters\n\n**Use cases**:\n- Publishing content (review before public release)\n- Executing sensitive operations (database deletions, API calls)\n- Financial transactions (payment approvals)\n- Policy decisions (final human judgment)\n\n### How Interrupts Work\n\n1. Graph executes normally\n2. Reaches node with `interrupt_before` \u2192 **PAUSES**\n3. Saves state via checkpointing\n4. Waits for human input\n5. Resume execution after approval\n\n### Key Concept\n\n**Checkpointing required**: Can't interrupt without saving state!\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple approval workflow\nfrom langgraph.checkpoint.memory import MemorySaver\n\nclass ApprovalState(TypedDict):\n    content: str\n    approved: bool\n\ndef generate_content(state: ApprovalState):\n    return {\"content\": \"Draft: Important announcement...\"}\n\ndef publish_content(state: ApprovalState):\n    return {\"content\": f\"Published: {state['content']}\"}\n\n# Build workflow\napproval_workflow = StateGraph(ApprovalState)\napproval_workflow.add_node(\"generate\", generate_content)\napproval_workflow.add_node(\"publish\", publish_content)\n\napproval_workflow.add_edge(START, \"generate\")\napproval_workflow.add_edge(\"generate\", \"publish\")\napproval_workflow.add_edge(\"publish\", END)\n\n# Compile with interrupt\nmemory = MemorySaver()\napp = approval_workflow.compile(\n    checkpointer=memory,\n    interrupt_before=[\"publish\"]  # Pause before publishing\n)\n\n# Run - will pause at publish\nconfig = {\"configurable\": {\"thread_id\": \"approval-1\"}}\nstate = app.invoke({\"content\": \"\", \"approved\": False}, config)\n\nprint(f\"Generated content: {state['content']}\")\nprint(\"\\n\u23f8\ufe0f  PAUSED - Waiting for human approval...\")\nprint(\"(In production, human would review and approve)\")\n\n# Resume (simulate approval)\nfinal_state = app.invoke(None, config)\nprint(f\"\\n{final_state['content']}\")\nprint(\"\u2705 Human-in-the-loop workflow completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 5: Production Readiness Checklist\n\nBefore deploying LLM systems to production, ensure you have these patterns in place:\n\n### Reliability\n\n- \u2705 **Retry logic** with exponential backoff (handle transient failures)\n- \u2705 **Circuit breakers** (fail fast when service is down)\n- \u2705 **Timeouts** (prevent hanging requests)\n- \u2705 **Fallback responses** (graceful degradation)\n\n### Security\n\n- \u2705 **Input validation** (prevent prompt injection)\n- \u2705 **Output filtering** (content moderation)\n- \u2705 **PII detection** (redact sensitive data)\n- \u2705 **Rate limiting** (prevent abuse)\n\n### Performance\n\n- \u2705 **Caching** (in-memory, prompt caching, semantic caching)\n- \u2705 **Streaming** (faster perceived latency)\n- \u2705 **Batching** (process multiple requests efficiently)\n\n### Quality\n\n- \u2705 **Hallucination detection** (LLM-as-judge, fact-checking)\n- \u2705 **Output validation** (schema enforcement with Pydantic)\n- \u2705 **Evaluation metrics** (accuracy, relevance, quality scores)\n\n### Observability\n\n- \u2705 **Logging** (structured logs for debugging)\n- \u2705 **Metrics** (latency, error rate, token usage, cache hit rate)\n- \u2705 **Tracing** (LangSmith or similar for request tracking)\n- \u2705 **Alerting** (notify on errors, anomalies)\n\n**Next**: See `06_production_patterns_evaluation.ipynb` for detailed implementation of these patterns.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Summary & Key Takeaways\n\n### What You Learned\n\n**Framework Decision-Making**:\n- \u2705 When to use LangChain vs LangGraph\n- \u2705 Decision matrix for architecture choices\n\n**Essential Patterns**:\n- \u2705 Memory management for multi-turn conversations\n- \u2705 Multi-agent supervisor pattern for complex workflows\n- \u2705 Human-in-the-loop for approval workflows\n\n**Production Readiness**:\n- \u2705 Comprehensive checklist for deployment\n- \u2705 Understanding of reliability, security, performance, and quality requirements\n\n### Key Insights\n\n1. **Start Simple**: Use LangChain (LCEL) until you need LangGraph's advanced features\n2. **Memory**: `RunnableWithMessageHistory` is the 2025 standard\n3. **Multi-Agent**: Supervisor pattern scales to complex team workflows\n4. **Human Approval**: Requires checkpointing + interrupts\n5. **Production**: See comprehensive checklist above\n\n### Next Steps\n\n1. **Practice**: Build a conversational chatbot with memory\n2. **Experiment**: Try multi-agent pattern with 3+ agents\n3. **Production Patterns**: Study `06_production_patterns_evaluation.ipynb`\n4. **Deploy**: Follow production checklist before going live\n\n### Resources\n\n- [LangChain Documentation](https://python.langchain.com/)\n- [LangGraph Documentation](https://www.langchain.com/langgraph)\n- [LangSmith for Observability](https://www.langchain.com/langsmith)\n\n---\n\n**Well done!** You now understand the key patterns for building production LLM systems. \ud83c\udf89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Advanced Reference\n\nThis section contains **detailed implementations** for those who want to dive deeper. Not required for the main learning flow.\n\n### Available Examples:\n1. **Advanced State Management** - Custom reducers, Annotated types\n2. **Complex Multi-Agent** - 3+ agent teams with routing logic\n3. **Production Error Handling** - Retry patterns, circuit breakers\n4. **Advanced Memory** - Window memory, summary memory implementations\n\n**Note**: These are reference materials. The concepts above are sufficient for most use cases.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Advanced Example 1: Custom State Reducers\n\nFor specialized state merge behavior beyond default replacement."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced: Custom state reducers\nfrom typing import Annotated\nimport operator\n\nclass AdvancedState(TypedDict):\n    # Append to list\n    messages: Annotated[list, operator.add]\n    \n    # Take maximum value\n    score: Annotated[int, lambda x, y: max(x, y)]\n    \n    # Concatenate strings with separator\n    notes: Annotated[str, lambda x, y: f\"{x} | {y}\"]\n    \n    # Default: replace\n    status: str\n\n# Usage in nodes\ndef node1(state: AdvancedState):\n    return {\n        \"messages\": [\"Message 1\"],\n        \"score\": 10,\n        \"notes\": \"Node1 executed\",\n        \"status\": \"processing\"\n    }\n\ndef node2(state: AdvancedState):\n    return {\n        \"messages\": [\"Message 2\"],\n        \"score\": 15,  # Will take max(10, 15) = 15\n        \"notes\": \"Node2 executed\",\n        \"status\": \"completed\"  # Will replace \"processing\"\n    }\n\nprint(\"\u2705 Advanced state management pattern\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Advanced Example 2: Error Handling with Retry\n\nProduction-grade error handling pattern."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced: Retry logic with exponential backoff\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10)\n)\ndef call_llm_with_retry(prompt: str):\n    \"\"\"LLM call with automatic retry on failures\"\"\"\n    from langchain_openai import ChatOpenAI\n    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n    return llm.invoke(prompt)\n\n# Test\ntry:\n    result = call_llm_with_retry(\"What is 2+2?\")\n    print(f\"Result: {result.content}\")\n    print(\"\u2705 Retry pattern implemented (will retry up to 3 times on failure)\")\nexcept Exception as e:\n    print(f\"Failed after retries: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**End of Notebook**\n\nFor production patterns (guardrails, caching, hallucination detection, etc.), continue to:\n\ud83d\udc49 **06_production_patterns_evaluation.ipynb**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}