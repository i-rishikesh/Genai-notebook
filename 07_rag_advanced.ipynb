{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Advanced RAG Techniques\n\n**GenAI Foundation Training - Day 2**\n\n---\n\n## Prerequisites\n\n**Before starting this notebook, you should have:**\n\n‚úÖ **Completed Notebook 03 (LangChain Essentials)** - LCEL, document loaders, text splitters, vector stores, basic RAG pipeline  \n‚úÖ Understanding of embeddings and similarity search  \n‚úÖ Familiarity with ChromaDB basics  \n\n**Note**: This notebook builds on the RAG fundamentals from Notebook 03. If you haven't completed it yet, please do so first.\n\n---\n\n## What You'll Learn\n\nIn this notebook, you'll learn **advanced RAG techniques** beyond the basics:\n\n1. ‚úÖ **Hybrid Search** - Combining semantic and keyword search\n2. ‚úÖ **Query Expansion** - Generating multiple search queries\n3. ‚úÖ **Reranking** - Improving retrieval quality with rerankers\n4. ‚úÖ **Contextual Compression** - Extracting relevant excerpts\n5. ‚úÖ **Multi-Query Retrieval** - Parallel searches with different perspectives\n6. ‚úÖ **Production Optimizations** - Caching, batching, streaming\n7. ‚úÖ **Advanced Citations** - Relevance scores and metadata tracking\n\n---\n\n## What is Advanced RAG?\n\nWhile basic RAG (from Notebook 03) works well for many use cases, production systems often need:\n\n- **Better retrieval quality**: Not all relevant documents rank highest\n- **Query understanding**: Handle complex or ambiguous questions\n- **Context optimization**: Too much context ‚Üí costs, too little ‚Üí poor answers\n- **Performance**: Reduce latency and API costs\n\n### Advanced RAG Techniques\n\n| Technique | Problem Solved | Example |\n|-----------|----------------|---------|\n| **Hybrid Search** | Pure semantic search misses exact matches | \"GPT-4\" vs \"generative model\" |\n| **Query Expansion** | Single query misses related docs | \"ML bias\" ‚Üí [\"algorithmic bias\", \"fairness in AI\"] |\n| **Reranking** | Top-K results may not be most relevant | Rerank with cross-encoder |\n| **Compression** | Retrieved docs too long/irrelevant | Extract only relevant sentences |\n\n---\n\n## What We'll Build\n\nBy the end of this notebook, you'll have:\n\nüéØ Hybrid search combining semantic + keyword  \nüéØ Multi-query retrieval system  \nüéØ Reranking pipeline for improved quality  \nüéØ Production-optimized RAG with caching  \n\nLet's dive into advanced RAG! üöÄ"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: What is LangChain?\n",
    "\n",
    "Before diving into RAG, let's understand the framework we'll use.\n",
    "\n",
    "## What is LangChain?\n",
    "\n",
    "**LangChain** is a framework for building applications powered by large language models (LLMs).\n",
    "\n",
    "### Why Use LangChain?\n",
    "\n",
    "‚úÖ **Abstracts Complexity** - Handles boilerplate code for you  \n",
    "‚úÖ **Reusable Components** - Pre-built loaders, splitters, chains  \n",
    "‚úÖ **Production-Ready** - Battle-tested patterns  \n",
    "‚úÖ **Framework Agnostic** - Works with OpenAI, Anthropic, Google, etc.  \n",
    "\n",
    "## Key Concepts We'll Use Today\n",
    "\n",
    "| Component | What It Does | Example |\n",
    "|-----------|--------------|----------|\n",
    "| **Document Loaders** | Load files (PDF, TXT, web) | PyPDFLoader, TextLoader |\n",
    "| **Text Splitters** | Chunk documents intelligently | RecursiveCharacterTextSplitter |\n",
    "| **Vector Stores** | Integrate with vector DBs | Chroma, FAISS |\n",
    "| **Chains** | Connect components together | RetrievalChain |\n",
    "| **Memory** | Maintain conversation history | ConversationBufferMemory |\n",
    "\n",
    "## The LangChain RAG Flow\n",
    "\n",
    "```\n",
    "Document Loaders ‚Üí Text Splitters ‚Üí Embeddings ‚Üí Vector Stores ‚Üí Retrieval Chains ‚Üí Complete RAG\n",
    "```\n",
    "\n",
    "## Important Note\n",
    "\n",
    "üìù **Today's Focus**: We'll learn **minimal LangChain** - just what's needed for RAG.\n",
    "\n",
    "üîÆ **Later in Training**: We'll cover **LangChain/LangGraph advanced patterns** (custom chains, agents, multi-step workflows).\n",
    "\n",
    "---\n",
    "\n",
    "Let's start building! üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Package Installation\n",
    "\n",
    "## Installing Latest LangChain Packages (December 2025)\n",
    "\n",
    "‚ö†Ô∏è **Important**: LangChain packages are now **modular**. We need separate packages for different integrations.\n",
    "\n",
    "### Packages We'll Install:\n",
    "\n",
    "| Package | Version | Purpose |\n",
    "|---------|---------|----------|\n",
    "| `langchain` | >=0.3.0 | Core framework (new retrieval chain APIs) |\n",
    "| `langchain-openai` | >=0.2.0 | OpenAI integrations (ChatOpenAI, embeddings) |\n",
    "| `langchain-chroma` | >=0.1.2 | ChromaDB integration (**separate package**) |\n",
    "| `langchain-community` | >=0.3.0 | Community integrations (PyPDFLoader, FAISS) |\n",
    "| `langchain-text-splitters` | >=0.3.0 | Text splitting (**separate package**) |\n",
    "| `pypdf` | >=5.1.0 | PDF parsing backend |\n",
    "| `chromadb` | >=0.4.0 | Vector database client |\n",
    "\n",
    "Let's install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uninstall existing langchain packages (Colab clean slate)\n!pip uninstall -y langchain langchain-core langchain-community langchain-openai langchain-chroma langchain-text-splitters\n\n# Install with compatible versions (let pip resolve dependencies)\n!pip install -qU \\\n    langchain \\\n    langchain-openai \\\n    langchain-chroma \\\n    langchain-community \\\n    langchain-text-splitters \\\n    pypdf \\\n    chromadb\n\n# Show installed versions for verification\n!pip list | grep langchain\n\nprint(\"\\n‚úÖ All packages installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation\n",
    "\n",
    "Let's verify that the packages are installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_openai\n",
    "import langchain_chroma\n",
    "import langchain_community\n",
    "import langchain_text_splitters\n",
    "import chromadb\n",
    "\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(f\"‚úÖ ChromaDB version: {chromadb.__version__}\")\n",
    "print(\"\\nüéâ All imports successful! Ready to build RAG.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### üîç Verify Imports\n\nAfter installation, let's verify that the critical imports work correctly:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Verify LCEL imports (modern LangChain 1.x approach)\ntry:\n    from langchain_core.prompts import ChatPromptTemplate\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_core.runnables import RunnablePassthrough\n    print(\"‚úÖ LCEL imports successful!\")\n    print(\"‚ÑπÔ∏è  Note: LangChain 1.x uses LCEL (pipe syntax) instead of legacy chains\")\nexcept ImportError as e:\n    print(f\"‚ùå Import failed: {e}\")\n    print(\"\\nüîß Troubleshooting:\")\n    print(\"1. Restart Colab runtime: Runtime > Restart runtime\")\n    print(\"2. Re-run installation cell above\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup API Keys\n",
    "\n",
    "We'll need an OpenAI API key for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "print(\"‚úÖ API key set successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìå 2025 LangChain Updates\n",
    "\n",
    "**Important**: This notebook uses the latest 2025 LangChain APIs.\n",
    "\n",
    "**Key Import Changes**:\n",
    "- `langchain_community` - Community integrations (loaders, vector stores)\n",
    "- `langchain_chroma` - ChromaDB integration (NEW package)\n",
    "- `langchain_text_splitters` - Text splitting (NEW separate package)\n",
    "- `create_retrieval_chain` - Replaces deprecated RetrievalQA\n",
    "\n",
    "All imports below use current 2025 standards.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Document Loading with LangChain\n",
    "\n",
    "The first step in RAG is loading documents. LangChain provides **Document Loaders** for this.\n",
    "\n",
    "## What is a Document Loader?\n",
    "\n",
    "A Document Loader:\n",
    "- Reads files from various sources (PDF, TXT, web, databases)\n",
    "- Extracts text content\n",
    "- Preserves metadata (source, page numbers, etc.)\n",
    "\n",
    "## Document Structure\n",
    "\n",
    "Each loaded document has:\n",
    "- `page_content`: The actual text\n",
    "- `metadata`: Dictionary with source info (file path, page number, etc.)\n",
    "\n",
    "## Common Document Loaders\n",
    "\n",
    "| Loader | File Type | Use Case |\n",
    "|--------|-----------|----------|\n",
    "| `PyPDFLoader` | PDF | Research papers, reports |\n",
    "| `TextLoader` | TXT | Plain text files |\n",
    "| `DirectoryLoader` | Multiple files | Bulk loading |\n",
    "| `WebBaseLoader` | Web pages | Scrape websites |\n",
    "\n",
    "Let's load a sample document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sample Document\n",
    "\n",
    "First, let's create a sample text file to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample document about Machine Learning\n",
    "sample_content = \"\"\"Machine Learning: A Comprehensive Guide\n",
    "\n",
    "Introduction to Machine Learning\n",
    "Machine learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from and make decisions based on data. Unlike traditional programming where rules are explicitly coded, machine learning algorithms learn patterns from data.\n",
    "\n",
    "Types of Machine Learning\n",
    "There are three main types of machine learning:\n",
    "\n",
    "1. Supervised Learning: The algorithm learns from labeled data. Examples include classification and regression tasks. Common algorithms include linear regression, logistic regression, decision trees, and neural networks.\n",
    "\n",
    "2. Unsupervised Learning: The algorithm finds patterns in unlabeled data. Examples include clustering and dimensionality reduction. Common algorithms include K-means clustering and principal component analysis (PCA).\n",
    "\n",
    "3. Reinforcement Learning: The algorithm learns through trial and error by receiving rewards or penalties. This is commonly used in robotics, game playing, and autonomous systems.\n",
    "\n",
    "Deep Learning\n",
    "Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks). It has revolutionized fields like computer vision, natural language processing, and speech recognition. Popular frameworks include TensorFlow, PyTorch, and Keras.\n",
    "\n",
    "Applications of Machine Learning\n",
    "Machine learning is used in various domains:\n",
    "- Healthcare: Disease diagnosis, drug discovery\n",
    "- Finance: Fraud detection, algorithmic trading\n",
    "- E-commerce: Recommendation systems, demand forecasting\n",
    "- Transportation: Autonomous vehicles, route optimization\n",
    "- Natural Language Processing: Chatbots, translation, sentiment analysis\n",
    "\n",
    "Challenges in Machine Learning\n",
    "Despite its success, machine learning faces several challenges:\n",
    "- Data quality and quantity requirements\n",
    "- Model interpretability and explainability\n",
    "- Bias and fairness concerns\n",
    "- Computational resource requirements\n",
    "- Overfitting and generalization issues\n",
    "\n",
    "The Future of Machine Learning\n",
    "The field continues to evolve with trends like AutoML, federated learning, and edge AI. As computing power increases and algorithms improve, machine learning will become even more integral to our daily lives.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"ml_guide.txt\", \"w\") as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(\"‚úÖ Sample document created: ml_guide.txt\")\n",
    "print(f\"Document length: {len(sample_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Document with TextLoader\n",
    "\n",
    "Now let's load our sample document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader(\"ml_guide.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} document(s)\")\n",
    "print(f\"\\nDocument structure:\")\n",
    "print(f\"- page_content: {len(documents[0].page_content)} characters\")\n",
    "print(f\"- metadata: {documents[0].metadata}\")\n",
    "\n",
    "print(f\"\\nFirst 300 characters:\")\n",
    "print(documents[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Document Metadata\n",
    "\n",
    "Metadata is crucial for RAG because it enables:\n",
    "- **Citations**: Show users where answers came from\n",
    "- **Filtering**: Search only specific sources\n",
    "- **Tracking**: Monitor which documents are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect metadata\n",
    "for doc in documents:\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Key Takeaways\n",
    "\n",
    "‚úÖ Document loaders extract text and preserve metadata  \n",
    "‚úÖ Use `langchain_community.document_loaders` for imports (2025)  \n",
    "‚úÖ Each document has `page_content` (text) and `metadata` (source info)  \n",
    "‚úÖ Metadata enables citations and filtering  \n",
    "\n",
    "**Next**: We'll chunk these documents into smaller pieces for better retrieval! üìÑ‚û°Ô∏èüìÑüìÑüìÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 4: Advanced Text Splitting with LangChain\n\nYou already learned chunking in the previous notebook. Now let's see how **LangChain** makes it even better!\n\n## Why Use LangChain Text Splitters?\n\n| Manual Chunking (Previous Notebook) | LangChain Splitters |\n|-------------------------------------|---------------------|\n| We wrote chunking logic ourselves | Pre-built, tested splitters |\n| Basic fixed-size or sentence split | Intelligent recursive splitting |\n| Manual edge case handling | Handles edge cases automatically |\n| Good for learning | Production-ready |\n\n## LangChain Text Splitters\n\n### RecursiveCharacterTextSplitter (Recommended)\n\nThis splitter:\n- Tries to split on paragraphs (`\\n\\n`) first\n- Falls back to sentences (`. `)\n- Then words (` `)\n- Finally characters\n\nThis preserves semantic meaning better!\n\n‚ö†Ô∏è **Import Change (2025)**: Text splitters are now in `langchain_text_splitters` package"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,  # Characters per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks (10%)\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    ")\n",
    "\n",
    "# Split our documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split {len(documents)} document(s) into {len(chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(chunks[0].page_content[:200] + \"...\")\n",
    "print(f\"\\nChunk metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Why 512 Characters + 50 Overlap?\n\n‚úÖ **Sweet spot**: 512 chars ‚âà 128 tokens (good balance)  \n‚úÖ **Overlap**: Maintains context across chunks  \n‚úÖ **Not too small**: Enough context for LLM  \n‚úÖ **Not too large**: Precise retrieval  \n\n### Compare with Manual Chunking\n\nRemember from the previous notebook? We did this manually:\n\n```python\n# Manual chunking (what we did before)\ndef chunk_by_tokens(text, chunk_size=512, overlap=50):\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), chunk_size - overlap):\n        chunk = ' '.join(words[i:i + chunk_size])\n        chunks.append(chunk)\n    return chunks\n```\n\n**LangChain's advantage**: Handles edge cases, preserves metadata, smarter separators!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect chunk sizes\nchunk_sizes = [len(chunk.page_content) for chunk in chunks]\n\nprint(f\"Chunk size statistics:\")\nprint(f\"- Average: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\nprint(f\"- Min: {min(chunk_sizes)} characters\")\nprint(f\"- Max: {max(chunk_sizes)} characters\")\nprint(f\"\\nAll chunks have metadata: {all(chunk.metadata for chunk in chunks)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üìù Key Takeaways\n\n‚úÖ LangChain splitters are **production-ready** versions of what we learned  \n‚úÖ Use `langchain_text_splitters.RecursiveCharacterTextSplitter` (2025)  \n‚úÖ 512 characters + 50 overlap is a good default  \n‚úÖ Splitters preserve metadata automatically  \n\n**Next**: We'll store these chunks in a vector database! üóÑÔ∏è"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 5: Vector Stores with LangChain\n\nYou already know ChromaDB. Now let's use LangChain's wrapper!\n\n‚ö†Ô∏è **Import Change (2025)**: Chroma is now in `langchain_chroma` package"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚úÖ CORRECT imports (2025)\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\nvector_store = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    collection_name=\"rag_demo\"\n)\n\nprint(f\"‚úÖ Created vector store with {len(chunks)} chunks\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test search\nresults = vector_store.similarity_search(\"What is deep learning?\", k=3)\n\nfor i, doc in enumerate(results, 1):\n    print(f\"{i}. {doc.page_content[:100]}...\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 6: Retrieval Chains with LCEL\n\n### üìå Modern LangChain 1.x: LCEL (Expression Language)\n\n**This notebook uses LCEL - the modern LangChain approach.**\n\n**What is LCEL?**\n- ‚úÖ Pipe operator syntax: `retriever | format | prompt | llm | parser`\n- ‚úÖ Composable: Chain components together naturally\n- ‚úÖ Built-in streaming, async, and batch processing\n- ‚úÖ Recommended for all new LangChain 1.x code\n\n**Why LCEL instead of legacy chains?**\n- ‚ùå `create_retrieval_chain` - removed in LangChain 1.0\n- ‚ùå `ConversationalRetrievalChain` - removed in LangChain 1.0\n- ‚úÖ LCEL is simpler, more powerful, and future-proof\n\n**LCEL RAG Pattern**:\n```python\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt | llm | StrOutputParser()\n)\n```\n\nLet's build a RAG chain using LCEL!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build RAG chain using LCEL (LangChain Expression Language)\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Setup components\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Create prompt template\ntemplate = \"\"\"Answer the question based on the following context:\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\n# Helper function to format documents\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Build LCEL chain with retriever\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"‚úÖ RAG chain created using LCEL!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the RAG chain with LCEL\nquestion = \"What are the types of machine learning?\"\n\n# With LCEL, we invoke with the question directly (simpler!)\nanswer = rag_chain.invoke(question)\n\nprint(f\"Question: {question}\\n\")\nprint(f\"Answer: {answer}\")\nprint(\"\\n‚úÖ LCEL makes RAG simple and clean!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 7: Complete RAG Pipeline with LCEL\n\nLet's create a reusable RAG pipeline class using LCEL:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports (global scope - best practice)\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nclass RAGPipeline:\n    \"\"\"Production-ready RAG pipeline using LCEL\"\"\"\n\n    def __init__(self, file_path):\n        # Load and chunk documents\n        loader = TextLoader(file_path)\n        documents = loader.load()\n\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n        chunks = text_splitter.split_documents(documents)\n\n        # Create vector store\n        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n        self.vector_store = Chroma.from_documents(chunks, embeddings)\n        self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n        # Helper to format docs\n        def format_docs(docs):\n            return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n        # Build LCEL chain\n        template = \"\"\"Answer based on context:\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n        prompt = ChatPromptTemplate.from_template(template)\n\n        self.rag_chain = (\n            {\"context\": self.retriever | format_docs, \"question\": RunnablePassthrough()}\n            | prompt\n            | self.llm\n            | StrOutputParser()\n        )\n\n        print(f\"‚úÖ RAG Pipeline ready with {len(chunks)} chunks (using LCEL)\")\n\n    def ask(self, question):\n        \"\"\"Ask a question and get answer with sources\"\"\"\n        answer = self.rag_chain.invoke(question)\n        # Get source docs separately for citations\n        docs = self.retriever.invoke(question)\n        return {\"answer\": answer, \"sources\": docs}\n\n# Create pipeline\npipeline = RAGPipeline(\"ml_guide.txt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the pipeline\nresponse = pipeline.ask(\"What are machine learning challenges?\")\n\nprint(\"Answer:\", response[\"answer\"])\nprint(f\"\\n‚úÖ Retrieved {len(response['sources'])} source documents\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 8: Citations & Source Attribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Citations with LCEL\ndef ask_with_citations(question):\n    \"\"\"Ask question and return answer with source citations\"\"\"\n    # Get answer from chain\n    answer = rag_chain.invoke(question)\n    \n    # Get source documents separately\n    source_docs = retriever.invoke(question)\n    \n    # Format citations\n    citations = []\n    for i, doc in enumerate(source_docs, 1):\n        source = doc.metadata.get(\"source\", \"Unknown\")\n        citations.append(f\"[{i}] {source}: {doc.page_content[:100]}...\")\n    \n    return f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(citations)\n\n# Test with citations\nresult = ask_with_citations(\"What is deep learning?\")\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 9: Conversation Memory with LCEL\n\nBuild a conversational RAG system that remembers chat history:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Conversational RAG using LCEL\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.messages import HumanMessage, AIMessage\n\n# Helper to format docs\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Contextualize question prompt\ncontextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\nwhich might reference context in the chat history, formulate a standalone question \\\nwhich can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed and otherwise return it as is.\"\"\"\n\ncontextualize_q_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", contextualize_q_system_prompt),\n    MessagesPlaceholder(\"chat_history\"),\n    (\"human\", \"{input}\"),\n])\n\n# Create contextualized question chain\ncontextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n\n# QA prompt with history\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, say that you don't know.\n\nContext: {context}\"\"\"\n\nqa_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", qa_system_prompt),\n    MessagesPlaceholder(\"chat_history\"),\n    (\"human\", \"{input}\"),\n])\n\n# Build conversational RAG chain with LCEL\nconversational_rag_chain = (\n    RunnablePassthrough.assign(\n        context=lambda x: format_docs(\n            retriever.invoke(\n                contextualize_q_chain.invoke(x) if x.get(\"chat_history\") else x[\"input\"]\n            )\n        )\n    )\n    | qa_prompt\n    | llm\n    | StrOutputParser()\n)\n\n# Example usage\nchat_history = []\n\n# First question\nresponse1 = conversational_rag_chain.invoke({\n    \"input\": \"What is supervised learning?\",\n    \"chat_history\": chat_history\n})\n\nchat_history.extend([\n    HumanMessage(content=\"What is supervised learning?\"),\n    AIMessage(content=response1)\n])\n\n# Second question (references first)\nresponse2 = conversational_rag_chain.invoke({\n    \"input\": \"What about unsupervised learning?\",\n    \"chat_history\": chat_history\n})\n\nprint(\"Q1:\", response1)\nprint(\"\\nQ2:\", response2)\nprint(\"\\n‚úÖ Conversational RAG using LCEL!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Section 10: Summary & Next Steps\n\n## What You Learned\n\n‚úÖ LangChain basics for RAG\n‚úÖ Latest 2025 APIs (create_retrieval_chain)\n‚úÖ Complete RAG pipeline\n‚úÖ Citations and memory\n\n## Key Updates (2025)\n\n| Old | New |\n|-----|-----|\n| `langchain.document_loaders` | `langchain_community.document_loaders` |\n| `langchain.text_splitter` | `langchain_text_splitters` |\n| `langchain.vectorstores` | `langchain_chroma` |\n| `RetrievalQA` | `create_retrieval_chain` |\n\n## Next Steps\n\nüîÆ LangChain/LangGraph advanced patterns\nüîÆ Function calling & agents\nüîÆ Security & guardrails\n\n**Congratulations! You've built a production-ready RAG system!** üéâ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}