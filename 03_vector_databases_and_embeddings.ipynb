{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases and Embeddings\n",
    "\n",
    "## GenAI Foundation Training - Day 2\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn the foundational concepts behind semantic search and RAG (Retrieval-Augmented Generation) systems:\n",
    "\n",
    "1. **What embeddings are** and why they're crucial for modern AI applications\n",
    "2. **Generate embeddings** using OpenAI, Google AI, and open-source models\n",
    "3. **Implement similarity search** with different metrics (cosine, Euclidean)\n",
    "4. **Choose appropriate chunking strategies** for your documents\n",
    "5. **Work with vector databases** (ChromaDB) for semantic search\n",
    "6. **Know when to use** vector databases vs traditional databases\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Embeddings and vector databases are the foundation of:\n",
    "- **RAG systems** - Chatbots with custom knowledge\n",
    "- **Semantic search** - Finding documents by meaning, not just keywords\n",
    "- **Recommendation engines** - \"Users who liked X also liked Y\"\n",
    "- **Duplicate detection** - Finding similar content at scale\n",
    "\n",
    "### Connection to Previous Notebook\n",
    "\n",
    "In the previous notebook, we learned how to:\n",
    "- Make API calls to LLMs (OpenAI, Anthropic, Google AI)\n",
    "- Get structured outputs with Pydantic\n",
    "\n",
    "Now we'll learn how to:\n",
    "- Store and retrieve relevant context for those LLMs\n",
    "- Build the \"R\" in RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "### Duration\n",
    "\n",
    "This notebook takes approximately **2 hours** to complete.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Completed Environment Setup notebook (or have API keys ready)\n",
    "- OpenAI and/or Google AI API keys\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Package Installation\n",
    "\n",
    "Let's install all the packages we'll need for this notebook.\n",
    "\n",
    "### Packages We'll Use:\n",
    "\n",
    "- **openai** - OpenAI API client for generating embeddings\n",
    "- **google-generativeai** - Google AI client for Gemini embeddings\n",
    "- **sentence-transformers** - Open-source embedding models that run locally\n",
    "- **chromadb** - Vector database for storing and searching embeddings\n",
    "- **numpy** - Numerical operations on vectors\n",
    "- **scikit-learn** - Machine learning utilities (similarity metrics)\n",
    "\n",
    "Run the cell below to install everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install openai google-generativeai sentence-transformers chromadb numpy scikit-learn -q\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# LLM clients\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vector database\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup API Keys (Using Google Colab Secrets)\n",
    "\n",
    "We'll use the same secure approach from the previous notebook.\n",
    "\n",
    "**Setting Up Google Colab Secrets:**\n",
    "\n",
    "1. Click the **ðŸ”‘ (key icon)** in the left sidebar\n",
    "2. Add these secrets:\n",
    "   - `OPENAI_API_KEY` - Your OpenAI API key\n",
    "   - `GOOGLE_API_KEY` - Your Google AI API key (optional)\n",
    "3. Toggle **\"Notebook access\"** ON for each key\n",
    "\n",
    "**Note:** You'll need at least OpenAI API key for this notebook. Google AI is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import userdata for Colab secrets\n",
    "from google.colab import userdata\n",
    "\n",
    "# Retrieve API keys from Colab secrets\n",
    "try:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"âœ… OpenAI API key loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading OpenAI API key: {e}\")\n",
    "    print(\"Please set OPENAI_API_KEY in Google Colab Secrets.\")\n",
    "\n",
    "# Setup Google AI (optional)\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    print(\"âœ… Google AI API key loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Google AI API key not found (optional): {e}\")\n",
    "\n",
    "print(\"\\nâœ… API keys configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: What Are Embeddings?\n",
    "\n",
    "### The Core Concept\n",
    "\n",
    "**Embeddings** are dense numerical representations of text that capture semantic meaning.\n",
    "\n",
    "Think of embeddings as coordinates in a multi-dimensional space where:\n",
    "- Similar concepts are close together\n",
    "- Different concepts are far apart\n",
    "- The distance between vectors = similarity in meaning\n",
    "\n",
    "### Why Embeddings Matter\n",
    "\n",
    "**Traditional keyword search:**\n",
    "- \"cat\" only matches documents containing \"cat\"\n",
    "- Misses \"feline\", \"kitten\", \"pet\"\n",
    "\n",
    "**Semantic search with embeddings:**\n",
    "- \"cat\" finds documents about cats, felines, kittens, pets\n",
    "- Understands meaning, not just exact words\n",
    "\n",
    "### The Famous Example\n",
    "\n",
    "Embeddings capture relationships:\n",
    "```\n",
    "King - Man + Woman â‰ˆ Queen\n",
    "```\n",
    "\n",
    "This works because embeddings capture semantic relationships in vector space!\n",
    "\n",
    "### Dimensionality\n",
    "\n",
    "Embeddings are vectors with many dimensions:\n",
    "- **384 dimensions** - Lightweight models (all-MiniLM-L6-v2)\n",
    "- **768 dimensions** - Medium models (Google embedding-001)\n",
    "- **1536 dimensions** - Large models (OpenAI text-embedding-3-small)\n",
    "- **3072 dimensions** - Extra large (OpenAI text-embedding-3-large)\n",
    "\n",
    "More dimensions = more nuanced representations, but also more storage and computation.\n",
    "\n",
    "### How LLMs Use Embeddings\n",
    "\n",
    "LLMs work with numbers, not text:\n",
    "1. Text â†’ Embeddings (numbers)\n",
    "2. LLM processes numbers\n",
    "3. Output numbers â†’ Text\n",
    "\n",
    "Embeddings are the bridge between human language and machine learning.\n",
    "\n",
    "### Let's See an Example\n",
    "\n",
    "We'll generate an embedding for a simple sentence and look at the raw numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample embedding\n",
    "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
    "\n",
    "response = openai_client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=sample_text\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"\\nEmbedding dimensions: {len(embedding)}\")\n",
    "print(f\"\\nFirst 10 values: {embedding[:10]}\")\n",
    "print(f\"\\nThese {len(embedding)} numbers capture the semantic meaning of the text!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Understanding\n",
    "\n",
    "Imagine a 2D space (in reality it's 1536D, but we can't visualize that!):\n",
    "\n",
    "```\n",
    "       ^                     \n",
    "       |   dog              \n",
    "       | cat  puppy          \n",
    "       |                    \n",
    "  -----.------------------>  \n",
    "       |                     \n",
    "       |         car         \n",
    "       |      vehicle        \n",
    "```\n",
    "\n",
    "- \"cat\", \"dog\", \"puppy\" are close (similar meanings)\n",
    "- \"car\", \"vehicle\" are close\n",
    "- Animals and vehicles are far apart\n",
    "\n",
    "This is how embeddings represent meaning in high-dimensional space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Generating Embeddings - OpenAI\n",
    "\n",
    "OpenAI offers two main embedding models:\n",
    "\n",
    "| Model | Dimensions | Cost per 1M tokens | Quality | Use Case |\n",
    "|-------|------------|-------------------|---------|----------|\n",
    "| text-embedding-3-small | 1536 | $0.02 | Excellent | Production, cost-effective |\n",
    "| text-embedding-3-large | 3072 | $0.13 | Best | When quality is critical |\n",
    "\n",
    "### Single Embedding\n",
    "\n",
    "Let's create a helper function to generate embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(text: str, model: str = \"text-embedding-3-small\") -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embedding using OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        model: OpenAI embedding model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of floats representing the embedding\n",
    "    \"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Test it\n",
    "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
    "embedding = get_openai_embedding(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(\"\\nâœ… OpenAI embedding generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Embeddings (More Efficient)\n",
    "\n",
    "When embedding multiple texts, it's much more efficient to send them in a batch:\n",
    "- **Faster** - Single API call instead of multiple\n",
    "- **Cheaper** - Reduced network overhead\n",
    "- **Better rate limits** - Fewer requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embeddings_batch(texts: List[str], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for multiple texts efficiently.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        model: OpenAI embedding model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings (each embedding is a list of floats)\n",
    "    \"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=model,\n",
    "        input=texts\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "# Test with multiple texts\n",
    "texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Machine learning uses algorithms to learn from data\",\n",
    "    \"Neural networks are inspired by biological neurons\"\n",
    "]\n",
    "\n",
    "embeddings = get_openai_embeddings_batch(texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings in a single API call\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")\n",
    "print(\"\\nâœ… Batch embeddings generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Which Model\n",
    "\n",
    "**Use text-embedding-3-small when:**\n",
    "- Building most production applications\n",
    "- Cost is a consideration\n",
    "- Quality is already sufficient (it's quite good!)\n",
    "\n",
    "**Use text-embedding-3-large when:**\n",
    "- Quality is absolutely critical\n",
    "- Working with complex, nuanced content\n",
    "- Budget allows for premium quality\n",
    "\n",
    "**Pro tip:** Start with small, upgrade to large only if needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Generating Embeddings - Google AI & Open-Source\n",
    "\n",
    "OpenAI isn't the only option! Let's explore alternatives:\n",
    "\n",
    "### Provider Comparison\n",
    "\n",
    "| Provider | Model | Dimensions | Cost | Speed | Quality | Privacy |\n",
    "|----------|-------|------------|------|-------|---------|----------|\n",
    "| OpenAI | text-embedding-3-small | 1536 | $0.02/1M | Fast | Excellent | API call |\n",
    "| OpenAI | text-embedding-3-large | 3072 | $0.13/1M | Medium | Best | API call |\n",
    "| Google | embedding-001 | 768 | Free tier | Fast | Good | API call |\n",
    "| Open-source | all-MiniLM-L6-v2 | 384 | Free | Fastest | Good | 100% local |\n",
    "\n",
    "### Google AI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embedding using Google AI.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "    \n",
    "    Returns:\n",
    "        List of floats representing the embedding\n",
    "    \"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"  # Optimized for retrieval\n",
    "    )\n",
    "    return result['embedding']\n",
    "\n",
    "# Test it\n",
    "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
    "embedding = get_google_embedding(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(\"\\nâœ… Google AI embedding generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-Source Embeddings (Sentence Transformers)\n",
    "\n",
    "**Sentence Transformers** is a Python library that provides state-of-the-art sentence, text, and image embeddings.\n",
    "\n",
    "**Benefits:**\n",
    "- Runs 100% locally (no API calls)\n",
    "- Complete privacy (data never leaves your machine)\n",
    "- Free (no usage costs)\n",
    "- Fast (especially on GPU)\n",
    "\n",
    "**Popular models:**\n",
    "- `all-MiniLM-L6-v2` - 384 dimensions, fast, good quality\n",
    "- `all-mpnet-base-v2` - 768 dimensions, higher quality\n",
    "- `multi-qa-MiniLM-L6-cos-v1` - Optimized for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local model (downloads on first use, then cached)\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "local_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Model loaded!\\n\")\n",
    "\n",
    "def get_local_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embedding using local Sentence Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "    \n",
    "    Returns:\n",
    "        List of floats representing the embedding\n",
    "    \"\"\"\n",
    "    return local_model.encode(text).tolist()\n",
    "\n",
    "# Test it\n",
    "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
    "embedding = get_local_embedding(sample_text)\n",
    "\n",
    "print(f\"Text: {sample_text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(\"\\nâœ… Local embedding generated successfully!\")\n",
    "print(\"\\nNote: This ran entirely on your machine - no API call!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Provider\n",
    "\n",
    "**Use OpenAI (text-embedding-3-small) when:**\n",
    "- Building production applications\n",
    "- Need excellent quality\n",
    "- Budget allows ($0.02/1M tokens is quite affordable)\n",
    "\n",
    "**Use OpenAI (text-embedding-3-large) when:**\n",
    "- Quality is absolutely critical\n",
    "- Working with complex, nuanced content\n",
    "\n",
    "**Use Google AI when:**\n",
    "- Experimenting and learning\n",
    "- Have free tier quota\n",
    "- Quality is good enough for your use case\n",
    "\n",
    "**Use Open-Source when:**\n",
    "- Privacy is critical (healthcare, legal, finance)\n",
    "- Working offline or air-gapped environments\n",
    "- Want zero ongoing API costs\n",
    "- Have sufficient compute resources\n",
    "\n",
    "**Pro tip:** Start with OpenAI small for simplicity, consider alternatives based on your constraints!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Similarity Search Fundamentals\n",
    "\n",
    "Now that we can generate embeddings, how do we compare them?\n",
    "\n",
    "### Similarity Metrics\n",
    "\n",
    "**1. Cosine Similarity** (Most Common)\n",
    "- Range: -1 to 1\n",
    "  - 1 = Identical meaning\n",
    "  - 0 = Unrelated\n",
    "  - -1 = Opposite meaning\n",
    "- Measures angle between vectors\n",
    "- Ignores magnitude, focuses on direction\n",
    "\n",
    "**2. Euclidean Distance**\n",
    "- Range: 0 to infinity\n",
    "  - 0 = Identical\n",
    "  - Larger = More different\n",
    "- Straight-line distance between points\n",
    "- Considers magnitude\n",
    "\n",
    "**3. Dot Product**\n",
    "- Similar to cosine but includes magnitude\n",
    "- Used when vector lengths matter\n",
    "\n",
    "For most semantic search use cases, **cosine similarity** is the standard choice.\n",
    "\n",
    "### Let's Compare Some Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec1: First vector\n",
    "        vec2: Second vector\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score between -1 and 1\n",
    "    \"\"\"\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "# Example texts\n",
    "text1 = \"The cat sat on the mat\"\n",
    "text2 = \"A feline rested on the rug\"\n",
    "text3 = \"Python is a programming language\"\n",
    "\n",
    "# Generate embeddings\n",
    "emb1 = get_openai_embedding(text1)\n",
    "emb2 = get_openai_embedding(text2)\n",
    "emb3 = get_openai_embedding(text3)\n",
    "\n",
    "# Calculate similarities\n",
    "sim_1_2 = calculate_cosine_similarity(emb1, emb2)\n",
    "sim_1_3 = calculate_cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(\"Similarity Comparison:\\n\")\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Similarity: {sim_1_2:.4f}\")\n",
    "print(\"\\n---\\n\")\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 3: {text3}\")\n",
    "print(f\"Similarity: {sim_1_3:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Notice: Text 1 and 2 are highly similar (about cats/mats)\")\n",
    "print(\"Text 1 and 3 have low similarity (different topics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Scores\n",
    "\n",
    "**High similarity (0.7 - 1.0):**\n",
    "- Same topic, similar meaning\n",
    "- Synonyms, paraphrases\n",
    "- Related concepts\n",
    "\n",
    "**Medium similarity (0.4 - 0.7):**\n",
    "- Related but distinct topics\n",
    "- Tangentially connected\n",
    "\n",
    "**Low similarity (0.0 - 0.4):**\n",
    "- Different topics\n",
    "- Unrelated content\n",
    "\n",
    "### Top-K Search\n",
    "\n",
    "In real applications, we want to find the **K most similar** documents from a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(query_embedding: List[float], \n",
    "                     document_embeddings: List[List[float]], \n",
    "                     documents: List[str],\n",
    "                     k: int = 3) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Find top-k most similar documents to a query.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Embedding of the search query\n",
    "        document_embeddings: List of document embeddings\n",
    "        documents: List of document texts\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (index, document, similarity_score)\n",
    "    \"\"\"\n",
    "    # Calculate similarities\n",
    "    similarities = [\n",
    "        cosine_similarity([query_embedding], [doc_emb])[0][0]\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    \n",
    "    # Get top k indices (argsort returns ascending, so reverse it)\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # Return results\n",
    "    return [\n",
    "        (idx, documents[idx], similarities[idx]) \n",
    "        for idx in top_k_indices\n",
    "    ]\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Python is a popular programming language for data science\",\n",
    "    \"Machine learning uses algorithms to learn from data\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"The cat sat on the mat and looked out the window\",\n",
    "    \"Computer vision enables machines to interpret visual information\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "doc_embeddings = get_openai_embeddings_batch(documents)\n",
    "\n",
    "# Search query\n",
    "query = \"How do neural networks work?\"\n",
    "query_embedding = get_openai_embedding(query)\n",
    "\n",
    "# Find top 3 most similar\n",
    "results = find_most_similar(query_embedding, doc_embeddings, documents, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 most similar documents:\\n\")\n",
    "for rank, (idx, doc, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}. Similarity: {score:.4f}\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Distance Metrics (Brief Overview)\n",
    "\n",
    "While cosine similarity is most common, here are other options:\n",
    "\n",
    "**Euclidean Distance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidean_distance(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate Euclidean distance (smaller = more similar)\"\"\"\n",
    "    return euclidean_distances([vec1], [vec2])[0][0]\n",
    "\n",
    "# Compare\n",
    "dist = calculate_euclidean_distance(emb1, emb2)\n",
    "print(f\"Euclidean distance between similar texts: {dist:.4f}\")\n",
    "print(\"(Smaller distance = more similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot Product:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dot_product(vec1: List[float], vec2: List[float]) -> float:\n",
    "    \"\"\"Calculate dot product (larger = more similar)\"\"\"\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "# Compare\n",
    "dot = calculate_dot_product(emb1, emb2)\n",
    "print(f\"Dot product between similar texts: {dot:.4f}\")\n",
    "print(\"(Larger value = more similar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Metric to Use?\n",
    "\n",
    "**For most semantic search: Use Cosine Similarity**\n",
    "- Standard in the industry\n",
    "- Works well with normalized embeddings\n",
    "- Easy to interpret (0-1 range)\n",
    "\n",
    "**When to consider alternatives:**\n",
    "- Euclidean: When magnitude matters\n",
    "- Dot product: For certain specialized models\n",
    "\n",
    "When in doubt, stick with cosine!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### When to Use Each Similarity Metric - Detailed Guide\n\nNow that you know **how** to calculate each metric, let's understand **when** to use each one.\n\n#### ðŸ“Š Quick Decision Guide\n\n| Metric | Use When | Common Applications | Don't Use When |\n|--------|----------|---------------------|----------------|\n| **Cosine Similarity** | Direction matters, magnitude doesn't | Text search, RAG, semantic similarity | Vectors need exact magnitude comparison |\n| **Euclidean Distance** | Magnitude matters, absolute distance counts | Clustering, spatial data, image similarity | Comparing different-length vectors |\n| **Dot Product** | Both direction AND magnitude matter | Ranking, scoring, specialized models | General semantic search |\n\n---\n\n#### ðŸŽ¯ Cosine Similarity - The Default Choice (90%+ of use cases)\n\n**Use for:**\n- âœ… **Text/document similarity** - Finding semantically similar content\n- âœ… **RAG systems** - Retrieving relevant context for LLMs\n- âœ… **Semantic search** - \"Find documents about machine learning\"\n- âœ… **Recommendation engines** - \"Users who liked X also liked Y\"\n- âœ… **Normalized embeddings** - When vectors are already normalized (most embedding models)\n\n**Why it works:**\n- Focuses on **direction** (meaning), not magnitude (length)\n- Range 0-1 is easy to interpret (or -1 to 1 if including opposite meanings)\n- Works consistently across different embedding models\n- Not affected by document length or vector normalization\n\n**Real-world examples:**\n```python\n# Example: Finding similar product descriptions\nquery = \"wireless bluetooth headphones\"\n# Cosine similarity finds:\n# 1. \"noise cancelling wireless earbuds\" (0.89)\n# 2. \"bluetooth audio headset\" (0.85)\n# 3. \"over-ear wireless headphones\" (0.83)\n```\n\n**When NOT to use:**\n- âŒ Vectors represent actual measurements/coordinates (use Euclidean)\n- âŒ Model specifically designed for dot product similarity\n\n---\n\n#### ðŸ“ Euclidean Distance - For Clustering & Spatial Data\n\n**Use for:**\n- âœ… **Clustering algorithms** - K-means, DBSCAN (spatial proximity)\n- âœ… **Image embeddings** - Comparing visual similarity where magnitude matters\n- âœ… **User profiles with features** - Age, income, preferences (actual measurements)\n- âœ… **Anomaly detection** - Finding outliers based on distance\n\n**Why it works:**\n- Measures **straight-line distance** in vector space\n- Considers both direction AND magnitude\n- Natural for spatial/geometric data\n- Works well when all dimensions are comparable/normalized\n\n**Real-world examples:**\n```python\n# Example: Clustering user profiles\nuser1 = [25, 50000, 3.5, 2]  # age, income, avg_rating, purchases\nuser2 = [27, 52000, 3.7, 3]  # Similar user (small Euclidean distance)\nuser3 = [65, 150000, 4.8, 50]  # Different segment (large distance)\n\n# K-means clustering uses Euclidean distance to group similar users\n```\n\n**When NOT to use:**\n- âŒ Text embeddings (direction matters more than length)\n- âŒ Comparing vectors of very different scales (need normalization first)\n- âŒ Semantic similarity tasks\n\n**Key difference from Cosine:**\n```python\nvec_a = [1, 0]\nvec_b = [10, 0]  # Same direction, 10x magnitude\n\n# Cosine: 1.0 (identical direction)\n# Euclidean: 9.0 (far apart due to magnitude)\n```\n\n---\n\n#### âš¡ Dot Product - For Ranking & Specialized Models\n\n**Use for:**\n- âœ… **Ranking models** - When model trained with dot product similarity\n- âœ… **Scoring relevance** - Combining direction and magnitude for ranking\n- âœ… **Certain recommendation systems** - Where magnitude encodes confidence/importance\n- âœ… **Dense retrieval models** - Some models (DPR) use dot product natively\n\n**Why it works:**\n- Combines **both direction and magnitude**\n- Can be faster to compute (no normalization needed)\n- Some neural models optimize for dot product directly\n- Magnitude can encode additional signal (confidence, importance)\n\n**Real-world examples:**\n```python\n# Example: Search ranking where confidence matters\nquery_vec = [0.8, 0.6, 0.9]  # High confidence query\n\ndoc1_vec = [0.9, 0.7, 0.85]  # High confidence, similar direction â†’ High score\ndoc2_vec = [0.1, 0.1, 0.15]  # Low confidence, similar direction â†’ Low score\n\n# Dot product ranks doc1 higher (captures both similarity AND confidence)\n```\n\n**When NOT to use:**\n- âŒ General semantic search (use cosine instead)\n- âŒ When you don't know if model is trained for dot product\n- âŒ Comparing unnormalized vectors from different sources\n\n**Relationship to Cosine:**\n```\nDot Product = Cosine Similarity Ã— ||A|| Ã— ||B||\n\nFor normalized vectors (||A|| = ||B|| = 1):\nDot Product = Cosine Similarity\n```\n\n---\n\n#### ðŸ§ª Practical Comparison Example\n\nLet's see how each metric behaves with the same data:\n```python\n# Same query, same documents, different metrics\n\nquery = \"machine learning tutorial\"\ndoc1 = \"beginner guide to ML algorithms\"\ndoc2 = \"introduction to machine learning\"\ndoc3 = \"advanced deep learning research paper\"\n\n# Results with Cosine Similarity:\n# 1. doc2 (0.89) - Very similar meaning\n# 2. doc1 (0.82) - Related but slightly different focus\n# 3. doc3 (0.65) - Same domain but different level\n\n# Results with Euclidean Distance (smaller = better):\n# 1. doc2 (0.15) - Closest in vector space\n# 2. doc1 (0.22) -\n# 3. doc3 (0.45) - Farthest\n\n# Results with Dot Product:\n# (Depends heavily on vector normalization)\n# If normalized: Similar to cosine\n# If not normalized: Magnitude affects ranking\n```\n\n---\n\n#### ðŸ’¡ Best Practice Recommendations\n\n**Default choice:** Start with **Cosine Similarity**\n- Works for 90%+ of semantic search use cases\n- Easy to interpret (0-1 range)\n- Robust across different embedding models\n\n**When to reconsider:**\n1. **Use Euclidean** if:\n   - Doing clustering (K-means, hierarchical)\n   - Vectors represent measurements/coordinates\n   - Magnitude has semantic meaning\n\n2. **Use Dot Product** if:\n   - Model documentation specifically recommends it\n   - Doing ranking/scoring tasks\n   - Working with models like DPR, ColBERT\n\n**Testing approach:**\n```python\n# Test all three metrics with your data\nresults_cosine = search_with_cosine(query)\nresults_euclidean = search_with_euclidean(query)\nresults_dot = search_with_dot_product(query)\n\n# Manually review top 10 results from each\n# Choose the metric that gives best quality\n```\n\n---\n\n#### ðŸ“š Summary Cheat Sheet\n\n**For beginners:**\n- Building a chatbot? â†’ **Cosine Similarity**\n- Semantic search? â†’ **Cosine Similarity**\n- RAG system? â†’ **Cosine Similarity**\n- Not sure? â†’ **Cosine Similarity**\n\n**For specific tasks:**\n- Clustering users/documents? â†’ **Euclidean Distance**\n- Image similarity with CNNs? â†’ **Euclidean Distance**\n- Using DPR or specialized ranking model? â†’ **Dot Product**\n\n**When in doubt:**\nTest cosine first. It's the safest default for semantic similarity tasks!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Chunking Strategies\n",
    "\n",
    "### Why Do We Need Chunking?\n",
    "\n",
    "Real-world documents are often too long for embedding models:\n",
    "- **Token limits:** Most embedding models have 8K token limits\n",
    "- **Precision:** Smaller chunks = more precise retrieval\n",
    "- **Context:** LLMs work better with focused, relevant chunks\n",
    "\n",
    "**The challenge:** How do we split documents effectively?\n",
    "\n",
    "### Chunking Strategy Comparison\n",
    "\n",
    "| Strategy | Chunk Size | Pros | Cons | Use Case |\n",
    "|----------|-----------|------|------|----------|\n",
    "| Fixed-size | 256-512 tokens | Simple, predictable | May split mid-sentence | General purpose |\n",
    "| Sentence-based | 3-5 sentences | Natural boundaries | Variable size | Articles, blogs |\n",
    "| Semantic | Variable | Intelligent splits | Complex, slower | Long documents |\n",
    "\n",
    "### 1. Fixed-Size Chunking\n",
    "\n",
    "Split text into chunks of fixed token/word count with overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_tokens(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into fixed-size chunks with overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        chunk_size: Number of words per chunk\n",
    "        overlap: Number of overlapping words between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:  # Don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "long_text = \"\"\"Machine learning is a subset of artificial intelligence that focuses on \n",
    "building systems that can learn from data. Deep learning is a subset of machine learning \n",
    "that uses neural networks with multiple layers. These neural networks are inspired by the \n",
    "structure of the human brain. Natural language processing is another important area of AI \n",
    "that helps computers understand and generate human language. Computer vision enables machines \n",
    "to interpret and analyze visual information from the world.\"\"\"\n",
    "\n",
    "chunks = chunk_by_tokens(long_text, chunk_size=20, overlap=5)\n",
    "\n",
    "print(f\"Original text length: {len(long_text.split())} words\\n\")\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk.split())} words):\")\n",
    "    print(f\"  {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence-Based Chunking\n",
    "\n",
    "Group sentences together (more natural boundaries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text by sentences, group into chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        sentences_per_chunk: Number of sentences per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (for production, use spaCy or NLTK)\n",
    "    sentences = text.replace('\\n', ' ').split('. ')\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = '. '.join(sentences[i:i + sentences_per_chunk])\n",
    "        if not chunk.endswith('.'):\n",
    "            chunk += '.'\n",
    "        chunks.append(chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example\n",
    "chunks_sentences = chunk_by_sentences(long_text, sentences_per_chunk=2)\n",
    "\n",
    "print(f\"Number of sentence-based chunks: {len(chunks_sentences)}\\n\")\n",
    "for i, chunk in enumerate(chunks_sentences, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chunking with Metadata\n",
    "\n",
    "**Critical concept:** Always include metadata with chunks!\n",
    "\n",
    "Metadata helps with:\n",
    "- **Citations** - Show users where information came from\n",
    "- **Filtering** - Search within specific sources or date ranges\n",
    "- **Debugging** - Track down retrieval issues\n",
    "- **Quality** - Prioritize trusted sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunk:\n",
    "    \"\"\"\n",
    "    Represents a text chunk with metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self, text: str, metadata: Dict):\n",
    "        self.text = text\n",
    "        self.metadata = metadata  # source, page, section, date, author, etc.\n",
    "        self.embedding = None\n",
    "    \n",
    "    def embed(self, embedding_function):\n",
    "        \"\"\"Generate embedding for this chunk.\"\"\"\n",
    "        self.embedding = embedding_function(self.text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DocumentChunk(text='{self.text[:50]}...', metadata={self.metadata})\"\n",
    "\n",
    "# Example: Creating chunks with rich metadata\n",
    "chunks_with_metadata = [\n",
    "    DocumentChunk(\n",
    "        text=\"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data.\",\n",
    "        metadata={\n",
    "            \"source\": \"ai_introduction.pdf\",\n",
    "            \"page\": 1,\n",
    "            \"section\": \"Introduction to Machine Learning\",\n",
    "            \"date\": \"2024-01-15\",\n",
    "            \"author\": \"Dr. Smith\"\n",
    "        }\n",
    "    ),\n",
    "    DocumentChunk(\n",
    "        text=\"Deep learning uses neural networks with multiple layers to learn complex patterns.\",\n",
    "        metadata={\n",
    "            \"source\": \"ai_introduction.pdf\",\n",
    "            \"page\": 2,\n",
    "            \"section\": \"Deep Learning Fundamentals\",\n",
    "            \"date\": \"2024-01-15\",\n",
    "            \"author\": \"Dr. Smith\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "for chunk in chunks_with_metadata:\n",
    "    chunk.embed(get_openai_embedding)\n",
    "\n",
    "print(\"Chunks with metadata:\")\n",
    "for i, chunk in enumerate(chunks_with_metadata, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Text: {chunk.text}\")\n",
    "    print(f\"  Source: {chunk.metadata['source']}\")\n",
    "    print(f\"  Page: {chunk.metadata['page']}\")\n",
    "    print(f\"  Section: {chunk.metadata['section']}\")\n",
    "    print(f\"  Embedding: {len(chunk.embedding)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Chunking\n",
    "\n",
    "**1. Chunk Size:**\n",
    "- **Sweet spot:** 256-512 tokens (~200-400 words)\n",
    "- Too small: Lost context, too many chunks\n",
    "- Too large: Less precise retrieval, may hit limits\n",
    "\n",
    "**2. Overlap:**\n",
    "- **Recommended:** 10-20% of chunk size\n",
    "- Maintains context continuity between chunks\n",
    "- Prevents important information from being split\n",
    "\n",
    "**3. Metadata:**\n",
    "- **Always include:** source, page/location\n",
    "- **Often useful:** date, author, section, category\n",
    "- **For citations:** exact page numbers, URLs\n",
    "\n",
    "**4. Testing:**\n",
    "- Test different strategies with your data\n",
    "- Measure retrieval quality\n",
    "- Iterate based on results\n",
    "\n",
    "### Semantic Chunking (Advanced)\n",
    "\n",
    "**Note:** We won't implement this, but you should know it exists.\n",
    "\n",
    "Semantic chunking uses NLP to:\n",
    "- Detect topic boundaries\n",
    "- Group related sentences\n",
    "- Create more coherent chunks\n",
    "\n",
    "**Tools:**\n",
    "- LangChain's SemanticChunker\n",
    "- spaCy for sentence detection\n",
    "- Custom models for topic segmentation\n",
    "\n",
    "**When to use:**\n",
    "- Very long documents\n",
    "- Multiple topics per document\n",
    "- Quality is more important than speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Introduction to Vector Databases\n",
    "\n",
    "### What is a Vector Database?\n",
    "\n",
    "A **vector database** is a specialized database optimized for storing and searching high-dimensional vectors (embeddings).\n",
    "\n",
    "**Key difference from traditional databases:**\n",
    "- Traditional DB: Exact match queries (\"WHERE name = 'John'\")\n",
    "- Vector DB: Similarity queries (\"Find documents similar to this query\")\n",
    "\n",
    "### Traditional DB vs Vector DB\n",
    "\n",
    "| Feature | Traditional DB | Vector DB |\n",
    "|---------|---------------|----------|\n",
    "| **Storage** | Rows, columns (structured data) | Vectors (embeddings) |\n",
    "| **Query Type** | SQL, exact match | Similarity search |\n",
    "| **Use Case** | CRUD operations, transactions | Semantic search, recommendations |\n",
    "| **Search Method** | Indexes (B-tree, hash) | HNSW, IVF indexes |\n",
    "| **Performance** | Fast exact queries | Fast similarity queries |\n",
    "| **Examples** | PostgreSQL, MySQL, MongoDB | Chroma, Pinecone, Weaviate |\n",
    "\n",
    "### Example Queries\n",
    "\n",
    "**Traditional DB:**\n",
    "```sql\n",
    "SELECT * FROM products WHERE price < 100 AND category = 'electronics'\n",
    "```\n",
    "\n",
    "**Vector DB:**\n",
    "```python\n",
    "# Find products similar to \"wireless headphones with noise cancellation\"\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=10\n",
    ")\n",
    "```\n",
    "\n",
    "### Popular Vector Databases\n",
    "\n",
    "**1. Chroma (We'll use this)**\n",
    "- Open-source, easy to use\n",
    "- Great for learning and prototyping\n",
    "- Local or client-server deployment\n",
    "- Python-first design\n",
    "\n",
    "**2. Pinecone**\n",
    "- Managed cloud service\n",
    "- Production-ready, highly scalable\n",
    "- Pay-as-you-go pricing\n",
    "- Excellent documentation\n",
    "\n",
    "**3. Weaviate**\n",
    "- Open-source, GraphQL API\n",
    "- Hybrid search (vector + keyword)\n",
    "- Self-hosted or cloud\n",
    "- Multi-tenant support\n",
    "\n",
    "**4. FAISS (Facebook AI)**\n",
    "- Library, not a full database\n",
    "- Extremely fast\n",
    "- Requires more manual setup\n",
    "- Best for researchers/advanced users\n",
    "\n",
    "**Others:** Qdrant, Milvus, Vespa, pgvector (PostgreSQL extension)\n",
    "\n",
    "### When to Use Vector DB vs Traditional DB\n",
    "\n",
    "**Use Vector Database when:**\n",
    "- Semantic search is needed\n",
    "- Building RAG applications\n",
    "- Recommendation systems\n",
    "- Similarity-based queries\n",
    "- Content discovery\n",
    "- Duplicate detection\n",
    "\n",
    "**Use Traditional Database when:**\n",
    "- Exact match queries\n",
    "- Transactions (ACID guarantees)\n",
    "- Structured data with relationships\n",
    "- Traditional CRUD operations\n",
    "- Complex joins and aggregations\n",
    "\n",
    "**Use Both (Hybrid Architecture):**\n",
    "```\n",
    "PostgreSQL (user data, transactions)\n",
    "    +\n",
    "ChromaDB (document embeddings, semantic search)\n",
    "    +\n",
    "Redis (caching)\n",
    "    =\n",
    "Complete production system\n",
    "```\n",
    "\n",
    "### How Vector Databases Work (Simplified)\n",
    "\n",
    "1. **Indexing:**\n",
    "   - Store vectors with metadata\n",
    "   - Build specialized indexes (HNSW, IVF)\n",
    "   - Optimize for fast similarity search\n",
    "\n",
    "2. **Querying:**\n",
    "   - Convert query to embedding\n",
    "   - Use approximate nearest neighbor (ANN) search\n",
    "   - Return top-K most similar vectors\n",
    "\n",
    "3. **Optimization:**\n",
    "   - Quantization (reduce memory)\n",
    "   - Sharding (distribute load)\n",
    "   - Caching (speed up repeated queries)\n",
    "\n",
    "Don't worry about the details - the vector DB handles this for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Hands-On with ChromaDB\n",
    "\n",
    "Let's get practical with ChromaDB - one of the easiest vector databases to use.\n",
    "\n",
    "### Why ChromaDB?\n",
    "\n",
    "- Simple Python API\n",
    "- No separate server needed (can run in-memory)\n",
    "- Perfect for learning and prototyping\n",
    "- Can scale to production with client-server mode\n",
    "\n",
    "### Step 1: Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client (in-memory mode)\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False  # Disable telemetry\n",
    "))\n",
    "\n",
    "# Create a collection (like a table in SQL)\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"my_documents\",\n",
    "    metadata={\"description\": \"A collection of document embeddings\"}\n",
    ")\n",
    "\n",
    "print(\"âœ… ChromaDB initialized!\")\n",
    "print(f\"Collection: {collection.name}\")\n",
    "print(f\"Count: {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Add Documents to ChromaDB\n",
    "\n",
    "ChromaDB stores:\n",
    "- **documents**: The actual text\n",
    "- **embeddings**: Vector representations\n",
    "- **metadata**: Additional information\n",
    "- **ids**: Unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI/ML\n",
    "documents = [\n",
    "    \"Machine learning is a subset of AI that uses algorithms to learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to learn complex patterns.\",\n",
    "    \"Python is a popular programming language for data science and machine learning.\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "    \"Computer vision enables machines to interpret visual information from images and videos.\",\n",
    "    \"Reinforcement learning teaches agents to make decisions through trial and error.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = get_openai_embeddings_batch(documents)\n",
    "\n",
    "# Add to ChromaDB\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
    "    metadatas=[\n",
    "        {\"source\": \"training_data\", \"topic\": \"machine_learning\", \"index\": i} \n",
    "        for i in range(len(documents))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Added {len(documents)} documents to ChromaDB\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Query ChromaDB (Semantic Search)\n",
    "\n",
    "Now comes the magic - searching by meaning, not keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chroma(query: str, n_results: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Search ChromaDB for similar documents.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        n_results: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with documents, metadata, and distances\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = get_openai_embedding(query)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "query = \"How do neural networks work?\"\n",
    "results = search_chroma(query, n_results=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    similarity = 1 - distance  # Convert distance to similarity\n",
    "    print(f\"{i}. Similarity: {similarity:.4f}\")\n",
    "    print(f\"   Document: {doc}\")\n",
    "    print(f\"   Metadata: {metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "Notice that:\n",
    "- The query mentions \"neural networks\"\n",
    "- Top result talks about \"deep learning\" and \"neural networks\"\n",
    "- This is **semantic search** - understanding meaning, not just matching keywords!\n",
    "\n",
    "### Step 4: Filtering with Metadata\n",
    "\n",
    "ChromaDB supports filtering results by metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filter\n",
    "query_embedding = get_openai_embedding(\"Tell me about AI\")\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3,\n",
    "    where={\"topic\": \"machine_learning\"}  # Filter by topic\n",
    ")\n",
    "\n",
    "print(\"Results filtered by metadata (topic='machine_learning'):\\n\")\n",
    "for i, (doc, metadata) in enumerate(zip(\n",
    "    results['documents'][0],\n",
    "    results['metadatas'][0]\n",
    "), 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "    print(f\"   Topic: {metadata['topic']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update and Delete\n",
    "\n",
    "You can also update or delete documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a document\n",
    "print(\"Original count:\", collection.count())\n",
    "\n",
    "# Delete a document\n",
    "collection.delete(ids=[\"doc_0\"])\n",
    "print(\"After deletion:\", collection.count())\n",
    "\n",
    "# Add it back\n",
    "collection.add(\n",
    "    documents=[documents[0]],\n",
    "    embeddings=[embeddings[0]],\n",
    "    ids=[\"doc_0\"],\n",
    "    metadatas=[{\"source\": \"training_data\", \"topic\": \"machine_learning\", \"index\": 0}]\n",
    ")\n",
    "print(\"After adding back:\", collection.count())\n",
    "\n",
    "print(\"\\nâœ… Update and delete operations work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB Key Operations Summary\n",
    "\n",
    "**Create collection:**\n",
    "```python\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")\n",
    "```\n",
    "\n",
    "**Add documents:**\n",
    "```python\n",
    "collection.add(\n",
    "    documents=[...],\n",
    "    embeddings=[...],\n",
    "    ids=[...],\n",
    "    metadatas=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "**Query (semantic search):**\n",
    "```python\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3,\n",
    "    where={\"key\": \"value\"}  # Optional metadata filter\n",
    ")\n",
    "```\n",
    "\n",
    "**Delete:**\n",
    "```python\n",
    "collection.delete(ids=[\"doc_1\", \"doc_2\"])\n",
    "```\n",
    "\n",
    "**Get count:**\n",
    "```python\n",
    "count = collection.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Practical Example - Document Search System\n",
    "\n",
    "Let's build a complete, production-ready document search engine class.\n",
    "\n",
    "This will encapsulate everything we've learned into a clean, reusable API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEngine:\n",
    "    \"\"\"\n",
    "    A complete vector search engine using ChromaDB and OpenAI embeddings.\n",
    "    \n",
    "    Features:\n",
    "    - Add documents with metadata\n",
    "    - Semantic search\n",
    "    - Metadata filtering\n",
    "    - Similarity scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"documents\", embedding_function=None):\n",
    "        \"\"\"\n",
    "        Initialize the search engine.\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name for the ChromaDB collection\n",
    "            embedding_function: Function to generate embeddings (defaults to OpenAI)\n",
    "        \"\"\"\n",
    "        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Create or get existing collection\n",
    "        try:\n",
    "            self.collection = self.client.create_collection(collection_name)\n",
    "        except:\n",
    "            self.collection = self.client.get_collection(collection_name)\n",
    "        \n",
    "        # Use provided embedding function or default to OpenAI\n",
    "        self.embedding_function = embedding_function or get_openai_embedding\n",
    "    \n",
    "    def add_documents(self, documents: List[str], metadatas: List[Dict] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add documents to the search engine.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            metadatas: Optional list of metadata dicts (one per document)\n",
    "        \"\"\"\n",
    "        # Generate embeddings (batch for efficiency)\n",
    "        if self.embedding_function == get_openai_embedding:\n",
    "            embeddings = get_openai_embeddings_batch(documents)\n",
    "        else:\n",
    "            embeddings = [self.embedding_function(doc) for doc in documents]\n",
    "        \n",
    "        # Generate IDs\n",
    "        start_id = self.collection.count()\n",
    "        ids = [f\"doc_{start_id + i}\" for i in range(len(documents))]\n",
    "        \n",
    "        # Use provided metadata or create empty dicts\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in documents]\n",
    "        \n",
    "        # Add to collection\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Added {len(documents)} documents. Total: {self.collection.count()}\")\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, metadata_filter: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Search for similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            n_results: Number of results to return\n",
    "            metadata_filter: Optional metadata filter (e.g., {\"source\": \"paper.pdf\"})\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with query, results (document, metadata, similarity)\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_emb = self.embedding_function(query)\n",
    "        \n",
    "        # Search\n",
    "        search_kwargs = {\n",
    "            \"query_embeddings\": [query_emb],\n",
    "            \"n_results\": n_results\n",
    "        }\n",
    "        \n",
    "        if metadata_filter:\n",
    "            search_kwargs[\"where\"] = metadata_filter\n",
    "        \n",
    "        results = self.collection.query(**search_kwargs)\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = {\n",
    "            \"query\": query,\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"document\": doc,\n",
    "                    \"metadata\": meta,\n",
    "                    \"similarity\": 1 - dist  # Convert distance to similarity\n",
    "                }\n",
    "                for doc, meta, dist in zip(\n",
    "                    results['documents'][0],\n",
    "                    results['metadatas'][0],\n",
    "                    results['distances'][0]\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search engine.\"\"\"\n",
    "        return {\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"total_documents\": self.collection.count()\n",
    "        }\n",
    "\n",
    "print(\"âœ… VectorSearchEngine class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Test Our Search Engine!\n",
    "\n",
    "We'll create a search engine with documents about AI and programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search engine\n",
    "engine = VectorSearchEngine(collection_name=\"test_search\")\n",
    "\n",
    "# Add AI-related documents\n",
    "ai_docs = [\n",
    "    \"Machine learning models learn patterns from training data.\",\n",
    "    \"Neural networks consist of interconnected layers of artificial neurons.\",\n",
    "    \"Deep learning has revolutionized computer vision and NLP tasks.\",\n",
    "    \"Transformers use self-attention mechanisms for sequence processing.\"\n",
    "]\n",
    "\n",
    "ai_metadata = [\n",
    "    {\"category\": \"ai\", \"topic\": \"machine_learning\"},\n",
    "    {\"category\": \"ai\", \"topic\": \"neural_networks\"},\n",
    "    {\"category\": \"ai\", \"topic\": \"deep_learning\"},\n",
    "    {\"category\": \"ai\", \"topic\": \"transformers\"}\n",
    "]\n",
    "\n",
    "engine.add_documents(ai_docs, ai_metadata)\n",
    "\n",
    "# Add programming-related documents\n",
    "prog_docs = [\n",
    "    \"Python uses dynamic typing and automatic memory management.\",\n",
    "    \"JavaScript is the primary language for web browser scripting.\",\n",
    "    \"React is a JavaScript library for building user interfaces.\",\n",
    "    \"Docker containers package applications with their dependencies.\"\n",
    "]\n",
    "\n",
    "prog_metadata = [\n",
    "    {\"category\": \"programming\", \"topic\": \"python\"},\n",
    "    {\"category\": \"programming\", \"topic\": \"javascript\"},\n",
    "    {\"category\": \"programming\", \"topic\": \"react\"},\n",
    "    {\"category\": \"programming\", \"topic\": \"docker\"}\n",
    "]\n",
    "\n",
    "engine.add_documents(prog_docs, prog_metadata)\n",
    "\n",
    "print(f\"\\n{engine.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Search for AI Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for AI-related content\n",
    "result = engine.search(\"How do neural networks learn?\", n_results=3)\n",
    "\n",
    "print(f\"Query: {result['query']}\\n\")\n",
    "print(\"Top 3 results:\\n\")\n",
    "for i, r in enumerate(result['results'], 1):\n",
    "    print(f\"{i}. Similarity: {r['similarity']:.4f}\")\n",
    "    print(f\"   Document: {r['document']}\")\n",
    "    print(f\"   Category: {r['metadata']['category']}\")\n",
    "    print(f\"   Topic: {r['metadata']['topic']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Search with Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only in programming documents\n",
    "result = engine.search(\n",
    "    \"Tell me about web development\",\n",
    "    n_results=3,\n",
    "    metadata_filter={\"category\": \"programming\"}\n",
    ")\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(\"Filter: category='programming'\\n\")\n",
    "print(\"Results:\\n\")\n",
    "for i, r in enumerate(result['results'], 1):\n",
    "    print(f\"{i}. Similarity: {r['similarity']:.4f}\")\n",
    "    print(f\"   Document: {r['document']}\")\n",
    "    print(f\"   Topic: {r['metadata']['topic']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Cross-Category Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General search across all categories\n",
    "result = engine.search(\"What programming tools are popular?\", n_results=5)\n",
    "\n",
    "print(f\"Query: {result['query']}\\n\")\n",
    "print(\"Top 5 results (all categories):\\n\")\n",
    "for i, r in enumerate(result['results'], 1):\n",
    "    print(f\"{i}. Similarity: {r['similarity']:.4f} | Category: {r['metadata']['category']}\")\n",
    "    print(f\"   {r['document']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Built\n",
    "\n",
    "We now have a complete, production-ready vector search engine that:\n",
    "\n",
    "âœ… **Stores documents** with rich metadata\n",
    "âœ… **Performs semantic search** (meaning-based, not keyword)\n",
    "âœ… **Filters by metadata** (category, topic, etc.)\n",
    "âœ… **Returns similarity scores** for ranking\n",
    "âœ… **Handles batch operations** efficiently\n",
    "\n",
    "This is the foundation of:\n",
    "- RAG chatbots\n",
    "- Document search engines\n",
    "- Recommendation systems\n",
    "- Content discovery platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Best Practices & Common Pitfalls\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Embedding Model Selection**\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use the **same model** for indexing and querying\n",
    "- Start with cost-effective models (OpenAI small, open-source)\n",
    "- Test quality before choosing expensive models\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Mix embeddings from different models\n",
    "- Assume larger models are always better\n",
    "\n",
    "**2. Chunking Strategy**\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use 256-512 token chunks for most use cases\n",
    "- Add 10-20% overlap between chunks\n",
    "- Include rich metadata (source, page, section)\n",
    "- Test different strategies with your data\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Use chunks that are too large (>1000 tokens)\n",
    "- Use chunks that are too small (<100 tokens)\n",
    "- Forget to add metadata\n",
    "\n",
    "**3. Indexing**\n",
    "\n",
    "âœ… **DO:**\n",
    "- Batch embeddings API calls (cheaper, faster)\n",
    "- Cache embeddings when possible\n",
    "- Update indexes incrementally\n",
    "- Monitor API costs\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Generate embeddings one at a time\n",
    "- Regenerate embeddings unnecessarily\n",
    "- Ignore rate limits\n",
    "\n",
    "**4. Search Quality**\n",
    "\n",
    "âœ… **DO:**\n",
    "- Set minimum similarity thresholds (e.g., >0.7)\n",
    "- Combine vector search with metadata filters\n",
    "- Consider hybrid search (vector + keyword)\n",
    "- Use reranking for top results\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Return all results regardless of similarity\n",
    "- Rely solely on vector search for everything\n",
    "- Ignore metadata filtering capabilities\n",
    "\n",
    "**5. Performance**\n",
    "\n",
    "âœ… **DO:**\n",
    "- Use appropriate vector DB for your scale\n",
    "  - Small: ChromaDB, FAISS\n",
    "  - Large: Pinecone, Weaviate\n",
    "- Monitor query latency\n",
    "- Implement caching for frequent queries\n",
    "- Consider async operations for scale\n",
    "\n",
    "âŒ **DON'T:**\n",
    "- Use in-memory DB for large datasets\n",
    "- Ignore performance metrics\n",
    "- Block on embedding generation\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "**1. Mixing Embedding Models**\n",
    "\n",
    "âŒ **Problem:**\n",
    "```python\n",
    "# Index with model A\n",
    "embeddings = get_openai_embedding(text)  \n",
    "\n",
    "# Query with model B\n",
    "query_emb = get_google_embedding(query)  # âŒ Different model!\n",
    "```\n",
    "\n",
    "âœ… **Solution:**\n",
    "```python\n",
    "# Use same model for both\n",
    "embeddings = get_openai_embedding(text)\n",
    "query_emb = get_openai_embedding(query)  # âœ… Same model\n",
    "```\n",
    "\n",
    "**2. Wrong Chunk Sizes**\n",
    "\n",
    "âŒ **Problem:**\n",
    "```python\n",
    "# Too large - loses precision\n",
    "chunks = chunk_by_tokens(text, chunk_size=2000)  \n",
    "\n",
    "# Too small - loses context\n",
    "chunks = chunk_by_tokens(text, chunk_size=50)  \n",
    "```\n",
    "\n",
    "âœ… **Solution:**\n",
    "```python\n",
    "# Just right - 256-512 tokens\n",
    "chunks = chunk_by_tokens(text, chunk_size=512, overlap=50)\n",
    "```\n",
    "\n",
    "**3. Missing Metadata**\n",
    "\n",
    "âŒ **Problem:**\n",
    "```python\n",
    "# No metadata - can't filter or cite\n",
    "collection.add(documents=docs, embeddings=embs, ids=ids)\n",
    "```\n",
    "\n",
    "âœ… **Solution:**\n",
    "```python\n",
    "# Rich metadata for filtering and citations\n",
    "collection.add(\n",
    "    documents=docs,\n",
    "    embeddings=embs,\n",
    "    ids=ids,\n",
    "    metadatas=[{\"source\": \"paper.pdf\", \"page\": 5, \"date\": \"2024-01-15\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "**4. No Text Preprocessing**\n",
    "\n",
    "âŒ **Problem:**\n",
    "```python\n",
    "# Raw text with HTML, special characters\n",
    "text = \"<div>Machine learning is...</div> \\n\\n\\n   \"\n",
    "embedding = get_openai_embedding(text)\n",
    "```\n",
    "\n",
    "âœ… **Solution:**\n",
    "```python\n",
    "# Clean text before embedding\n",
    "import re\n",
    "text = re.sub('<.*?>', '', text)  # Remove HTML\n",
    "text = ' '.join(text.split())  # Normalize whitespace\n",
    "embedding = get_openai_embedding(text)\n",
    "```\n",
    "\n",
    "**5. Not Testing Similarity Thresholds**\n",
    "\n",
    "âŒ **Problem:**\n",
    "```python\n",
    "# Return all results, even low-quality ones\n",
    "results = collection.query(query_embeddings=[emb], n_results=10)\n",
    "return results  # May include irrelevant documents\n",
    "```\n",
    "\n",
    "âœ… **Solution:**\n",
    "```python\n",
    "# Filter by similarity threshold\n",
    "results = collection.query(query_embeddings=[emb], n_results=10)\n",
    "filtered = [\n",
    "    r for r in results \n",
    "    if (1 - r['distance']) > 0.7  # Only high-quality matches\n",
    "]\n",
    "return filtered\n",
    "```\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] Same embedding model for index and query\n",
    "- [ ] Optimal chunk size tested (256-512 tokens)\n",
    "- [ ] Overlap between chunks (10-20%)\n",
    "- [ ] Rich metadata for all documents\n",
    "- [ ] Similarity threshold configured\n",
    "- [ ] Batch API calls for efficiency\n",
    "- [ ] Error handling and retries\n",
    "- [ ] Monitoring and logging\n",
    "- [ ] Caching strategy\n",
    "- [ ] Rate limiting\n",
    "- [ ] Cost monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: When to Use Vector Databases\n",
    "\n",
    "Understanding when to use (and not use) vector databases is critical for building effective systems.\n",
    "\n",
    "### Use Vector Databases When:\n",
    "\n",
    "**1. Semantic Search**\n",
    "\n",
    "Finding documents by **meaning**, not exact keywords.\n",
    "\n",
    "**Example:**\n",
    "- Query: \"budget management strategies\"\n",
    "- Matches: \"financial planning\", \"cost control\", \"expense tracking\"\n",
    "\n",
    "**Use cases:**\n",
    "- Document search engines\n",
    "- Knowledge bases\n",
    "- FAQ systems\n",
    "\n",
    "**2. RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "Retrieving relevant context for LLMs.\n",
    "\n",
    "**Example:**\n",
    "- User asks: \"What's our refund policy?\"\n",
    "- Vector DB retrieves relevant policy documents\n",
    "- LLM generates answer using retrieved context\n",
    "\n",
    "**Use cases:**\n",
    "- Chatbots with custom knowledge\n",
    "- Q&A systems\n",
    "- Customer support agents\n",
    "\n",
    "**3. Recommendation Systems**\n",
    "\n",
    "Finding similar items based on embeddings.\n",
    "\n",
    "**Example:**\n",
    "- \"Users who liked this article also liked...\"\n",
    "- \"Similar products to what you viewed\"\n",
    "\n",
    "**Use cases:**\n",
    "- Content recommendations\n",
    "- Product recommendations\n",
    "- User matching\n",
    "\n",
    "**4. Duplicate Detection**\n",
    "\n",
    "Finding similar or duplicate content at scale.\n",
    "\n",
    "**Example:**\n",
    "- Detect duplicate support tickets\n",
    "- Find plagiarized content\n",
    "- Identify similar documents\n",
    "\n",
    "**Use cases:**\n",
    "- Content moderation\n",
    "- Data deduplication\n",
    "- Plagiarism detection\n",
    "\n",
    "**5. Anomaly Detection**\n",
    "\n",
    "Finding outliers in high-dimensional data.\n",
    "\n",
    "**Example:**\n",
    "- Unusual transaction patterns\n",
    "- Anomalous user behavior\n",
    "- Fraud detection\n",
    "\n",
    "**Use cases:**\n",
    "- Security monitoring\n",
    "- Fraud prevention\n",
    "- Quality control\n",
    "\n",
    "### Don't Use Vector Databases When:\n",
    "\n",
    "**1. Exact Match Queries**\n",
    "\n",
    "âŒ **Wrong tool:**\n",
    "```python\n",
    "# Finding user by email\n",
    "vector_db.query(\"user@example.com\")  # Overkill!\n",
    "```\n",
    "\n",
    "âœ… **Use traditional DB:**\n",
    "```sql\n",
    "SELECT * FROM users WHERE email = 'user@example.com'\n",
    "```\n",
    "\n",
    "**2. Transactional Data**\n",
    "\n",
    "âŒ **Wrong tool:**\n",
    "- Banking transactions\n",
    "- E-commerce orders\n",
    "- User account management\n",
    "\n",
    "âœ… **Use traditional DB:**\n",
    "- Need ACID guarantees\n",
    "- PostgreSQL, MySQL\n",
    "- Relational integrity\n",
    "\n",
    "**3. Simple Keyword Search**\n",
    "\n",
    "âŒ **Wrong tool:**\n",
    "```python\n",
    "# Searching by exact title or ID\n",
    "vector_db.query(\"DOC-12345\")\n",
    "```\n",
    "\n",
    "âœ… **Use full-text search:**\n",
    "- Elasticsearch\n",
    "- PostgreSQL full-text search\n",
    "- Simple keyword matching\n",
    "\n",
    "**4. Structured Relationships**\n",
    "\n",
    "âŒ **Wrong tool:**\n",
    "- Complex joins (users â†’ orders â†’ products)\n",
    "- Foreign key relationships\n",
    "- Multi-table aggregations\n",
    "\n",
    "âœ… **Use relational DB:**\n",
    "- PostgreSQL, MySQL\n",
    "- SQL for complex queries\n",
    "- Referential integrity\n",
    "\n",
    "**5. Real-time Analytics**\n",
    "\n",
    "âŒ **Wrong tool:**\n",
    "```sql\n",
    "-- Aggregations, GROUP BY, SUM\n",
    "SELECT category, COUNT(*) FROM products GROUP BY category\n",
    "```\n",
    "\n",
    "âœ… **Use analytics DB:**\n",
    "- ClickHouse\n",
    "- BigQuery\n",
    "- Time-series databases\n",
    "\n",
    "### Hybrid Architecture (Best Practice)\n",
    "\n",
    "Most production systems use **both** traditional and vector databases:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Application Layer               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚            â”‚           â”‚\n",
    "           â–¼            â–¼           â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚PostgreSQLâ”‚  â”‚ ChromaDB â”‚  â”‚Redis â”‚\n",
    "    â”‚          â”‚  â”‚          â”‚  â”‚      â”‚\n",
    "    â”‚User data â”‚  â”‚Embeddingsâ”‚  â”‚Cache â”‚\n",
    "    â”‚Orders    â”‚  â”‚Semantic  â”‚  â”‚      â”‚\n",
    "    â”‚          â”‚  â”‚search    â”‚  â”‚      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Example: E-commerce with RAG**\n",
    "\n",
    "```python\n",
    "# User data and orders â†’ PostgreSQL\n",
    "user = postgres.query(\"SELECT * FROM users WHERE id = 123\")\n",
    "orders = postgres.query(\"SELECT * FROM orders WHERE user_id = 123\")\n",
    "\n",
    "# Product descriptions and reviews â†’ Vector DB\n",
    "similar_products = chroma.search(\"wireless headphones noise cancelling\")\n",
    "\n",
    "# Frequent queries â†’ Redis cache\n",
    "cached_results = redis.get(\"popular_products\")\n",
    "\n",
    "# Combine results\n",
    "recommendations = combine(user, orders, similar_products)\n",
    "```\n",
    "\n",
    "### Decision Matrix\n",
    "\n",
    "| Scenario | Use This |\n",
    "|----------|----------|\n",
    "| Semantic search | Vector DB |\n",
    "| RAG chatbot | Vector DB |\n",
    "| Recommendations | Vector DB |\n",
    "| User authentication | Traditional DB |\n",
    "| E-commerce orders | Traditional DB |\n",
    "| Simple keyword search | Full-text search |\n",
    "| Analytics queries | Analytics DB |\n",
    "| Caching | Redis/Memcached |\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Vector databases are excellent for:**\n",
    "- Semantic similarity\n",
    "- Meaning-based search\n",
    "- RAG applications\n",
    "- Recommendations\n",
    "\n",
    "**But they're not a replacement for:**\n",
    "- Traditional databases (exact queries, transactions)\n",
    "- Full-text search (keyword matching)\n",
    "- Analytics databases (aggregations, reporting)\n",
    "\n",
    "**Use the right tool for each job!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: Summary & Next Steps\n",
    "\n",
    "Congratulations! You've completed the Vector Databases and Embeddings notebook.\n",
    "\n",
    "### What We Learned Today\n",
    "\n",
    "âœ… **Embeddings Fundamentals**\n",
    "- Text â†’ vectors that capture semantic meaning\n",
    "- Different providers: OpenAI, Google, open-source\n",
    "- When to use each model (cost, quality, privacy trade-offs)\n",
    "\n",
    "âœ… **Similarity Search**\n",
    "- Cosine similarity, Euclidean distance, dot product\n",
    "- Top-k search implementation\n",
    "- Measuring semantic similarity between texts\n",
    "\n",
    "âœ… **Chunking Strategies**\n",
    "- Fixed-size chunking (256-512 tokens)\n",
    "- Sentence-based chunking\n",
    "- Importance of overlap (10-20%) and metadata\n",
    "\n",
    "âœ… **Vector Databases**\n",
    "- Hands-on experience with ChromaDB\n",
    "- When to use vector DB vs traditional DB\n",
    "- Production considerations and hybrid architecture\n",
    "\n",
    "âœ… **Best Practices**\n",
    "- Consistent embedding models\n",
    "- Proper chunking with metadata\n",
    "- Similarity thresholds\n",
    "- Batch operations for efficiency\n",
    "- Common pitfalls to avoid\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**1. Embeddings are the foundation of modern AI applications**\n",
    "- Enable semantic search (meaning-based, not keyword-based)\n",
    "- Critical for RAG, recommendations, and more\n",
    "\n",
    "**2. Chunking strategy matters**\n",
    "- Chunk size: 256-512 tokens (sweet spot)\n",
    "- Add overlap for context continuity\n",
    "- Always include rich metadata\n",
    "\n",
    "**3. Use the right tool for the job**\n",
    "- Vector DB: Semantic search, RAG, recommendations\n",
    "- Traditional DB: Exact queries, transactions\n",
    "- Hybrid: Most production systems use both\n",
    "\n",
    "**4. Start simple, scale smart**\n",
    "- Begin with OpenAI small or open-source\n",
    "- Use ChromaDB for prototyping\n",
    "- Upgrade models/infrastructure based on needs\n",
    "\n",
    "**5. Test and iterate**\n",
    "- Different chunking strategies for different data\n",
    "- Set similarity thresholds based on testing\n",
    "- Monitor quality and costs\n",
    "\n",
    "### Preview: Next Session - RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "In the next notebook, we'll combine everything we've learned:\n",
    "\n",
    "**Topics we'll cover:**\n",
    "- Building complete RAG pipeline\n",
    "- Document loading (PDF, TXT, web)\n",
    "- Combining vector search with LLMs\n",
    "- Citation and source tracking\n",
    "- Handling multi-document queries\n",
    "- **Hands-on project:** Build a RAG chatbot\n",
    "\n",
    "**You'll be able to:**\n",
    "- Load documents from various sources\n",
    "- Chunk and embed them efficiently\n",
    "- Retrieve relevant context for queries\n",
    "- Generate answers with citations\n",
    "- Build production-ready RAG applications\n",
    "\n",
    "### Resources for Deeper Learning\n",
    "\n",
    "**Official Documentation:**\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "**Vector Database Comparisons:**\n",
    "- [Vector Database Benchmark](https://github.com/erikbern/ann-benchmarks)\n",
    "- [Vector DB Landscape](https://www.pinecone.io/learn/vector-database/)\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Hybrid search (vector + keyword)\n",
    "- Reranking models\n",
    "- Fine-tuning embedding models\n",
    "- Quantization and compression\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "Before the next session, try:\n",
    "\n",
    "1. **Build a mini search engine** for your own documents\n",
    "2. **Compare embedding models** - test OpenAI vs Google vs open-source on your data\n",
    "3. **Experiment with chunking** - try different sizes and overlap percentages\n",
    "4. **Add metadata filtering** - categorize documents and filter searches\n",
    "\n",
    "### Questions?\n",
    "\n",
    "Common questions:\n",
    "\n",
    "**Q: Which embedding model should I use?**\n",
    "A: Start with OpenAI text-embedding-3-small (good quality/cost balance). Upgrade if needed.\n",
    "\n",
    "**Q: How do I know if my chunk size is right?**\n",
    "A: Test retrieval quality with sample queries. If results are too broad, use smaller chunks. If too fragmented, use larger chunks.\n",
    "\n",
    "**Q: Should I use ChromaDB in production?**\n",
    "A: ChromaDB works well for small-medium datasets. For large-scale production, consider Pinecone or Weaviate.\n",
    "\n",
    "**Q: How much does this cost?**\n",
    "A: OpenAI embeddings: $0.02 per 1M tokens. For a 10,000 document corpus (~500 tokens each), that's ~$0.10.\n",
    "\n",
    "### You're Ready for RAG!\n",
    "\n",
    "You now understand the core building blocks:\n",
    "- âœ… How to generate embeddings\n",
    "- âœ… How to measure similarity\n",
    "- âœ… How to chunk documents\n",
    "- âœ… How to store and search vectors\n",
    "\n",
    "Next, we'll put it all together and build a RAG chatbot that can answer questions using your custom knowledge base!\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! See you in the next session! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}