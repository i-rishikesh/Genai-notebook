{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# LangGraph Essentials\n\n**Duration**: ~2-2.5 hours\n\n## What You'll Learn\n\nThis notebook covers LangGraph - the framework for building stateful, agentic workflows:\n\n1. What is LangGraph and when to use it\n2. LangChain vs LangGraph decision guide\n3. Core concepts: nodes, edges, state\n4. State management with TypedDict and reducers\n5. Building your first graph\n6. **Tools and Function Calling** (NEW!)\n7. Conditional routing and cycles\n8. Multi-agent supervisor pattern\n9. Human-in-the-loop workflows\n10. Checkpointing and persistence\n11. Production patterns and best practices\n\n## Prerequisites\n\n- Completed LangChain Essentials notebook (notebook 03)\n- Understanding of LCEL pipe syntax\n- OpenAI API key\n\n## What is LangGraph?\n\n**LangGraph** builds on LangChain to add:\n- **State management**: Shared data across steps\n- **Cycles**: Loops and iterative processes\n- **Conditional logic**: Dynamic routing\n- **Human-in-the-loop**: Pause for human input\n\n### When to Use LangGraph\n\n**Use LangGraph when you need**:\n- Cyclical workflows (loops, retries)\n- Complex state management\n- Multi-agent systems\n- Human approval workflows\n- Conditional decision points\n\n**Don't use LangGraph for**:\n- Simple linear workflows â†’ use LangChain LCEL\n- Stateless operations â†’ use direct API calls\n- Quick prototypes â†’ start simple first\n\n### Visual Comparison\n\n```\nLangChain: [Question] â†’ [Retrieve] â†’ [LLM] â†’ [Answer]\n           (Linear, stateless)\n\nLangGraph: [Question] â†’ [Analyze] â†’ [Research?] â”€Yesâ†’ [Research] â”\n                           â”‚                                       â”‚\n                           No                                      â”‚\n                           â†“                                       â”‚\n                      [Answer] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           (Cyclical, stateful, conditional)\n```\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## LangChain vs LangGraph Decision Guide\n",
    "\n",
    "Use this flowchart to decide which framework to use:\n",
    "\n",
    "```\n",
    "Do you need loops/retries?\n",
    "â”œâ”€ YES â†’ LangGraph\n",
    "â””â”€ NO\n",
    "    â”‚\n",
    "    Do you need complex shared state?\n",
    "    â”œâ”€ YES â†’ LangGraph\n",
    "    â””â”€ NO\n",
    "        â”‚\n",
    "        Do you need multiple agents?\n",
    "        â”œâ”€ YES â†’ LangGraph\n",
    "        â””â”€ NO\n",
    "            â”‚\n",
    "            Do you need human-in-the-loop?\n",
    "            â”œâ”€ YES â†’ LangGraph\n",
    "            â””â”€ NO â†’ LangChain (LCEL)\n",
    "```\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Use LangChain (LCEL)**:\n",
    "- Simple RAG chatbot\n",
    "- Document summarization\n",
    "- Q&A pipeline\n",
    "\n",
    "**Use LangGraph**:\n",
    "- Research agent that verifies facts (loops)\n",
    "- Code generator with testing (retry logic)\n",
    "- Multi-agent content team (supervisor pattern)\n",
    "- Approval workflows (human-in-the-loop)\n",
    "\n",
    "### Quick Decision Table\n",
    "\n",
    "| Use LangChain | Use LangGraph |\n",
    "|---------------|---------------|\n",
    "| Linear workflows (A â†’ B â†’ C) | Cyclical workflows (loops, retries) |\n",
    "| Simple RAG chatbot | Complex agents with tools |\n",
    "| Stateless operations | Stateful operations |\n",
    "| Single decision path | Multiple decision points |\n",
    "| Quick prototypes | Production agents |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "**LangGraph = LangChain + State + Cycles + Agents**\n",
    "\n",
    "You'll often use both in the same application!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Section 1: Package Installation\n",
    "\n",
    "### Learning Objectives\n",
    "- Install LangGraph and dependencies\n",
    "- Configure OpenAI API key\n",
    "- Verify setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    langchain-openai \\\n",
    "    langchain-community \\\n",
    "    langgraph \\\n",
    "    langgraph-checkpoint-sqlite\n",
    "\n",
    "# Show installed versions\n",
    "!pip list | grep -E \"langchain|langgraph\"\n",
    "\n",
    "print(\"\\nâœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"âœ… API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Core Concepts\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand nodes, edges, and state\n",
    "- Learn StateGraph basics\n",
    "- Build a simple graph\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Nodes**: Functions that process state (like LLM calls, tools, etc.)\n",
    "2. **Edges**: Connections between nodes (can be conditional)\n",
    "3. **State**: Shared data structure (TypedDict) passed between nodes\n",
    "4. **Cycles**: Support for loops - nodes can revisit earlier nodes\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input â†’ [Node A] â†â†’ [Node B] â†’ Decide â†’ [Node C] â†’ Output\n",
    "              â†‘                  â†“\n",
    "              â””â”€â”€â”€â”€ Loop back â”€â”€â”€â”˜\n",
    "(Cyclical, stateful, conditional)\n",
    "```\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Let's see a basic LangGraph in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77qls2irwtt",
   "source": "## Understanding StateGraph Fundamentals\n\nBefore building our first graph, let's understand the core LangGraph concepts.\n\n### What is StateGraph?\n\n**StateGraph** is the container for building LangGraph workflows. Think of it as a workflow builder.\n\n```python\nworkflow = StateGraph(SimpleState)\n```\n\n**Key differences from LangChain**:\n- LangChain chains: Sequential data transformations (immutable)\n- LangGraph StateGraph: Stateful workflows with cycles (mutable state)\n\n**StateGraph enables**:\n- Cyclic workflows (nodes can loop back)\n- Conditional routing (dynamic paths based on state)\n- Persistent state across steps\n- Human-in-the-loop workflows\n\n### What are Nodes?\n\n**Nodes** are Python functions that process and update state.\n\n```python\ndef greet(state: SimpleState):\n    return {\"message\": f\"Hello, {state['message']}!\"}\n```\n\n**Critical rules**:\n1. Node receives current state as input (dictionary)\n2. Node returns PARTIAL state updates (only fields it modifies)\n3. Other fields in state remain unchanged\n4. Multiple nodes' returns are merged using reducers\n\n**Adding nodes**:\n```python\nworkflow.add_node(\"greet\", greet)  # name (string), function\n```\n\nNode names are arbitrary - use descriptive names for clarity.\n\n### What are Edges?\n\n**Edges** define connections between nodes (control flow).\n\n**Basic edges** (always go to next node):\n```python\nworkflow.add_edge(\"greet\", \"increment\")  # greet â†’ increment\n```\n\n**END sentinel** (terminate workflow):\n```python\nworkflow.add_edge(\"increment\", END)  # increment â†’ END\n```\n\n**Key points**:\n- END is imported from `langgraph.graph`\n- Every workflow path must eventually reach END\n- Multiple edges can exist (conditional routing - covered later)\n\n### Entry Points\n\n**Entry point** specifies where execution starts.\n\n**Modern syntax** (recommended - used throughout this notebook):\n```python\nworkflow.add_edge(START, \"greet\")  # START from langgraph.graph\n```\n\n**Legacy syntax** (still functional but deprecated):\n```python\nworkflow.set_entry_point(\"greet\")  # Older approach\n```\n\n**Required** - workflow won't compile without an entry point.\n\n### Compilation\n\n**compile()** validates and prepares the workflow for execution.\n\n```python\napp = workflow.compile()  # Returns executable app\n```\n\n**What compilation does**:\n- âœ… Validates graph structure (no unreachable nodes)\n- âœ… Checks all paths reach END\n- âœ… Detects infinite loops without termination\n- âœ… Creates optimized execution plan\n\n**When to add parameters**:\n```python\napp = workflow.compile(\n    checkpointer=memory,           # For persistence\n    interrupt_before=[\"approve\"]   # For human-in-the-loop\n)\n```\n\nNow let's build our first graph!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Simple LangGraph example\nfrom langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict\n\n# Define state\nclass SimpleState(TypedDict):\n    message: str\n    count: int\n\n# Define nodes (functions that modify state)\ndef greet(state: SimpleState):\n    return {\"message\": f\"Hello! Count: {state['count']}\"}\n\ndef increment(state: SimpleState):\n    return {\"count\": state[\"count\"] + 1}\n\n# Build graph\nworkflow = StateGraph(SimpleState)\n\n# Add nodes\nworkflow.add_node(\"greet\", greet)\nworkflow.add_node(\"increment\", increment)\n\n# Add edges\nworkflow.add_edge(START, \"greet\")\nworkflow.add_edge(\"greet\", \"increment\")\nworkflow.add_edge(\"increment\", END)\n\n# Compile\napp = workflow.compile()\n\n# Run\nresult = app.invoke({\"message\": \"\", \"count\": 0})\nprint(\"Final state:\", result)\nprint(\"\\nâœ… LangGraph executed: greet â†’ increment â†’ END\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **StateGraph**: Main class for building graphs\n",
    "- **Nodes**: Processing functions that return state updates\n",
    "- **Edges**: Connect nodes in sequence\n",
    "- **END**: Special marker for terminal nodes\n",
    "- **compile()**: Creates executable graph\n",
    "- **invoke()**: Runs the graph with initial state\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Section 3: State Management\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand state reducers (how updates merge)\n",
    "- Use Annotated types for custom merge logic\n",
    "- Design effective state schemas\n",
    "\n",
    "### State Reducers\n",
    "\n",
    "**Problem**: When multiple nodes update the same field, how do we merge?\n",
    "\n",
    "**Solution**: Reducer functions\n",
    "\n",
    "### Common Reducer Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8rn1z8wezn5",
   "source": "## Understanding State Merging and Reducers\n\nThis is the MOST IMPORTANT concept in LangGraph. Understanding state merging is critical.\n\n### The Problem: Multiple Nodes, One State\n\nWhen multiple nodes update the same state field, how do we merge the updates?\n\n**Example scenario**:\n- Node 1 returns: `{\"messages\": [\"Hello\"]}`\n- Node 2 returns: `{\"messages\": [\"How are you?\"]}`\n- What should final state be?\n\n**Without reducers**: Second update OVERWRITES first\n```python\nfinal_state = {\"messages\": [\"How are you?\"]}  # âŒ Lost \"Hello\"\n```\n\n**With reducers**: Updates are MERGED\n```python\nfinal_state = {\"messages\": [\"Hello\", \"How are you?\"]}  # âœ… Both preserved\n```\n\n### What is a Reducer?\n\nA **reducer** is a function that defines HOW to merge state updates.\n\n**Syntax**:\n```python\nfrom typing import Annotated\nimport operator\n\nclass State(TypedDict):\n    messages: Annotated[list, operator.add]  # â† Reducer\n```\n\n**Annotated** (from `typing`):\n- Adds metadata to type hints\n- First parameter: type (list, int, str, etc.)\n- Second parameter: reducer function\n\n**operator.add**:\n- For lists: concatenate (append items)\n- For numbers: addition\n\n### Common Reducer Patterns\n\n**Pattern 1: Accumulate messages** (most common)\n```python\nmessages: Annotated[list, operator.add]\n\n# Node 1: return {\"messages\": [\"Hello\"]}\n# Node 2: return {\"messages\": [\"World\"]}\n# Result: {\"messages\": [\"Hello\", \"World\"]}\n```\n\n**Pattern 2: Keep maximum value**\n```python\nscore: Annotated[int, lambda x, y: max(x, y)]\n\n# Current state: {\"score\": 10}\n# Node returns: {\"score\": 15}\n# Result: {\"score\": 15}\n```\n\n**Pattern 3: Default (no reducer) - replace**\n```python\nstatus: str  # No Annotated = replace\n\n# Current state: {\"status\": \"pending\"}\n# Node returns: {\"status\": \"completed\"}\n# Result: {\"status\": \"completed\"}\n```\n\n### How State Merging Works\n\n**Execution flow**:\n```\n1. Entry node executes â†’ returns {\"messages\": [\"Step 1\"]}\n2. LangGraph merges with initial state using reducer\n3. Next node executes â†’ returns {\"messages\": [\"Step 2\"]}\n4. LangGraph merges again using reducer\n5. Final state: {\"messages\": [\"Step 1\", \"Step 2\"]}\n```\n\n**Visual example**:\n```python\n# Initial state\n{\"messages\": [], \"count\": 0}\n\n# Node 1 returns\n{\"messages\": [\"Hello\"]}\n# After merge (operator.add for messages)\n{\"messages\": [\"Hello\"], \"count\": 0}\n\n# Node 2 returns\n{\"messages\": [\"World\"], \"count\": 1}\n# After merge\n{\"messages\": [\"Hello\", \"World\"], \"count\": 1}\n```\n\nğŸ¯ **Key insight**: Nodes return ONLY the fields they modify. Reducers determine how updates merge.\n\nNow let's see reducers in action!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State with different reducer patterns\n",
    "from typing import Annotated\n",
    "import operator\n",
    "\n",
    "class AdvancedState(TypedDict):\n",
    "    # List - appends new items (using operator.add)\n",
    "    messages: Annotated[list, operator.add]\n",
    "    \n",
    "    # Replace - overwrites (default behavior)\n",
    "    current_step: str\n",
    "    \n",
    "    # Custom reducer - take maximum\n",
    "    score: Annotated[int, lambda x, y: max(x, y)]\n",
    "    \n",
    "    # Custom reducer - concatenate strings\n",
    "    notes: Annotated[str, lambda x, y: x + \" | \" + y]\n",
    "\n",
    "print(\"âœ… Advanced state schema with reducers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yl7sdaq9u4",
   "source": "## Understanding Graph Invocation\n\nHow do you run a compiled graph? The `invoke()` method.\n\n### Basic Invocation\n\n```python\nresult = app.invoke({\"messages\": [], \"count\": 0})\n```\n\n**What invoke() does**:\n1. Takes initial state dictionary as input\n2. Executes graph from entry point\n3. Follows edges until reaching END\n4. Returns FULL final state (all fields)\n\n### Initial State Requirements\n\n**Must include all state fields** defined in TypedDict:\n\n```python\nclass State(TypedDict):\n    messages: Annotated[list, operator.add]\n    count: int\n\n# âœ… Valid - all fields present\napp.invoke({\"messages\": [], \"count\": 0})\n\n# âŒ Invalid - missing count\napp.invoke({\"messages\": []})  # Error!\n```\n\n**Tip**: Initialize lists as empty `[]`, numbers as `0`, strings as `\"\"`\n\n### Return Value\n\n**invoke() returns FULL state** after all nodes execute:\n\n```python\nresult = app.invoke({\"messages\": [], \"count\": 0})\nprint(result)\n# {'messages': ['Hello', 'World'], 'count': 2}\n```\n\n**Not just last node's return** - it's the accumulated state after all merges.\n\n### Execution Flow\n\n```\napp.invoke(initial_state)\n         â†“\nEntry point node executes\n         â†“\nReturns partial state â†’ Merge with current state using reducers\n         â†“\nFollow edges to next node\n         â†“\nNext node executes â†’ Returns partial state â†’ Merge\n         â†“\n... repeat until END\n         â†“\nReturn final merged state\n```\n\nğŸ¯ **Key insight**: invoke() is blocking (waits for completion) and returns full final state.\n\nLet's test this!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Test reducer behavior\ndef node1(state: AdvancedState):\n    return {\n        \"messages\": [\"Message from node1\"],\n        \"current_step\": \"node1\",\n        \"score\": 10,\n        \"notes\": \"Node1 executed\"\n    }\n\ndef node2(state: AdvancedState):\n    return {\n        \"messages\": [\"Message from node2\"],\n        \"current_step\": \"node2\",\n        \"score\": 5,  # Lower than node1, should stay 10\n        \"notes\": \"Node2 executed\"\n    }\n\n# Build graph\nworkflow = StateGraph(AdvancedState)\nworkflow.add_node(\"node1\", node1)\nworkflow.add_node(\"node2\", node2)\nworkflow.add_edge(START, \"node1\")\nworkflow.add_edge(\"node1\", \"node2\")\nworkflow.add_edge(\"node2\", END)\n\napp = workflow.compile()\n\n# Run\nresult = app.invoke({\n    \"messages\": [],\n    \"current_step\": \"\",\n    \"score\": 0,\n    \"notes\": \"Start\"\n})\n\nprint(\"Final state:\")\nprint(f\"  Messages: {result['messages']}\")  # Should have 2 messages (appended)\nprint(f\"  Current step: {result['current_step']}\")  # Should be 'node2' (replaced)\nprint(f\"  Score: {result['score']}\")  # Should be 10 (max of 10 and 5)\nprint(f\"  Notes: {result['notes']}\")  # Should be concatenated"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### State Design Best Practices\n",
    "\n",
    "#### What to Track in State\n",
    "\n",
    "**DO track**:\n",
    "- User input/question\n",
    "- Processing results\n",
    "- Decision points (what path was taken)\n",
    "- Iteration count (for retry logic)\n",
    "- Accumulated data (messages, findings)\n",
    "\n",
    "**DON'T track**:\n",
    "- Computed values (recalculate instead)\n",
    "- Temporary variables (use node-local variables)\n",
    "- Constant configuration (pass as parameters)\n",
    "\n",
    "#### State Schema Guidelines\n",
    "\n",
    "1. **Use TypedDict**: Clear schema, type safety\n",
    "2. **Use reducers**: Control how updates merge\n",
    "3. **Keep it flat**: Avoid deep nesting\n",
    "4. **Document fields**: Add comments explaining purpose\n",
    "\n",
    "#### Example\n",
    "\n",
    "```python\n",
    "class WellDesignedState(TypedDict):\n",
    "    # User input\n",
    "    query: str  # Original user question\n",
    "    \n",
    "    # Processing state\n",
    "    messages: Annotated[list[str], operator.add]  # Accumulated messages (append)\n",
    "    \n",
    "    # Decision tracking\n",
    "    needs_research: bool  # Whether research is needed\n",
    "    iteration_count: int  # How many loops executed\n",
    "    \n",
    "    # Results\n",
    "    final_answer: str  # Final response to user\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Reducers**: Control how state updates merge\n",
    "- **operator.add**: Append to lists\n",
    "- **Custom reducers**: max, concatenate, custom logic\n",
    "- **Design principles**: Flat, documented, typed schemas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Section 4: Building Your First Graph\n",
    "\n",
    "### Learning Objectives\n",
    "- Build a research agent with conditional routing\n",
    "- Use StateGraph with multiple nodes\n",
    "- Implement conditional edges\n",
    "\n",
    "### Use Case: Research Agent\n",
    "\n",
    "**Goal**: Build an agent that:\n",
    "1. Analyzes the question\n",
    "2. Decides if research is needed\n",
    "3. Either researches (complex question) or answers directly (simple question)\n",
    "\n",
    "### State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLM components first\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define state schema\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    needs_research: bool\n",
    "    reasoning: str\n",
    "\n",
    "print(\"âœ… State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Analyze question\n",
    "def analyze_question(state: AgentState):\n",
    "    \"\"\"Determine if the question needs research\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Use LLM to decide\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Determine if this question needs research or can be answered directly. \"\n",
    "                   \"Respond with 'RESEARCH' or 'DIRECT'.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    decision = chain.invoke({\"question\": question})\n",
    "    \n",
    "    needs_research = \"RESEARCH\" in decision.upper()\n",
    "    \n",
    "    return {\n",
    "        \"needs_research\": needs_research,\n",
    "        \"reasoning\": f\"Decision: {decision}\"\n",
    "    }\n",
    "\n",
    "# Node 2: Research (for complex questions)\n",
    "def research(state: AgentState):\n",
    "    \"\"\"Simulate research process\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a research assistant. Provide a detailed, well-researched answer.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"answer\": f\"[Researched] {answer}\"}\n",
    "\n",
    "# Node 3: Direct answer (for simple questions)\n",
    "def direct_answer(state: AgentState):\n",
    "    \"\"\"Provide quick answer\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Provide a concise, direct answer.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"answer\": f\"[Direct] {answer}\"}\n",
    "\n",
    "print(\"âœ… All nodes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Build Graph with Conditional Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o13fzlwwh9",
   "source": "## Understanding Conditional Routing\n\nSo far, edges are static (always go to same node). What if you want dynamic routing based on state?\n\n### Conditional Edges\n\n**add_conditional_edges()** enables dynamic routing.\n\n```python\nworkflow.add_conditional_edges(\n    \"analyze\",           # Source node\n    route_question,      # Router function\n    {                    # Edge mapping\n        \"search\": \"search_web\",\n        \"generate\": \"generate_answer\"\n    }\n)\n```\n\n**Three parameters**:\n1. **Source node**: Which node's output triggers routing decision\n2. **Router function**: Function that examines state and returns routing decision\n3. **Edge mapping**: Dictionary mapping router return values to target nodes\n\n### Router Functions\n\n**Router** is a Python function that takes state and returns a string.\n\n```python\ndef route_question(state: AgentState) -> str:\n    if \"search\" in state[\"question\"].lower():\n        return \"search\"\n    else:\n        return \"generate\"\n```\n\n**Critical rules**:\n1. Takes state as argument (receives current state)\n2. Returns string matching key in edge mapping\n3. Returned value must be in mapping dict keys\n4. Can return END to terminate\n5. Called AFTER source node completes\n\n### Execution Flow\n\n```\n\"analyze\" node executes\n         â†“\nState updated via reducer\n         â†“\nRouter function called: route_question(state)\n         â†“\nRouter returns \"search\"\n         â†“\nLook up \"search\" in edge mapping â†’ \"search_web\"\n         â†“\nExecute \"search_web\" node next\n```\n\n### Visual Example\n\n```python\n# State after analyze node\n{\"question\": \"search for Python tutorials\", \"answer\": \"\"}\n\n# Router examines state\ndef route_question(state):\n    if \"search\" in state[\"question\"]:\n        return \"search\"  # â† Returns \"search\"\n    return \"generate\"\n\n# Edge mapping\n{\"search\": \"search_web\", \"generate\": \"generate_answer\"}\n\n# Result: Go to \"search_web\" node\n```\n\n### Common Patterns\n\n**Pattern 1: Route based on content**\n```python\ndef route(state):\n    if \"urgent\" in state[\"message\"]:\n        return \"priority_handler\"\n    return \"normal_handler\"\n```\n\n**Pattern 2: Route based on flags**\n```python\ndef route(state):\n    if state[\"needs_approval\"]:\n        return \"approval\"\n    return END\n```\n\n**Pattern 3: Multi-way routing**\n```python\ndef route(state):\n    if state[\"score\"] > 0.8:\n        return \"high_confidence\"\n    elif state[\"score\"] > 0.5:\n        return \"medium_confidence\"\n    else:\n        return \"low_confidence\"\n```\n\nğŸ¯ **Key insight**: Router functions enable branching logic based on runtime state.\n\nNow let's build a routing workflow!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Build the graph\nworkflow = StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"analyze\", analyze_question)\nworkflow.add_node(\"research\", research)\nworkflow.add_node(\"answer\", direct_answer)\n\n# Add conditional edge from analyze\ndef route_question(state: AgentState):\n    \"\"\"Route to research or direct answer based on analysis\"\"\"\n    if state[\"needs_research\"]:\n        return \"research\"\n    else:\n        return \"answer\"\n\nworkflow.add_edge(START, \"analyze\")\nworkflow.add_conditional_edges(\n    \"analyze\",\n    route_question,\n    {\n        \"research\": \"research\",\n        \"answer\": \"answer\"\n    }\n)\n\n# Both paths end\nworkflow.add_edge(\"research\", END)\nworkflow.add_edge(\"answer\", END)\n\n# Compile\nresearch_agent = workflow.compile()\n\nprint(\"âœ… Research agent compiled!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with simple question\n",
    "simple_question = \"What is 2 + 2?\"\n",
    "\n",
    "result = research_agent.invoke({\n",
    "    \"question\": simple_question,\n",
    "    \"answer\": \"\",\n",
    "    \"needs_research\": False,\n",
    "    \"reasoning\": \"\"\n",
    "})\n",
    "\n",
    "print(\"Simple Question:\", simple_question)\n",
    "print(\"Reasoning:\", result[\"reasoning\"])\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Path taken:\", \"RESEARCH\" if result[\"needs_research\"] else \"DIRECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with complex question\n",
    "complex_question = \"What are the implications of quantum computing on cryptography?\"\n",
    "\n",
    "result = research_agent.invoke({\n",
    "    \"question\": complex_question,\n",
    "    \"answer\": \"\",\n",
    "    \"needs_research\": False,\n",
    "    \"reasoning\": \"\"\n",
    "})\n",
    "\n",
    "print(\"Complex Question:\", complex_question)\n",
    "print(\"Reasoning:\", result[\"reasoning\"])\n",
    "print(\"Answer:\", result[\"answer\"][:200] + \"...\")\n",
    "print(\"Path taken:\", \"RESEARCH\" if result[\"needs_research\"] else \"DIRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **StateGraph**: Define state schema with TypedDict\n",
    "- **Nodes**: Functions that modify state\n",
    "- **Conditional edges**: Dynamic routing with `add_conditional_edges`\n",
    "- **Router function**: Returns node name based on state\n",
    "- **Compile & invoke**: Just like LCEL chains\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yq6lpjv276c",
   "source": "## Section 4.5: Tools and Function Calling\n\n### Learning Objectives\n- Understand what tools are and when to use them\n- Define tools using @tool decorator\n- Use ToolNode component for tool execution\n- Use tools_condition for automatic routing\n- Build a complete tool-enabled agent\n\n### What Are Tools?\n\n**Tools** are functions that LLMs can call to get external data or perform actions they can't do on their own.\n\n**Examples of tools**:\n- ğŸ” Web search (Tavily, Google)\n- ğŸ§® Calculator (math operations)\n- ğŸ“š Database queries (SQL, vector stores)\n- ğŸŒ API calls (weather, stock prices, etc.)\n- ğŸ“ File operations (read, write, delete)\n\n**When to use tools vs custom nodes**:\n- **Use tools when**: LLM should decide what action to take based on user input\n- **Use custom nodes when**: You always perform the same deterministic operation\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "01af3w1jv3pl",
   "source": "# Subsection 1: Defining a Simple Tool\nfrom langchain.tools import tool\n\n@tool\ndef get_word_length(word: str) -> int:\n    \"\"\"Returns the length of a word.\"\"\"\n    return len(word)\n\n# Test the tool\nprint(f\"Tool name: {get_word_length.name}\")\nprint(f\"Tool description: {get_word_length.description}\")\nprint(f\"Test: get_word_length('hello') = {get_word_length.invoke('hello')}\")\n\nprint(\"\\nâœ… Simple tool defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rstsjvkslh",
   "source": "### Using Tools with LLM\n\nHow do LLMs know about tools? We **bind tools** to the LLM using `bind_tools()`.\n\nWhen the LLM receives a question, it can:\n1. **Respond directly** (if it knows the answer)\n2. **Call a tool** (if it needs external data)\n\nThe LLM returns **tool_calls** in its response when it wants to use a tool.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "x4eii7ixeom",
   "source": "# Subsection 2: LLM with Tools\nfrom langchain_core.messages import HumanMessage\n\n# Define multiple tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n# Bind tools to LLM\ntools = [multiply, add]\nllm_with_tools = llm.bind_tools(tools)\n\n# LLM decides which tool to call\nresponse = llm_with_tools.invoke([HumanMessage(content=\"What is 3 times 4?\")])\n\nprint(\"LLM Response:\")\nprint(f\"  Content: {response.content}\")\nprint(f\"  Tool calls: {response.tool_calls}\")\n\nprint(\"\\nâœ… LLM decided to call the 'multiply' tool!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f72raamfset",
   "source": "### ToolNode - Executing Tools Automatically\n\n**Problem**: The LLM returns tool_calls, but doesn't actually execute them.\n\n**Solution**: `ToolNode` is a pre-built component that:\n1. Takes messages with tool_calls\n2. Executes the tools\n3. Returns ToolMessage with results\n\n**Why use it?**\n- âœ… Handles tool execution automatically\n- âœ… Error handling built-in\n- âœ… Properly formats results as ToolMessage\n\n**Important**: ToolNode is designed to work within compiled graphs where config and state are managed automatically. You'll see it in action in Subsection 5 where we build a complete tool-enabled agent.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gllq7p7ucdv",
   "source": "# Subsection 3: Understanding ToolNode Conceptually\n\n# ToolNode is a pre-built component that executes tool calls\n# It's designed to work within compiled graphs, not standalone\n\n# What ToolNode does:\n# 1. Takes messages with tool_calls (from LLM)\n# 2. Executes the requested tools\n# 3. Returns ToolMessage objects with results\n\n# Example of what messages look like:\nfrom langchain_core.messages import AIMessage, ToolMessage\n\n# Step 1: LLM returns AIMessage with tool_calls\nai_message_example = AIMessage(\n    content=\"\",\n    tool_calls=[{\n        'name': 'multiply',\n        'args': {'a': 3, 'b': 4},\n        'id': 'call_1'\n    }]\n)\n\nprint(\"ğŸ“‹ What the LLM returns:\")\nprint(f\"  Tool to call: {ai_message_example.tool_calls[0]['name']}\")\nprint(f\"  Arguments: {ai_message_example.tool_calls[0]['args']}\")\n\n# Step 2: ToolNode would execute multiply(3, 4) and return:\ntool_message_example = ToolMessage(\n    content=\"12\",  # Result of multiply(3, 4)\n    tool_call_id=\"call_1\"\n)\n\nprint(f\"\\nğŸ”§ What ToolNode returns:\")\nprint(f\"  Result: {tool_message_example.content}\")\nprint(f\"  Message type: {type(tool_message_example).__name__}\")\n\nprint(\"\\nâœ… ToolNode automates this execution within graphs!\")\nprint(\"ğŸ’¡ See the full working example in Subsection 5 below.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "284bnnhi5gwh",
   "source": "### tools_condition - Automatic Routing\n\n**Problem**: How do we know when to route to ToolNode vs END?\n\n**Solution**: `tools_condition` is a pre-built router function that:\n- Returns `\"tools\"` if the last message has tool_calls\n- Returns `END` if no tool_calls exist\n\n**This replaces** writing custom router functions for tools!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lum73vc5yb",
   "source": "# Subsection 4: tools_condition\nfrom langgraph.prebuilt import tools_condition\n\n# Example state with tool calls\nstate_with_tools = {\n    \"messages\": [AIMessage(\n        content=\"\",\n        tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 4}, 'id': 'call_1'}]\n    )]\n}\n\n# Example state without tool calls\nstate_without_tools = {\n    \"messages\": [AIMessage(content=\"The answer is 12\")]\n}\n\n# tools_condition checks for tool_calls\nprint(\"Routing decisions:\")\nprint(f\"  State WITH tool_calls: {tools_condition(state_with_tools)}\")\nprint(f\"  State WITHOUT tool_calls: {tools_condition(state_without_tools)}\")\n\nprint(\"\\nâœ… tools_condition automatically determines routing!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "g7s2m65ieit",
   "source": "### Building a Complete Tool-Enabled Agent\n\nNow let's put it all together! We'll build an agent that:\n1. Receives user questions\n2. Decides whether to use tools\n3. Executes tools if needed\n4. Processes results and responds\n\n**Key components**:\n- `AgentState`: State with messages\n- `agent_node`: LLM with bound tools\n- `ToolNode`: Executes tools\n- `tools_condition`: Routes to tools or END",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bxv5e1yz9qb",
   "source": "# Subsection 5: Complete Tool-Enabled Agent\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.messages import BaseMessage\nimport operator\n\n# State schema\nclass AgentState(TypedDict):\n    messages: Annotated[list[BaseMessage], operator.add]\n\n# Define tools\n@tool\ndef search_web(query: str) -> str:\n    \"\"\"Search the web for information (simulated).\"\"\"\n    return f\"Search results for '{query}': LangGraph is a framework for building stateful, multi-actor applications with LLMs.\"\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"Calculate a mathematical expression (simulated).\"\"\"\n    try:\n        result = eval(expression)  # Note: eval is unsafe in production!\n        return str(result)\n    except:\n        return \"Error in calculation\"\n\n# Tools list\nagent_tools = [search_web, calculate]\n\n# Node 1: Agent (LLM with tools)\ndef agent_node(state: AgentState):\n    \"\"\"LLM decides whether to use tools or respond.\"\"\"\n    llm_with_tools = llm.bind_tools(agent_tools)\n    response = llm_with_tools.invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n# Build graph\nworkflow = StateGraph(AgentState)\n\n# Add nodes\nworkflow.add_node(\"agent\", agent_node)\nworkflow.add_node(\"tools\", ToolNode(agent_tools))  # ToolNode executes tools\n\n# Add edges\nworkflow.add_edge(START, \"agent\")\n\n# Conditional routing: if LLM calls tools â†’ execute them, otherwise END\nworkflow.add_conditional_edges(\n    \"agent\",\n    tools_condition,  # Pre-built router!\n    {\n        \"tools\": \"tools\",  # If tool_calls exist â†’ go to tools\n        END: END           # Otherwise â†’ end\n    }\n)\n\n# After tools execute, loop back to agent\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Compile\nagent_with_tools = workflow.compile()\n\nprint(\"âœ… Tool-enabled agent created!\")\nprint(\"\\nGraph structure:\")\nprint(\"  START â†’ agent â†’ tools_condition\")\nprint(\"             â†“            â†“\")\nprint(\"            END      tools â†’ agent\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o0lzc3c5vg9",
   "source": "### Testing the Agent\n\nLet's test our agent with different types of questions!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vgye3ctdt6",
   "source": "# Test 1: Question that requires search\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n\nresult = agent_with_tools.invoke({\n    \"messages\": [HumanMessage(content=\"What is LangGraph?\")]\n})\n\nprint(\"=== Test 1: Web Search ===\")\nprint(\"\\nConversation:\")\nfor msg in result[\"messages\"]:\n    if isinstance(msg, HumanMessage):\n        print(f\"ğŸ‘¤ Human: {msg.content}\")\n    elif isinstance(msg, AIMessage):\n        if msg.tool_calls:\n            print(f\"ğŸ¤– AI: [Calling tool: {msg.tool_calls[0]['name']}]\")\n        else:\n            print(f\"ğŸ¤– AI: {msg.content}\")\n    elif isinstance(msg, ToolMessage):\n        print(f\"ğŸ”§ Tool: {msg.content[:80]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nbg7u059jj",
   "source": "# Test 2: Question that requires calculation\nresult = agent_with_tools.invoke({\n    \"messages\": [HumanMessage(content=\"What is 25 * 4 + 10?\")]\n})\n\nprint(\"\\n=== Test 2: Calculation ===\")\nprint(\"\\nConversation:\")\nfor msg in result[\"messages\"]:\n    if isinstance(msg, HumanMessage):\n        print(f\"ğŸ‘¤ Human: {msg.content}\")\n    elif isinstance(msg, AIMessage):\n        if msg.tool_calls:\n            print(f\"ğŸ¤– AI: [Calling tool: {msg.tool_calls[0]['name']}]\")\n        else:\n            print(f\"ğŸ¤– AI: {msg.content}\")\n    elif isinstance(msg, ToolMessage):\n        print(f\"ğŸ”§ Tool: {msg.content}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jyeyxb5xh1",
   "source": "### Key Takeaways\n\n**Core Concepts**:\n- **Tools**: Functions that LLMs can call to get external data\n- **`@tool` decorator**: Define tools with type hints and docstrings\n- **`bind_tools()`**: Attach tools to LLM so it knows they're available\n- **`ToolNode`**: Pre-built component that executes tool calls\n- **`tools_condition`**: Pre-built router (routes to \"tools\" if tool_calls exist, END otherwise)\n\n**The Pattern**:\n```\nUser â†’ Agent (LLM) â†’ tools_condition\n           â†“              â†“\n          END        ToolNode â†’ Agent (process results) â†’ Response\n```\n\n**When to use tools**:\n- âœ… Need external data (web search, APIs, databases)\n- âœ… Need calculations, code execution\n- âœ… LLM should decide what action to take\n\n**When NOT to use tools**:\n- âŒ Simple deterministic logic â†’ use custom nodes\n- âŒ Always need same operation â†’ use custom nodes\n- âŒ No LLM decision needed â†’ use custom nodes\n\n**Key Benefits of ToolNode + tools_condition**:\n- âœ… No manual tool execution code\n- âœ… No custom routing logic for tools\n- âœ… Built-in error handling\n- âœ… Properly formatted ToolMessages\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "## Section 6: Conditional Routing and Cycles\n\n### Learning Objectives\n- Implement loops and retries\n- Add cycle prevention\n- Build iterative workflows\n\n### Use Case: Code Generator with Testing\n\n**Goal**: Build an agent that:\n1. Generates code\n2. Tests the code\n3. If tests fail, fix and retry (up to 3 times)\n4. If tests pass, deliver\n\n### State Schema with Iteration Tracking"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Code generator state\nclass CodeGenState(TypedDict):\n    requirement: str\n    code: str\n    test_result: str\n    iteration: int\n    status: str  # \"success\", \"failed\", \"retry\"\n\n# Node 1: Generate code\ndef generate_code(state: CodeGenState):\n    \"\"\"Generate code based on requirement\"\"\"\n    requirement = state[\"requirement\"]\n    iteration = state.get(\"iteration\", 0)\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a Python code generator. Generate clean, working code.\"),\n        (\"human\", \"Requirement: {requirement}\\n\\nGenerate Python code.\")\n    ])\n    \n    if iteration > 0:\n        # Include previous test failure\n        prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a Python code generator. Fix the code based on test failure.\"),\n            (\"human\", \"Requirement: {requirement}\\n\\nPrevious test result: {test_result}\\n\\nGenerate fixed code.\")\n        ])\n    \n    chain = prompt | llm | StrOutputParser()\n    code = chain.invoke({\"requirement\": requirement, \"test_result\": state.get(\"test_result\", \"\")})\n    \n    return {\n        \"code\": code,\n        \"iteration\": iteration + 1\n    }\n\n# Node 2: Test code (simulated)\ndef test_code(state: CodeGenState):\n    \"\"\"Simulate testing the code\"\"\"\n    code = state[\"code\"]\n    iteration = state[\"iteration\"]\n    \n    # Simulate: first 2 attempts fail, 3rd succeeds\n    if iteration < 3:\n        return {\n            \"test_result\": f\"Test failed (attempt {iteration}): Syntax error\",\n            \"status\": \"retry\"\n        }\n    else:\n        return {\n            \"test_result\": \"All tests passed!\",\n            \"status\": \"success\"\n        }\n\n# Router: Decide next step\ndef route_after_test(state: CodeGenState):\n    \"\"\"Route based on test result and iteration count\"\"\"\n    if state[\"status\"] == \"success\":\n        return \"success\"  # âœ… Fixed: Return routing key, not node name\n    elif state[\"iteration\"] >= 3:\n        return \"failed\"\n    else:\n        return \"retry\"\n\n# Node 3: Deliver\ndef deliver_code(state: CodeGenState):\n    \"\"\"Deliver successful code\"\"\"\n    return {\"status\": \"delivered\"}\n\n# Node 4: Failed\ndef mark_failed(state: CodeGenState):\n    \"\"\"Mark as failed after max retries\"\"\"\n    return {\"status\": \"failed_max_retries\"}\n\nprint(\"âœ… Code generator nodes defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "0oax7u9y0o1n",
   "source": "## Understanding Cycles and Loops\n\nLangGraph allows cycles - nodes can route back to themselves or earlier nodes.\n\n### What are Cycles?\n\n**Cycle**: When a node can route back to itself or a previous node in the graph.\n\n```python\nworkflow.add_conditional_edges(\n    \"test\",\n    route_test_result,\n    {\n        \"pass\": \"deploy\",\n        \"retry\": \"generate\"  # â† Loop back to earlier node!\n    }\n)\n```\n\n**Visual**:\n```\ngenerate â†’ test â†’ [pass] â†’ deploy â†’ END\n            â†‘       |\n            â””â”€[retry]\n```\n\n### Why Cycles?\n\n**Use cases**:\n- Retry logic (regenerate until tests pass)\n- Iterative refinement (improve until quality threshold met)\n- Human feedback loops (revise until approved)\n- Multi-turn conversations (continue until user satisfied)\n\n### Preventing Infinite Loops\n\n**Problem**: Cycles can loop forever!\n\n**Solution**: Iteration counter with max check\n\n```python\nclass State(TypedDict):\n    iteration: int\n    max_iterations: int\n    content: str\n\ndef route_test_result(state: State) -> str:\n    # Check iteration limit\n    if state[\"iteration\"] >= state[\"max_iterations\"]:\n        return \"fail\"  # Stop looping\n\n    # Check test result\n    if tests_pass(state[\"content\"]):\n        return \"pass\"\n    else:\n        return \"retry\"  # Loop back\n```\n\n**Increment counter in node**:\n```python\ndef generate(state: State):\n    # Generate content\n    new_content = generate_code()\n\n    # Increment iteration counter\n    return {\n        \"content\": new_content,\n        \"iteration\": state[\"iteration\"] + 1\n    }\n```\n\n### Cycle Best Practices\n\nâœ… **Always have termination condition** (max iterations, timeout, success criteria)\nâœ… **Track iteration count** in state\nâœ… **Log iteration progress** for debugging\nâœ… **Provide escape hatch** (manual override, fallback)\n\nâŒ **Don't assume** first iteration will succeed\nâŒ **Don't create** cycles without counters\nâŒ **Don't set** max_iterations too high (costs!)\n\n### Example: Retry with Limit\n\n```python\n# Initial state\n{\"iteration\": 0, \"max_iterations\": 3, \"content\": \"\"}\n\n# Iteration 1: generate â†’ test â†’ retry â†’ generate (iteration=1)\n# Iteration 2: generate â†’ test â†’ retry â†’ generate (iteration=2)\n# Iteration 3: generate â†’ test â†’ retry â†’ generate (iteration=3)\n# Iteration 4: generate â†’ test â†’ [max reached] â†’ fail â†’ END\n```\n\nğŸ¯ **Key insight**: Cycles are powerful but require careful termination logic.\n\nNow let's implement a retry loop!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Build graph with cycle\nworkflow = StateGraph(CodeGenState)\n\n# Add nodes\nworkflow.add_node(\"generate\", generate_code)\nworkflow.add_node(\"test\", test_code)\nworkflow.add_node(\"deliver\", deliver_code)\nworkflow.add_node(\"failed\", mark_failed)\n\n# Edges\nworkflow.add_edge(START, \"generate\")\nworkflow.add_edge(\"generate\", \"test\")\n\n# Conditional routing after test\nworkflow.add_conditional_edges(\n    \"test\",\n    route_after_test,\n    {\n        \"retry\": \"generate\",  # Loop back!\n        \"success\": \"deliver\",\n        \"failed\": \"failed\"\n    }\n)\n\nworkflow.add_edge(\"deliver\", END)\nworkflow.add_edge(\"failed\", END)\n\ncode_gen_agent = workflow.compile()\n\nprint(\"âœ… Code generator with retry logic compiled!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the cycle\n",
    "result = code_gen_agent.invoke({\n",
    "    \"requirement\": \"Write a function to calculate fibonacci numbers\",\n",
    "    \"code\": \"\",\n",
    "    \"test_result\": \"\",\n",
    "    \"iteration\": 0,\n",
    "    \"status\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"Final status: {result['status']}\")\n",
    "print(f\"Iterations: {result['iteration']}\")\n",
    "print(f\"Test result: {result['test_result']}\")\n",
    "print(f\"\\nâœ… Graph executed with {result['iteration']} iterations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **Cycles**: Nodes can loop back to earlier nodes\n",
    "- **Iteration tracking**: Use state to count loops\n",
    "- **Cycle prevention**: Set max iterations to avoid infinite loops\n",
    "- **Conditional routing**: Route based on state (retry/success/fail)\n",
    "- **Real-world use**: Code generation, validation workflows, retries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "## Section 7: Multi-Agent Patterns\n\n### Learning Objectives\n- Understand supervisor pattern\n- Build multi-agent system with specialized agents\n- Implement centralized state management\n\n### Supervisor Pattern\n\n**Architecture**: One supervisor orchestrates multiple specialized agents\n\n```\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â”‚ Supervisor  â”‚\n           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â†“          â†“          â†“\n  [Researcher] [Writer] [Critic]\n```\n\n### Use Case: Content Creation Team\n\nBuild a team with:\n1. **Researcher**: Gathers information\n2. **Writer**: Creates content\n3. **Critic**: Reviews and provides feedback\n4. **Supervisor**: Orchestrates the workflow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-agent state\n",
    "from typing import Literal\n",
    "\n",
    "class MultiAgentState(TypedDict):\n",
    "    messages: Annotated[list[str], operator.add]  # All agent communications\n",
    "    topic: str  # What to write about\n",
    "    research: str  # Research findings\n",
    "    draft: str  # Written content\n",
    "    feedback: str  # Critic feedback\n",
    "    next_agent: str  # Who to call next\n",
    "    final_content: str  # Final approved content\n",
    "\n",
    "print(\"âœ… Multi-agent state defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "def researcher_agent(state: MultiAgentState):\n",
    "    \"\"\"Research the topic\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a research specialist. Provide key facts and information about the topic.\"),\n",
    "        (\"human\", \"Research: {topic}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    research = chain.invoke({\"topic\": topic})\n",
    "    \n",
    "    return {\n",
    "        \"research\": research,\n",
    "        \"messages\": [f\"Researcher: Completed research on '{topic}'\"]\n",
    "    }\n",
    "\n",
    "# Agent 2: Writer\n",
    "def writer_agent(state: MultiAgentState):\n",
    "    \"\"\"Write content based on research\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    research = state[\"research\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a professional writer. Create engaging content based on the research.\"),\n",
    "        (\"human\", \"Topic: {topic}\\n\\nResearch:\\n{research}\\n\\nWrite a short article.\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    draft = chain.invoke({\"topic\": topic, \"research\": research})\n",
    "    \n",
    "    return {\n",
    "        \"draft\": draft,\n",
    "        \"messages\": [\"Writer: Completed first draft\"]\n",
    "    }\n",
    "\n",
    "# Agent 3: Critic\n",
    "def critic_agent(state: MultiAgentState):\n",
    "    \"\"\"Review and provide feedback\"\"\"\n",
    "    draft = state[\"draft\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a content critic. Review the draft and decide if it's APPROVED or needs REVISION. \"\n",
    "                   \"If approved, respond with 'APPROVED'. If not, provide specific feedback.\"),\n",
    "        (\"human\", \"Draft:\\n{draft}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    feedback = chain.invoke({\"draft\": draft})\n",
    "    \n",
    "    return {\n",
    "        \"feedback\": feedback,\n",
    "        \"messages\": [f\"Critic: {feedback[:50]}...\"]\n",
    "    }\n",
    "\n",
    "# Supervisor: Orchestrate workflow\n",
    "def supervisor_agent(state: MultiAgentState):\n",
    "    \"\"\"Decide which agent to call next\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Simple routing logic\n",
    "    if not state.get(\"research\"):\n",
    "        return {\"next_agent\": \"researcher\"}\n",
    "    elif not state.get(\"draft\"):\n",
    "        return {\"next_agent\": \"writer\"}\n",
    "    elif not state.get(\"feedback\"):\n",
    "        return {\"next_agent\": \"critic\"}\n",
    "    elif \"APPROVED\" in state.get(\"feedback\", \"\").upper():\n",
    "        return {\n",
    "            \"next_agent\": \"END\",\n",
    "            \"final_content\": state[\"draft\"],\n",
    "            \"messages\": [\"Supervisor: Content approved!\"]\n",
    "        }\n",
    "    else:\n",
    "        # Need revision - go back to writer\n",
    "        return {\n",
    "            \"next_agent\": \"writer\",\n",
    "            \"messages\": [\"Supervisor: Requesting revision\"]\n",
    "        }\n",
    "\n",
    "print(\"âœ… All agents defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "# Build multi-agent graph\nworkflow = StateGraph(MultiAgentState)\n\n# Add all agents\nworkflow.add_node(\"supervisor\", supervisor_agent)\nworkflow.add_node(\"researcher\", researcher_agent)\nworkflow.add_node(\"writer\", writer_agent)\nworkflow.add_node(\"critic\", critic_agent)\n\n# Supervisor routes to agents\ndef route_supervisor(state: MultiAgentState):\n    \"\"\"Route based on supervisor's decision\"\"\"\n    next_agent = state.get(\"next_agent\", \"researcher\")\n    return next_agent\n\nworkflow.add_edge(START, \"supervisor\")\nworkflow.add_conditional_edges(\n    \"supervisor\",\n    route_supervisor,\n    {\n        \"researcher\": \"researcher\",\n        \"writer\": \"writer\",\n        \"critic\": \"critic\",\n        \"END\": END\n    }\n)\n\n# All agents return to supervisor\nworkflow.add_edge(\"researcher\", \"supervisor\")\nworkflow.add_edge(\"writer\", \"supervisor\")\nworkflow.add_edge(\"critic\", \"supervisor\")\n\n# Compile\nmulti_agent_system = workflow.compile()\n\nprint(\"âœ… Multi-agent system compiled!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-agent system\n",
    "result = multi_agent_system.invoke({\n",
    "    \"messages\": [],\n",
    "    \"topic\": \"The future of artificial intelligence\",\n",
    "    \"research\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"feedback\": \"\",\n",
    "    \"next_agent\": \"\",\n",
    "    \"final_content\": \"\"\n",
    "})\n",
    "\n",
    "print(\"Agent Communication Log:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"  {msg}\")\n",
    "\n",
    "print(\"\\nFinal Content:\")\n",
    "print(result[\"final_content\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **Supervisor pattern**: One orchestrator, multiple specialists\n",
    "- **Centralized state**: All agents communicate via shared state\n",
    "- **Conditional routing**: Supervisor decides next agent\n",
    "- **Cycles**: Can loop back (e.g., writer â†’ critic â†’ writer)\n",
    "- **Production-ready**: Scales to complex multi-agent systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": "## Section 8: Human-in-the-Loop Workflows\n\n### Learning Objectives\n- Implement interrupt points for human approval\n- Resume execution after human input\n- Build approval workflows\n\n### Why Human-in-the-Loop?\n\n**Use cases**:\n- Approval workflows (e.g., review before publishing)\n- Sensitive operations (e.g., confirm before deleting)\n- Quality control (e.g., human review of AI output)\n- Data collection (e.g., gather additional input mid-workflow)\n\n### Interrupt Mechanisms\n\nLangGraph supports interrupts:\n- **Before node execution**: `interrupt_before=[\"node_name\"]`\n- **After node execution**: `interrupt_after=[\"node_name\"]`\n- **Manual checkpointing**: Save state, resume later"
  },
  {
   "cell_type": "markdown",
   "id": "bbh14246bsr",
   "source": "## Understanding Checkpointing and Persistence\n\nCheckpointing saves graph state between invocations. Required for interrupts and resuming workflows.\n\n### What is a Checkpointer?\n\nA **checkpointer** saves state snapshots at each step of execution.\n\n**Without checkpointer**:\n```python\napp = workflow.compile()  # No persistence\nresult = app.invoke(state)  # Runs to completion, then state lost\n```\n\n**With checkpointer**:\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\nresult = app.invoke(state, config={\"configurable\": {\"thread_id\": \"1\"}})\n# State saved! Can resume later.\n```\n\n### Why Checkpointing?\n\n**Enables**:\n1. **Interrupts**: Pause execution for human approval\n2. **Resuming**: Continue from where you left off\n3. **Persistence**: Survive crashes and restarts\n4. **Multi-turn**: Maintain state across conversations\n5. **Debugging**: Inspect state at each step\n\n**Without checkpointer**: None of these work!\n\n### MemorySaver vs SqliteSaver\n\n**MemorySaver** (in-memory):\n```python\nfrom langgraph.checkpoint.memory import MemorySaver\nmemory = MemorySaver()\n```\n- âœ… Fast\n- âœ… Simple setup\n- âŒ Lost on restart\n- âŒ Not production-ready\n\n**SqliteSaver** (persistent):\n```python\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nsaver = SqliteSaver.from_conn_string(\"checkpoints.db\")\n```\n- âœ… Persistent (survives restarts)\n- âœ… Production-ready\n- âœ… Can inspect history\n- âš ï¸ Slightly slower\n\n**When to use**:\n- Notebooks/demos: MemorySaver\n- Production: SqliteSaver\n\n### How Checkpointing Works\n\n**Execution with checkpointer**:\n```\ninvoke(state, config={\"configurable\": {\"thread_id\": \"chat-1\"}})\n         â†“\nEntry node executes\n         â†“\nSave checkpoint (thread_id=\"chat-1\", step=1)\n         â†“\nNext node executes\n         â†“\nSave checkpoint (thread_id=\"chat-1\", step=2)\n         â†“\n... continue until END or interrupt\n```\n\n### thread_id: Isolating State\n\n**thread_id** identifies separate workflow instances.\n\n```python\n# User Alice's workflow\napp.invoke(state, config={\"configurable\": {\"thread_id\": \"alice\"}})\n\n# User Bob's workflow (completely separate!)\napp.invoke(state, config={\"configurable\": {\"thread_id\": \"bob\"}})\n```\n\n**Different thread_id** = different state = different execution path.\n\n**Same thread_id** = resume from last checkpoint.\n\nğŸ¯ **Key insight**: Checkpointer + thread_id enable multi-user, persistent workflows.\n\nNow let's add checkpointing!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "# Human-in-the-loop example: Content approval workflow\nfrom langgraph.checkpoint.memory import MemorySaver\n\nclass ApprovalState(TypedDict):\n    content: str\n    approved: bool\n    feedback: str\n\ndef generate_content(state: ApprovalState):\n    \"\"\"Generate content (simulated)\"\"\"\n    return {\"content\": \"Draft article: AI is transforming the world...\"}\n\ndef publish_content(state: ApprovalState):\n    \"\"\"Publish approved content\"\"\"\n    return {\"feedback\": f\"Published: {state['content'][:50]}...\"}\n\n# Build workflow with interrupt\napproval_workflow = StateGraph(ApprovalState)\napproval_workflow.add_node(\"generate\", generate_content)\napproval_workflow.add_node(\"publish\", publish_content)\n\napproval_workflow.add_edge(START, \"generate\")\napproval_workflow.add_edge(\"generate\", \"publish\")\napproval_workflow.add_edge(\"publish\", END)\n\n# Compile with checkpointer (required for interrupts)\nmemory = MemorySaver()\napproval_app = approval_workflow.compile(\n    checkpointer=memory,\n    interrupt_before=[\"publish\"]  # Pause before publishing\n)\n\nprint(\"âœ… Approval workflow with interrupt created\")"
  },
  {
   "cell_type": "markdown",
   "id": "39mju27zj2y",
   "source": "## Understanding Interrupts and Human-in-the-Loop\n\nInterrupts pause execution for human approval or input. Essential for production workflows.\n\n### What are Interrupts?\n\n**Interrupts** pause graph execution at specified nodes.\n\n```python\napp = workflow.compile(\n    checkpointer=memory,\n    interrupt_before=[\"publish\"]  # Pause BEFORE publish node\n)\n```\n\n**Two types**:\n- **interrupt_before**: Pause BEFORE executing specified nodes\n- **interrupt_after**: Pause AFTER executing specified nodes\n\n### How Interrupts Work\n\n**Execution flow**:\n```\ninvoke(state, config={\"configurable\": {\"thread_id\": \"1\"}})\n         â†“\nNodes execute normally\n         â†“\nReach \"publish\" node â†’ INTERRUPT!\n         â†“\nSave checkpoint and return current state\n         â†“\nWait for human approval...\n         â†“\ninvoke(None, config={\"configurable\": {\"thread_id\": \"1\"}})\n         â†“\nLoad checkpoint and resume from \"publish\" node\n         â†“\nContinue to END\n```\n\n### Resuming from Interrupt\n\n**Key pattern**: `invoke(None, config)` to resume.\n\n```python\n# Step 1: Initial invocation (pauses at interrupt)\nresult = app.invoke(\n    {\"content\": \"Draft blog post\"},\n    config={\"configurable\": {\"thread_id\": \"approval-1\"}}\n)\nprint(result)  # Shows state at interrupt point\n\n# Step 2: Human reviews and approves\n# ... (human decision-making)\n\n# Step 3: Resume execution\nresult = app.invoke(\n    None,  # â† None means \"load from checkpoint\"\n    config={\"configurable\": {\"thread_id\": \"approval-1\"}}  # Same thread_id!\n)\nprint(result)  # Shows final state after completion\n```\n\n**Why None?**\n- None tells LangGraph \"don't use new state, load from checkpoint\"\n- config with same thread_id specifies which checkpoint to load\n- Execution picks up exactly where it left off\n\n### Use Cases\n\n**Approval workflows**:\n```python\ninterrupt_before=[\"publish\"]  # Require approval before publishing\n```\n\n**Human feedback**:\n```python\ninterrupt_after=[\"generate\"]  # Let human review and edit generation\n```\n\n**Multi-stage approval**:\n```python\ninterrupt_before=[\"review\", \"publish\"]  # Two approval gates\n```\n\n### Requirements\n\n**Interrupts require checkpointer**:\n```python\n# âŒ This won't work\napp = workflow.compile(interrupt_before=[\"publish\"])\n\n# âœ… This works\napp = workflow.compile(\n    checkpointer=memory,\n    interrupt_before=[\"publish\"]\n)\n```\n\nWithout checkpointer, graph cannot save state to resume later.\n\n### Checking Interrupt Status\n\n**How to know if interrupted?**\n- invoke() returns state at interrupt point\n- Check if all nodes completed or stopped mid-execution\n- Use graph tracing/logging (covered in advanced topics)\n\nğŸ¯ **Key insight**: Interrupts + checkpointing enable human-in-the-loop workflows.\n\nNow let's build an approval workflow!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run until interrupt - ACTUAL human-in-the-loop demo\n",
    "config = {\"configurable\": {\"thread_id\": \"approval-1\"}}\n",
    "\n",
    "print(\"ğŸš€ Starting approval workflow...\")\n",
    "result = approval_app.invoke({\n",
    "    \"content\": \"\",\n",
    "    \"approved\": False,\n",
    "    \"feedback\": \"\"\n",
    "}, config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“„ GENERATED CONTENT:\")\n",
    "print(\"=\"*60)\n",
    "print(result['content'])\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâ¸ï¸  WORKFLOW PAUSED before 'publish' node\")\n",
    "print(\"\\nğŸ” Human Review Required!\")\n",
    "print(\"In a real application, this content would be shown to a reviewer.\")\n",
    "\n",
    "# ACTUALLY WAIT for human input\n",
    "approval = input(\"\\nğŸ‘¤ Type 'yes' to approve and publish, or 'no' to reject: \").strip().lower()\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "if approval == 'yes':\n",
    "    print(\"âœ… APPROVED by human reviewer!\")\n",
    "    print(\"ğŸ“¤ Resuming workflow to publish content...\\n\")\n",
    "    \n",
    "    # Resume workflow\n",
    "    final_result = approval_app.invoke(None, config)\n",
    "    \n",
    "    print(f\"âœ… {final_result['feedback']}\")\n",
    "    print(\"\\nğŸ‰ Workflow completed after human approval!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ REJECTED by human reviewer\")\n",
    "    print(\"ğŸ›‘ Workflow stopped - content was NOT published.\")\n",
    "    print(\"\\nğŸ’¡ In production, you could:\")\n",
    "    print(\"   â€¢ Send content back for revision\")\n",
    "    print(\"   â€¢ Log rejection reason\")\n",
    "    print(\"   â€¢ Trigger alternative workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **Interrupts**: Pause before/after nodes for human input\n",
    "- **Checkpointing required**: Must use checkpointer for interrupts\n",
    "- **Resume with None**: Continue from checkpoint\n",
    "- **Use cases**: Approval workflows, quality control, data collection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": "## Section 9: Checkpointing and Persistence\n\n### Learning Objectives\n- Understand checkpointing for state persistence\n- Use MemorySaver for in-memory persistence\n- Implement conversation threads\n\n### Checkpointing - Persistent State\n\n**Checkpointing** saves state between runs, enabling:\n- Resume from failure\n- Time-travel debugging\n- Multi-turn conversations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": "# Checkpointing with MemorySaver\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Define simple state\nclass ConversationState(TypedDict):\n    messages: Annotated[list[str], operator.add]\n    score: Annotated[int, lambda x, y: max(x, y)]\n\ndef process_turn(state: ConversationState):\n    \"\"\"Process a conversation turn\"\"\"\n    return {\n        \"messages\": [\"Processed turn\"],\n        \"score\": 10\n    }\n\n# Build graph\nworkflow = StateGraph(ConversationState)\nworkflow.add_node(\"process\", process_turn)\nworkflow.add_edge(START, \"process\")\nworkflow.add_edge(\"process\", END)\n\n# Create in-memory checkpointer\nmemory = MemorySaver()\n\n# Compile graph with checkpointer\napp_with_memory = workflow.compile(checkpointer=memory)\n\nprint(\"âœ… Graph with checkpointing created\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with thread_id for persistence\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "\n",
    "result1 = app_with_memory.invoke({\n",
    "    \"messages\": [\"Turn 1\"],\n",
    "    \"score\": 0\n",
    "}, config)\n",
    "\n",
    "print(\"First run completed\")\n",
    "print(f\"  Score: {result1['score']}\")\n",
    "print(f\"  Messages: {len(result1['messages'])} messages\")\n",
    "\n",
    "# Run again with same thread_id - state persists!\n",
    "result2 = app_with_memory.invoke({\n",
    "    \"messages\": [\"Turn 2\"],\n",
    "    \"score\": 15  # Higher score\n",
    "}, config)\n",
    "\n",
    "print(\"\\nSecond run completed\")\n",
    "print(f\"  Score: {result2['score']}\")  # Should be 15 (max of previous 10 and new 15)\n",
    "print(f\"  Messages: {len(result2['messages'])} messages\")  # Should accumulate\n",
    "\n",
    "print(\"\\nâœ… Checkpointing preserves state across runs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- **MemorySaver**: In-memory checkpointer for demos\n",
    "- **thread_id**: Maintain separate conversation threads\n",
    "- **State persistence**: State accumulates across invocations\n",
    "- **Production**: Use SqliteSaver or other persistent storage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": "## Section 10: Production Patterns and Best Practices\n\n### Learning Objectives\n- Implement error handling in nodes\n- Add retry logic\n- Build fallback mechanisms\n- Production best practices\n\n### Error Handling in Nodes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": "# Production workflow with error handling and retry logic\nclass ProductionState(TypedDict):\n    input: str\n    result: str\n    error: str\n    retry_count: int\n\ndef process_with_retry(state: ProductionState):\n    \"\"\"Process input with error handling\"\"\"\n    try:\n        # Simulate processing\n        prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a helpful assistant. Provide thoughtful answers.\"),\n            (\"human\", \"{input}\")\n        ])\n        \n        chain = prompt | llm | StrOutputParser()\n        result = chain.invoke({\"input\": state[\"input\"]})\n        \n        return {\n            \"result\": result,\n            \"error\": \"\"\n        }\n    except Exception as e:\n        # Increment retry counter\n        retry_count = state.get(\"retry_count\", 0) + 1\n        \n        if retry_count < 3:  # Max 3 retries\n            print(f\"âš ï¸  Error occurred: {str(e)}\")\n            print(f\"ğŸ”„ Retrying... (attempt {retry_count + 1}/3)\")\n            return {\n                \"error\": str(e),\n                \"retry_count\": retry_count\n            }\n        else:\n            print(f\"âŒ Max retries reached. Failing.\")\n            return {\n                \"error\": f\"Failed after 3 retries: {str(e)}\",\n                \"retry_count\": retry_count\n            }\n\ndef check_error(state: ProductionState):\n    \"\"\"Route based on error state\"\"\"\n    if state.get(\"error\") and state.get(\"retry_count\", 0) < 3:\n        return \"retry\"\n    elif state.get(\"error\"):\n        return \"failed\"\n    else:\n        return \"success\"\n\n# Build production graph with error handling\nprod_workflow = StateGraph(ProductionState)\n\n# Add nodes\nprod_workflow.add_node(\"process\", process_with_retry)\n\n# Add conditional routing for retries\nprod_workflow.add_edge(START, \"process\")  # âœ… Fixed: Use prod_workflow instead of workflow\nprod_workflow.add_conditional_edges(\n    \"process\",\n    check_error,\n    {\n        \"retry\": \"process\",  # Loop back to retry\n        \"success\": END,\n        \"failed\": END\n    }\n)\n\n# Compile\nprod_app = prod_workflow.compile()\n\nprint(\"âœ… Production workflow with error handling created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production workflow\n",
    "result = prod_app.invoke({\n",
    "    \"input\": \"What is the meaning of life?\",\n",
    "    \"result\": \"\",\n",
    "    \"error\": \"\",\n",
    "    \"retry_count\": 0\n",
    "})\n",
    "\n",
    "if result[\"error\"]:\n",
    "    print(f\"âŒ Error: {result['error']}\")\n",
    "    print(f\"   Retries: {result['retry_count']}\")\n",
    "else:\n",
    "    print(f\"âœ… Success: {result['result'][:100]}...\")\n",
    "    print(f\"   Retries: {result['retry_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### Production Best Practices\n",
    "\n",
    "**1. Observability**\n",
    "- Use LangSmith for tracing and monitoring\n",
    "- Log all state transitions\n",
    "- Track token usage and costs\n",
    "\n",
    "**2. Error Handling**\n",
    "- Try-except in every node\n",
    "- Retry with exponential backoff\n",
    "- Fallback paths for failures\n",
    "\n",
    "**3. State Management**\n",
    "- Explicit schemas with TypedDict\n",
    "- Immutable state updates (use reducers)\n",
    "- Checkpointing for reliability\n",
    "\n",
    "**4. Performance**\n",
    "- Parallel execution where possible\n",
    "- Streaming for long-running tasks\n",
    "- Cache expensive operations\n",
    "\n",
    "**5. Testing**\n",
    "- Unit test each node individually\n",
    "- Integration tests for full graph\n",
    "- LLM-as-a-judge for quality evaluation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Error handling**: Try-except in all nodes\n",
    "- **Retry logic**: Loop back on failure, max retries\n",
    "- **Observability**: LangSmith for production monitoring\n",
    "- **Checkpointing**: Reliability and resume capability\n",
    "- **Testing**: Unit and integration tests essential\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "## Exercise: Build an Approval Workflow\n",
    "\n",
    "**Duration**: ~20 minutes\n",
    "\n",
    "Build a simple document approval workflow with these requirements:\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. **State**: document_content, approved (bool), reviewer_feedback, revision_count\n",
    "2. **Nodes**:\n",
    "   - generate_document: Creates sample document\n",
    "   - review_document: Human reviews (interrupt point)\n",
    "   - publish_document: Publishes if approved\n",
    "   - revise_document: Handles rejections\n",
    "\n",
    "3. **Flow**:\n",
    "   - generate â†’ review (interrupt) â†’ approved? â†’ publish OR revise\n",
    "   - If revised, loop back to review\n",
    "\n",
    "**Bonus**: Add revision counter, max 3 revisions\n",
    "\n",
    "### Hints\n",
    "\n",
    "- Use `interrupt_before=[\"review_document\"]` for human review\n",
    "- Use conditional edges to route based on approval\n",
    "- Track revision_count in state\n",
    "- Use checkpointer (MemorySaver) for interrupts\n",
    "\n",
    "### Solution Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Build your approval workflow here\n",
    "\n",
    "# 1. Define state\n",
    "class DocumentState(TypedDict):\n",
    "    document_content: str\n",
    "    approved: bool\n",
    "    reviewer_feedback: str\n",
    "    revision_count: int\n",
    "\n",
    "# 2. Define nodes\n",
    "def generate_document(state: DocumentState):\n",
    "    # TODO: Implement document generation\n",
    "    pass\n",
    "\n",
    "def review_document(state: DocumentState):\n",
    "    # TODO: Implement human review (this is where interrupt happens)\n",
    "    pass\n",
    "\n",
    "def publish_document(state: DocumentState):\n",
    "    # TODO: Implement publishing\n",
    "    pass\n",
    "\n",
    "def revise_document(state: DocumentState):\n",
    "    # TODO: Implement revision\n",
    "    pass\n",
    "\n",
    "# 3. Build graph\n",
    "# TODO: Create StateGraph, add nodes, add edges, compile with interrupt\n",
    "\n",
    "# 4. Test workflow\n",
    "# TODO: Invoke workflow and handle human approval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**Core Concepts**:\n",
    "- LangGraph architecture: nodes, edges, state\n",
    "- State management with TypedDict and reducers\n",
    "- Conditional routing and cycles\n",
    "\n",
    "**Patterns**:\n",
    "- Research agent with decision points\n",
    "- Code generator with retry logic\n",
    "- Multi-agent supervisor pattern\n",
    "- Human-in-the-loop workflows\n",
    "\n",
    "**Production**:\n",
    "- Error handling and retry logic\n",
    "- Checkpointing for persistence\n",
    "- Best practices for observability\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LangGraph = LangChain + State + Cycles** - Use when complexity demands it\n",
    "\n",
    "2. **State design matters** - Use TypedDict, reducers, and clear schemas\n",
    "\n",
    "3. **Conditional routing** - Dynamic workflows with decision points\n",
    "\n",
    "4. **Cycles enable iteration** - Loops, retries, refinement\n",
    "\n",
    "5. **Multi-agent = specialized expertise** - Supervisor pattern enables complex workflows\n",
    "\n",
    "6. **Human-in-the-loop** - Interrupts for approval and quality control\n",
    "\n",
    "7. **Production-ready** - Error handling, observability, and checkpointing are essential\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Official Documentation**:\n",
    "- [LangGraph Documentation](https://www.langchain.com/langgraph)\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangSmith](https://www.langchain.com/langsmith) - Observability platform\n",
    "\n",
    "**Learn More**:\n",
    "- [LangChain Academy](https://academy.langchain.com/) - Free course\n",
    "- [Building LangGraph Blog](https://blog.langchain.com/building-langgraph/)\n",
    "- [LangGraph Multi-Agent Workflows](https://blog.langchain.com/langgraph-multi-agent-workflows/)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Complete the exercise above\n",
    "2. **Explore**: Try different state patterns and reducers\n",
    "3. **Integrate**: Combine with RAG from previous notebooks\n",
    "4. **Production**: Add observability with LangSmith\n",
    "5. **Advanced**: Explore function calling and tool use\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You've mastered LangGraph essentials! You can now:\n",
    "- Build stateful workflows with cycles\n",
    "- Implement multi-agent systems\n",
    "- Add human-in-the-loop workflows\n",
    "- Design production-ready graphs\n",
    "\n",
    "**You're ready for advanced agent patterns!**\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}